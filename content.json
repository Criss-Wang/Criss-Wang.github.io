{"posts":[{"title":"Zhenlin Wang (Criss)","text":"Welcome to my personal website! I'm Zhenlin Wang. You can call me Criss if it's easier for you. I'm currently studying for a Master of Machine Learning degree in Carnegie Mellon University. Before coming to the US, I completed my undergraduate study at National University of Singapore (NUS). My academic and practical interests lie in the intersection of data science, machine learning and software engineering, and I've been actively seeking for SDE/MLE internships recently. If you are interested to know more about me as a person, you can find some information [here]. What's in my websiteThis is a space to drop down some notes about my life. I started maintaining this website from my sophomore year in NUS. The original version used a minimal-mistake style and I've migrated into the hexo-icarus style on Aug 2022. There are several sections in this space: 1. Technical BlogsBlogs about various topics in DS/AI/ML and software engineering are written here. The mathematical/technical details are presented. I often included some discussion about the pros/cons of the methodologies outlined in these blogs. Nonetheless, I never believe that any blog post can be &quot;DONE&quot; as new perspectives on these topics can always supplement what's on the posts. Thus, I consistently update these posts whenever I learn some new knowledge about the topics discussed in these posts. Disclaimer: I try my best to give credit for all sources I made referenecs to. If you found some parts in my post that were referenced from your work without credit, please kindly contact me so I can immediately correct my mistakes and make apology. 2. Experience and skill setMy project experiences are listed here for record, many are driven by the interests to explore tech/skills I never knew. I'm eager to understand the basics of various technology toolkits and learn new stuff along the way. 3. Anime and EnlightenmentThese posts will be more of a casual type. I watch anime once in a while, especially those classical series that touched me deeply in heart. I've decided to reflect upon reviewing these series and drop them down here. Sometimes I meditate ((aka. emo)) or read. Now that I think it's a good idea to take note of them so that I won't waste time repeating myself during my meditations. Special thanksIn retrospect, I did make various references to many websites/blogs when building this version of personal space. I've got to thank @Lei Mao for the inspiration to build a website from scratch. I followed him since 2018 when I read his background story and found it so motivating. Learning from his blog styles, I built my first version of website and the journey of bloghing started afterwards. When trying to customizing the pages, I learnt so much from @iMaeGoo, @Xinyu Liu and @Pengyuan Li. I should thank them for their great tutorials on website styling.","link":"/post/Zhenlin%20Wang/"},{"title":"Clustering: K Means and Gaussian Mixture Models","text":"OverviewIn this blog we talk about K means and GMM algorithms, the famous and intuitively useful algorithms. As we venture further into unsupervised learning/clustering problems, we will see more interesting problem formulations as well as diverse evaluation metrics. Hope we would enjoy this learning journey along the way :) K means1. Defintion Clustering: A cluster refers to a collection of data points aggregated together because of certain similarities. K-means: an iterative algorithm that tries to partition the dataset into ùêæ pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group Kmeans gives more weight to the bigger clusters. Kmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn't work well when clusters are in different shapes such as elliptical clusters. Full procedure: Specify number of clusters . Initialize centroids by first shuffling the dataset and then randomly selecting data points for the centroids without replacement. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn't changing. Compute the sum of the squared distance between data points and all centroids. Assign each data point to the closest cluster (centroid). Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster. Expectation-Maximization: The approach kmeans follows to solve the problem is called Expectation-Maximization. The EM algorithm attempts to find maximum likelihood estimates for models with latent variables. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it). The objective function is: where for data point if it belongs to cluster ; otherwise, . Also, is the centroid of 's cluster. It's a minimization problem of two parts. We first minimize J w.r.t. and treat fixed. Then we minimize J w.r.t. and treat fixed. Technically speaking, we differentiate J w.r.t. first and update cluster assignments (E-step). Then we differentiate J w.r.t. and recompute the centroids after the cluster assignments from previous step (M-step). Therefore, E-step is: In other words, assign the data point to the closest cluster judged by its sum of squared distance from cluster's centroid. And M-step is: Which translates to recomputing the centroid of each cluster to reflect the new assignments. Standardization: Since clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it's recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income. Cold start the code may lead to Local optimum: Need to use different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance. Evaluation Method: Contrary to supervised learning where we have the ground truth to evaluate the model's performance, clustering analysis doesn't have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires as an input and doesn't learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different clusters since clusters are used in the downstream modeling. In this notebook we'll cover two metrics that may give us some intuition about : Elbow method Elbow method gives us an idea on what a good number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters' centroids. We pick at the spot where SSE starts to flatten out and forming an elbow. We'll use the geyser dataset and evaluate SSE for different values of and see where the curve might form an elbow and flatten out. Silhouette analysis Silhouette analysis can be used to determine the degree of separation between clusters. For each sample: Compute the average distance from all data points in the same cluster (). Compute the average distance from all data points in the closest cluster (). Compute the coefficient: The coefficient can take values in the interval [-1, 1]. If it is 0 ‚Äì&gt; the sample is very close to the neighboring clusters. It it is 1 ‚Äì&gt; the sample is far away from the neighboring clusters. It it is -1 ‚Äì&gt; the sample is assigned to the wrong clusters. Therefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. We'll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely only two groups of data points. 2. Pros &amp; ConsPros Easy to interpret Relatively fast Scalable for large data sets Able to choose the positions of initial centroids in a smart way that speeds up the convergence Guarantees convergence Cons The globally optimal result may not be achieved The number of clusters must be selected beforehand k-means is limited to linear cluster boundaries: this one may be solved using Similar technique as SVM does. One possible solution is the ‚ÄúSpectral Clustering‚Äù: i.e Kernelized K-means below 3. Applications Not to use if it contains heavily overlapping data/full of outliers Not so well if there are many categorical fields Not so well if the clusters have a complicated geometric shapes Real-world samples Market Segmentation Document clustering Image segmentation Image compression 4. Code ImplementationSklearn package's GMM12345from sklearn.cluster import KMeanskm = KMeans(n_clusters=2, max_iter=100)km.fit(X_std)centroids = km.cluster_centers_ GMM1. Definition The Gaussian mixture model (GMM) can be regarded as an optimization of the k-means model. It is not only a commonly used in industry but also a generative model. A model composed of K single Gaussian models. These K submodels are the hidden variables of the hybrid model Single Gaussian model: A Univariate Gaussian Distribution for the data Attempts to find a mixed representation of the probability distribution of the multidimensional Gaussian model, thereby fitting a data distribution of arbitrary shape. Also uses EM algorithm1: if our observations come from a mixture model with mixture components, the marginal probability distribution of is of the form: where is the latent variable representing the mixture component for is the mixture component, and is the mixture proportion representing the probability that belongs to the -th mixture component. Let denote the probability distribution function for a normal random variable. In this scenario, we have that the conditional distribution so that the marginal distribution of is: Similarly, the joint probability of observations is therefore: This note describes the EM algorithm which aims to obtain the maximum likelihood estimates of and given a data set of observations . Likelihood expression is ; Take log, compute and simplify the expected value of the complete log-likelihood: From here, we derive the expressions for each parameter: The EM algorithm, motivated by the two observations above, proceeds as follows: Initialize the 's, 's and 's and evaluate the log-likelihood with these parameters. E-step: Evaluate the posterior probabilities using the current values of the 's and 's with equation (2) M-step: Estimate new parameters and with the current values of using equations (3), (4) and (5). Evaluate the log-likelihood with the new parameter estimates. If the loglikelihood has changed by less than some small , stop. Otherwise, go back to step 2 . The EM algorithm is sensitive to the initial values of the parameters, so care must be taken in the first step. However, assuming the initial values are \"valid\", one property of the EM algorithm is that the log-likelihood increases at every step. This invariant proves to be useful when debugging the algorithm in practice. 2. Pros &amp; ConsPros GMM is a lot more flexible in terms of cluster covariance It is a soft-clustering method, which assign sample membersips to multiple clusters. This characteristic makes it the fastest algorithm to learn mixture models Cons Slower than k-means does not work if the mixture is not really a gaussian distribution It is very sensitive to the initial values which will condition greatly its performance. GMM may converge to a local minimum, which would be a sub-optimal solution. When having insufficient points per mixture, the algorithm diverges and finds solutions with infinite likelihood unless we regularize the covariances between the data points artificially. 3. Application perform GMM when you know that the data points are mixtures of a gaussian distribution if you think that your model is having some hidden, not observable parameters, then you can try to use GMM. 4. Simple codeSklearn package's GMM1234from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components = 3) gmm.fit(X_principal)gmm.fit_predict(X_principal)","link":"/post/clustering-1/"},{"title":"Clustering: Hierachial, BIRCH and Spectral","text":"Hierachial Clustering1. Definition 2 Main approaches Agglomerative : This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive : This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. Agglomerative Clustering Initially each data point is considered as an individual cluster. At each iteration, the most similar clusters merge with other clusters until 1/ K clusters are formed. No need to specify number of clusters, performance In sklearn, if we specify the number of clusters, performance can be improved Procedure Compute the proximity matrix Let each data point be a cluster Repeat: Merge two closest clusters and update the proximity matrix until 1/ K cluster remains Divisive Clustering Opposite of agglomerative clustering. We start with one giant cluster including all data points. Then data points are separated into different clusters. Similarity score: Basically the proximity between two clusters Distance calculation Euclidean Distance Squared Euclidean Distance Manhattan Distance Maximum Distance: Mahalanobis Distance: where is Covariance matrix For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used. For details, see Distance metrics &amp; Evaluation method[Unsupervised Learning/0. Distance metrics and Evaluation Methods/Distance_Metrics_Evaluation_Methods.ipynb] Distance references Complete-linkage: The maximum distance between elements of each cluster Single-linkage: The minimum distance between elements of each cluster Average linkage: The mean distance between elements of each cluster Ward‚Äôs linkage: Minimizes the variance of the clusters being merged. Least increase in total variance around cluster centroids is aimed. 2. Pros &amp; ConsPros Do not have to specify the number of clusters beforehand It is easy to implement and interpretable with the help of dendrograms Always generates the same clusters (Stability) Cons Exponential runtime for larger datasets 3. Application Text grouping: However, it is a highly complex task due the high-dimensionality of data. Social network analysis Outlier detection 4. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.cluster import AgglomerativeClustering from sklearn.preprocessing import StandardScaler, normalizefrom sklearn.decomposition import PCAfrom sklearn.metrics import silhouette_scoreimport scipy.cluster.hierarchy as shcraw_df = pd.read_csv('CC GENERAL.csv')raw_df = raw_df.drop('CUST_ID', axis = 1) raw_df.fillna(method ='ffill', inplace = True) # Standardize datascaler = StandardScaler() scaled_df = scaler.fit_transform(raw_df) # Normalizing the Data normalized_df = normalize(scaled_df) # Converting the numpy array into a pandas DataFrame normalized_df = pd.DataFrame(normalized_df) # Reducing the dimensions of the data pca = PCA(n_components = 2) X_principal = pca.fit_transform(normalized_df) X_principal = pd.DataFrame(X_principal) X_principal.columns = ['P1', 'P2'] plt.figure(figsize =(6, 6)) plt.title('Visualising the data') Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) # Determine the optimal number of clusters using [Silhouette Score]silhouette_scores = []for n_cluster in range(2, 8): silhouette_scores.append( silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) # Plotting a bar graph to compare the results k = [2, 3, 4, 5, 6,7] plt.bar(k, silhouette_scores) plt.xlabel('Number of clusters', fontsize = 10) plt.ylabel('Silhouette Score', fontsize = 10) plt.show() agg = AgglomerativeClustering(n_clusters=3)agg.fit(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = AgglomerativeClustering(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) plt.show() BIRCH Clustering1. Definition Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) Rationale: Existing data clustering methods do not adequately address the problem of processing large datasets with a limited amount of resources (i.e. memory and cpu cycles). In consequence, as the dataset size increases, they scale poorly in terms of running time, and result quality. Main logic of BIRCH: Deals with large datasets by first generating a more compact summary that retains as much distribution information as possible, and then clustering the data summary instead of the original dataset Metric attributes Definition: values can be represented by explicit Euclidean coordinates (no categorical variables). BIRCH can only deal with metric attributes Clustering Features BIRCH summarize the information contained in dense regions as Clustering Feature (CF); where = # of data points in a cluster, = linear sum of data; = square sum of data; CF additivity theorem: ; CF Tree Clustering Feature tree structure is similar to the balanced B+ tree A very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster. Each non-leaf node contains at most entries. Each leaf node contains at most entries, and each entry is a CF Threshold for leaf entry: all sample points in this CF must be in the radius In a hyper-sphere less than T. Insertion Algo: (Insert a new CF/Point entry in to the tree) Starting from the root, recursively traverse down the tree by choosing the node that has shortest Euclidean distance to the inserted entry; Upon reaching a leaf node, find the shorest distance CF and see if it can include the new CF/Point into the cluster without radius threshold violation;If can: do not create a new leaf, but update all the CF triplets on the path, the insertion ends;If cannot: go to 3; If the number of CF nodes of the current leaf node is less than the threshold , create a new CF node, put in a new sample and the new CF node into this leaf node, update all CF triplets on the path, and insertion Ends.Otherwise, go to 4 If the leaf node has &gt; L entires after addition, then split the leaf node by choosing the 2 entries that are farthest apart and redistribute CF based on distance to each of the 2 entries; Modify the path to leaf: Since the leaf node is updated, we need to update the entire path from root to leaf; In the event of split, we need to insert a nonleaf entry into the parent node, and if parent node has &gt; nodes, then we need to split again; do so until it reaches the root Complete procedure Phase 1: The algorithm starts with an initial threshold value (ideally start from low), scans the data, and inserts points into the tree. If it runs out of memory before it finishes scanning the data, it increases the threshold value, and rebuilds a new, smaller CF-tree, by re-inserting the leaf entries of the old CF-tree into the new CF-tree. After all the old leaf entries have been re-inserted, the scanning of the data and insertion into the new CF-tree is resumed from the point at which it was interrupted. (Optional) Filter the CF Tree created in the first step to remove some abnormal CF nodes. (Optional) Use other clustering algorithms such as K-Means to cluster all CF tuples to get a better CF Tree. Phase 2: Given that certain clustering algorithms perform best when the number of objects is within a certain range, we can group crowded subclusters into larger ones resulting in an overall smaller CF-tree. Phase 3: Almost any clustering algorithm can be adapted to categorize Clustering Features instead of data points. For instance, we could use KMEANS to categorize our data, all the while deriving the benefits from BIRCH Additional passes over the data to correct inaccuracies caused by the fact that the clustering algorithm is applied to a coarse summary of the data. The complexity of the algorithm is 2. Pros &amp; ConsPros Save memory, all samples are on disk, CF Tree only stores CF nodes and corresponding pointers. The clustering speed is fast, and it only takes one scan of the training set to build the CF Tree, and the addition, deletion, and modification of the CF Tree are very fast. Noise points can be identified, and preliminary classification pre-processing can be performed on the data set. Cons There is need to specify number of clusters; The clustering result may be different from the real category distribution. Does not perform well on non-convex dataset distribution Apart from number of clusters we have to specify two more parameters; Birch doesn‚Äôt perform well on high dimensional data (if there are &gt;20 features, you‚Äôd better use something else). 3. Applications If the dimension of the data features is very large, such as greater than 20, BIRCH is not suitable. At this time, Mini Batch K-Means performs better. 4. Code implementation12345678910111213141516import numpy as npfrom matplotlib import pyplot as pltimport seaborn as snssns.set()from sklearn.datasets import make_blobsfrom sklearn.cluster import BirchX, clusters = make_blobs(n_samples=450, centers=6, cluster_std=0.70, random_state=0)plt.scatter(X[:,0], X[:,1], alpha=0.7, edgecolors='b')# Predict and visualizebrc = Birch(branching_factor=50, n_clusters=None, threshold=1.5)brc.fit(X)labels = brc.predict(X)plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow', alpha=0.7, edgecolors='b') Spectral Clustering1. Definition In spectral clustering, data points are treated as nodes of a graph. Thus, spectral clustering is a graph partitioning problem. The nodes are mapped to a low-dimensional space that can be easily segregated to form clusters. No assumption is made about the shape/form of the clusters. The goal of spectral clustering is to cluster data that is connected but not necessarily compact or clustered within convex boundaries. In general, spectral clustering is a generalized version of k-means: it does not assume a circular shape, but apply different affinity functions in its similarity matrix Procedures Project data into matrix Define an Affinity matrix A , using a Gaussian Kernel K or an Adjacency matrix Construct the Graph Laplacian from A (i.e. decide on a normalization) Solve the Eigenvalue problem Select k eigenvectors corresponding to the k lowest (or highest) eigenvalues to define a k-dimensional subspace Form clusters in this subspace using k-means Similarity Graph We first create an undirected graph G = (V, E) with vertex set V = {v1, v2, ‚Ä¶, vn} = 1, 2, ‚Ä¶, n observations in the data. -neighbourhood Graph: : Each point is connected to all the points which lie in it‚Äôs -radius. If all the distances between any two points are similar in scale then typically the weights of the edges (i.e. the distance between the two points) are not stored since they do not provide any additional information. Hence, the graph built is an undirected and unweighted graph. K-Nearest Neighbours: : For two vertices and , an edge is directed from to only if is among the k-nearest neighbours of u. The graph is a weighted and directed graph because it is not always the case that for each u having v as one of the k-nearest neighbours, it will be the same case for v having u among its k-nearest neighbours. To make this graph undirected, one of the following approaches are followed: Direct an edge from u to v and from v to u if either v is among the k-nearest neighbours of u OR u is among the k-nearest neighbours of v. Direct an edge from u to v and from v to u if v is among the k-nearest neighbours of u AND u is among the k-nearest neighbours of v. Fully-Connected Graph: Each point is connected with an undirected edge-weighted by the distance between the two points to every other point. Since this approach is used to model the local neighbourhood relationships thus typically the Gaussian similarity metric is used to calculate the distance: Thus, when we create an adjacency matrix for any of these graphs, when the points are close and if the points are far apart.Consider the following graph with nodes 1 to 4, weights (or similarity) wij and its adjacency matrix: Adjacency Matrix Adjacency Matrix Affinity metric determines how close, or similar, two points our in our space. We will use a Gaussian Kernel and not the standard Euclidean metric. Given 2 data points (projected in ), we define an Affinity that is positive, symmetric, and depends on the Euclidian distance between the data points We might provide a hard cut off threshold , so that if when the points are close in , and if the points , are far apart. Close data points are in the same cluster. Data points in different clusters are far away. But data points in the same cluster may also be far away‚Äìeven farther away than points in different clusters. Our goal then is to transform the space so that when 2 points , are close, they are always in same cluster, and when they are far apart, they are in different clusters. Generally we use the Gaussian Kernel K directly, or we form the Graph Laplacian . Degree Matrix The degree matrix of a graph is the matrix defined by where of a vertex is the number of edges that terminate at Graph Laplacian The whole purpose of computing the Graph Laplacian was to find eigenvalues and eigenvectors for it, in order to embed the data points into a low-dimensional space. Just another matrix representation of a graph. It can be computed as: Simple Laplacian where is the Adjacency matrix and is the Degree Matrix Normalized Laplacian Generalized Laplacian Relaxed Laplacian Ng, Jordan, &amp; Weiss Laplacian , where The Cluster Eigenspace Problem To identify good clusters, Laplacian should be approximately a block-diagonal, with each block defining a cluster. If we have 3 major clusters (C1, C2, C3), we would expect - We also expect that the 3 lowest eigenvalues &amp; eigenvectors of each correspond to a different cluster. - For K clusters, compute the first K eigen vectors. . Stack the vectors vertically to form the matrix with eigen vecttors as columns. Represent every node as the corresponding row of this new matrix, these rows form the feature vector of the nodes. Use Kmeans to cluster these points into k clusters 2. Pros &amp; ConsPros Clusters not assumed to be any certain shape/distribution, in contrast to e.g. k-means. This means the spectral clustering algorithm can perform well with a wide variety of shapes of data. Works quite well when relations are approximately transitive (like similarity) Do not necessarily need the actual data set, just similarity/distance matrix, or even just Laplacian Because of this, we can cluster one dimensional data as a result of this; other algos that can do this are k-medoids and heirarchical clustering. Cons Need to choose the number of clusters k, although there is a heuristic to help choose Can be costly to compute, although there are algorithms and frameworks to help 3. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import SpectralClustering from sklearn.preprocessing import StandardScaler, normalize from sklearn.decomposition import PCA from sklearn.metrics import silhouette_scoreraw_df = pd.read_csv('CC GENERAL.csv')raw_df = raw_df.drop('CUST_ID', axis = 1) raw_df.fillna(method ='ffill', inplace = True) # Preprocessing the data to make it visualizable # Scaling the Data scaler = StandardScaler() X_scaled = scaler.fit_transform(raw_df) # Normalizing the Data X_normalized = normalize(X_scaled) # Converting the numpy array into a pandas DataFrame X_normalized = pd.DataFrame(X_normalized) # Reducing the dimensions of the data pca = PCA(n_components = 2) X_principal = pca.fit_transform(X_normalized) X_principal = pd.DataFrame(X_principal) X_principal.columns = ['P1', 'P2'] ## Affinity matrix with Gaussian Kernel## affinity = \"rbf\"# Building the clustering model spectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf') # Training the model and Storing the predicted cluster labels labels_rbf = spectral_model_rbf.fit_predict(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = SpectralClustering(n_clusters = 2, affinity ='rbf') .fit_predict(X_principal), cmap =plt.cm.winter) plt.show() ## Affinity matrix with Eucledean Distance## affinity = ‚Äònearest_neighbors‚Äô# Building the clustering model spectral_model_nn = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') # Training the model and Storing the predicted cluster labels labels_nn = spectral_model_nn.fit_predict(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') .fit_predict(X_principal), cmap =plt.cm.winter) plt.show() ### Evaluate performance# List of different values of affinity affinity = ['rbf', 'nearest-neighbours'] # List of Silhouette Scores s_scores = [] # Evaluating the performance s_scores.append(silhouette_score(raw_df, labels_rbf)) s_scores.append(silhouette_score(raw_df, labels_nn)) # Plotting a Bar Graph to compare the models plt.bar(affinity, s_scores) plt.xlabel('Affinity') plt.ylabel('Silhouette Score') plt.title('Comparison of different Clustering Models') plt.show() print(s_scores)","link":"/post/clustering-2/"},{"title":"Clustering: DBSCAN","text":"DBSCAN Introduction Density-based spatial clustering of applications with noise (DBSCAN) Summary: DBSCAN is a density-based clustering method that discovers clusters of nonspherical shape. Main Concept: Locate regions of high density that are separated from one another by regions of low density. It also marks as outliers the points that are in low-density regions. Implicit assumptions about the method: Densities across all the clusters are the same. Cluster sizes or standard deviations are the same. Density of region: Mainly defined by 2 parameters Density at a point P: Number of points within a circle of Radius from point P. Dense Region: For each point in the cluster, a circle with radius contains at least minimum number of points () The Epsilon neighborhood of a point P in the database D is defined as The function is usually defined by Euclidean Distance 3 classification of points: Core point: if the point has Border point: if the point has but it lies in the neighborhood of another Core point. Noise: any data point that is neither Core nor Border point Density Reachable/Density Connected/Directly Density Reachable Directly Density Reachable: Data-point is directly density reachable from a point if is a core point is in the epsilon neighborhood of Density Reachable: Data-point is density reachable from a point if For a chain of points , , is directly density reachable from . Density reachable is transitive in nature but, just like direct density reachable, it is not symmetric Density Connected: Data-point is density connected to a point if with respect to and there is a point such that, both and are density reachable from w.r.t. to and Procedure Starts with an arbitrary point which has not been visited and its neighborhood information is retrieved from the parameter. If this point contains neighborhood points, cluster formation starts.Otherwise the point is labeled as noise.- This point can be later found within the neighborhood of a different point and, thus can be made a part of the cluster. If a point is found to be a core point then the points within the neighborhood is also part of the cluster. So all the points found within neighborhood are added, along with their own neighborhood, if they are also core points. Continue the steps above (1-3) until the density-connected cluster is completely found. The process restarts with a new point which can be a part of a new cluster or labeled as noise. 2. Pros &amp; ConsPros Identifies randomly shaped clusters doesn‚Äôt necessitate to know the number of clusters in the data previously (as opposed to K-means) Handles noise Cons If the database has data points that form clusters of varying density, then DBSCAN fails to cluster the data points well, since the clustering depends on œµ and MinPts parameter, they cannot be chosen separately for all clusters May Overcome this issue by running additional rounds over large clusters If the data and features are not so well understood by a domain expert then, setting up and could be tricky Computational complexity ‚Äî when the dimensionality is high, it takes 3. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport numpy as npfrom scipy import statsfrom sklearn.cluster import DBSCANfrom sklearn.metrics import silhouette_score# To choose the best combination of the algorithm parameters I will first create a matrix of investigated combinations.from itertools import productmall_data = pd.read_csv('Mall_Customers.csv')X_numerics = mall_data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']] # subset with numeric variables onlyeps_values = np.arange(8,12.75,0.25) # eps values to be investigatedmin_samples = np.arange(3,10) # min_samples values to be investigatedDBSCAN_params = list(product(eps_values, min_samples))no_of_clusters = []sil_score = []for p in DBSCAN_params: DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_numerics) no_of_clusters.append(len(np.unique(DBS_clustering.labels_))) sil_score.append(silhouette_score(X_numerics, DBS_clustering.labels_))tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples']) tmp['No_of_clusters'] = no_of_clusterspivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')fig, ax = plt.subplots(figsize=(12,6))sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)ax.set_title('Number of clusters')plt.show()tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples']) tmp['Sil_score'] = sil_scorepivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')fig, ax = plt.subplots(figsize=(18,6))sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)plt.show()DBS_clustering = DBSCAN(eps=12.5, min_samples=4).fit(X_numerics)DBSCAN_clustered = X_numerics.copy()DBSCAN_clustered.loc[:,'Cluster'] = DBS_clustering.labels_ # append labels to pointsDBSCAN_clust_sizes = DBSCAN_clustered.groupby('Cluster').size().to_frame()DBSCAN_clust_sizes.columns = [\"DBSCAN_size\"]display(DBSCAN_clust_sizes)outliers = DBSCAN_clustered[DBSCAN_clustered['Cluster']==-1]fig2, (axes) = plt.subplots(1,2,figsize=(12,5))sns.scatterplot('Annual Income (k$)', 'Spending Score (1-100)', data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1], hue='Cluster', ax=axes[0], palette='Set1', legend='full', s=45)sns.scatterplot('Age', 'Spending Score (1-100)', data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1], hue='Cluster', palette='Set1', ax=axes[1], legend='full', s=45)axes[0].scatter(outliers['Annual Income (k$)'], outliers['Spending Score (1-100)'], s=5, label='outliers', c=\"k\")axes[1].scatter(outliers['Age'], outliers['Spending Score (1-100)'], s=5, label='outliers', c=\"k\")axes[0].legend()axes[1].legend()plt.setp(axes[0].get_legend().get_texts(), fontsize='10')plt.setp(axes[1].get_legend().get_texts(), fontsize='10')plt.show()","link":"/post/clustering-3/"},{"title":"Clustering: Affinity Propagation","text":"Affinity Propagation Introduction Developed recently (2007), a centroid based clustering algorithm similar to k Means or K medoids Affinity propagation finds \"exemplars\" i.e. members of the input set that are representative of clusters. It uses a graph based approach to let points 'vote' on their preferred 'exemplar'. The end result is a set of cluster 'exemplars' from which we derive clusters by essentially doing what K-Means does and assigning each point to the cluster of it's nearest exemplar. We need to calculate the following matrices: Here we must specify to notation: = row, = column Similarity matrix Responsibility matrix Availability matrix Criterion matrix Similarity matrix Rationale: information about the similarity between any instances, for an element i we look for another element j for which is the highest (least negative). Hence the diagonal values are all set to the most negative to exclude the case where i find i itself Barring those on the diagonal, every cell in the similarity matrix is calculated by the negative sum of the squares differences between participants. Note that the diagonal values will not be just 0: It is Responsibility matrix Rationale: quantifies how well-suited element k is, to be an exemplar for element i , taking into account the nearest contender k‚Äô to be an exemplar for i. We initialize R matrix with zeros. Then calculate every cell in the responsibility matrix using the following formula: Interpretation: R_{i,k} can be thought of as relative similarity between i and k. It quantifies how similar is i to k, compared to some k‚Äô, taking into account the availability of k‚Äô. The responsibility of k towards i will decrease as the availability of some other k‚Äô to i increases. Availability matrix Rationale: It quantifies how appropriate is it for i to choose k as its exemplar, taking into account the support from other elements that k should an exemplar. The Availability formula for different instances is The Self-Availability is Interpretation of the formulas Availability is self-responsibility of k plus the positive responsibilities of k towards elements other than i. We include only positive responsibilities as an exemplar should be positively responsible/explain at least for some data points well, regardless of how poorly it explains other data points. If self-responsibility is negative, it means that k is more suitable to belong to another exemplar, rather than being an exemplar. The maximum value of is 0. reflects accumulated evidence that point k is suitable to be an exemplar, based on the positive responsibilities of k towards other elements. and matrices are iteratively updated. This procedure may be terminated after a fixed number of iterations, after changes in the values obtained fall below a threshold, or after the values stay constant for some number of iterations. Criterion Matrix Criterion matrix is calculated after the updating is terminated. Criterion matrix is the sum of and : An element i will be assigned to an exemplar k which is not only highly responsible but also highly available to i. The highest criterion value of each row is designated as the exemplar. Rows that share the same exemplar are in the same cluster. Sample run Data Similarity Matrix Responsibility Matrix (First round) Availability Matrix (First round) Criterion Matrix 2. Pros &amp; ConsPros Does not need to specify the cluster number Allows for non-metric dissimilarities (i.e. we can have dissimilarities that don't obey the triangle inequality, or aren't symmetric) Providebetter stability over runs Cons Similar issue as K-means: susceptible to outliers Affinity Propagation tends to be very slow. In practice running it on large datasets is essentially impossible without a carefully crafted and optimized implementation 3. Code Implementation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from sklearn.cluster import AffinityPropagationfrom sklearn import metricsfrom sklearn.datasets import make_blobs# ############################################################################## Generate sample datacenters = [[1, 1], [-1, -1], [1, -1]]X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)# ############################################################################## Compute Affinity Propagationaf = AffinityPropagation(preference=-50).fit(X)cluster_centers_indices = af.cluster_centers_indices_labels = af.labels_n_clusters_ = len(cluster_centers_indices)print('Estimated number of clusters: %d' % n_clusters_)print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels, metric='sqeuclidean'))# ############################################################################## Plot resultimport matplotlib.pyplot as pltfrom itertools import cycleplt.close('all')plt.figure(1)plt.clf()colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')for k, col in zip(range(n_clusters_), colors): class_members = labels == k cluster_center = X[cluster_centers_indices[k]] plt.plot(X[class_members, 0], X[class_members, 1], col + '.') plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14) for x in X[class_members]: plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)plt.title('Estimated number of clusters: %d' % n_clusters_)plt.show()","link":"/post/clustering-4/"},{"title":"Clustering: Apriori","text":"Association RuleAssociation rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. For example, we may want to find 1-1 product category assocaition rule: product cateogry 1 -&gt; product category 2 This is often used for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. Because we don't have initial associations in our data, it is an unsupervised learning problem for marketing activities such as, e.g., promotional pricing or product placements. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. [Wikipedia] Evaluation Metrics1 Support : % of transactions where items in X AND Y are bought together Property of down-ward closure which means that all sub sets of a frequent set (support &gt; min. support threshold) are also frequent Cons: Items that occur very infrequently in the data set are pruned although they would still produce interesting and potentially valuable rules. Confidence : % of transactions amongst all customers who bought Y given that they have bought X While support is used to prune the search space and only leave potentially interesting rules, confidence is used in a second step to filter rules that exceed a min. confidence threshold Cons: sensitive to the frequency of the consequent (Y) in the data set. Caused by the way confidence is calculated, Ys with higher support will automatically produce higher confidence values even if they exists no association between the items. Lift An association rule X -&gt; Y is only useful if the lift value &gt; 1 Want to consider also the presence of Y being bought independently without knowledge about X Largely solves to problem of confidence threshold: sensitive to the frequency of the consequent (Y) Conviction : How poor can the association be. A directed measure monotone in confidence and lift. Leverage : difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically independent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells. Cons: suffer from the rare item problem. Apriori PropertyAll subsets of a frequent itemset must be frequent (Apriori propertry). If an itemset is infrequent, all its supersets will be infrequent. Applying the apriori property, we get the following algorithm. Algorithm Generating Support Value for Itemsets containing one items (One Itemset) With a pre-defined support threshold, identify itemsets worth exploring With the shortlisted One Itemset that are above the support threshold, generate Itemsets containing two items (Two Itemsets) With the same pre-definited support threshold, identify associations in Two Itemsets that are worth exploring With the shortlisted Two Itemsets, association rule is generated between the two items Confidence value is generated for each association rule With a pre-defined confidence threshold, association rules are being shortlisted With shortlisted association rules, the lift values are computed for each of them Only association rules with lift value &gt; 1 is considered as meaningful associations","link":"/post/clustering-5/"},{"title":"Dimension Reduction: Life savers","text":"OverviewHigh dimensional data modeling has always been a popular topic of discussion. Many research and work are done in this field simply because we have limited computational power. Even if quantum technology can greatly boost this power in near future, we will still face the curse of unlimited flow of data and features. Thus, it's actually extremely important that we reduce the amount of data input in a model. In this blog, we explore several well-known tools for dimensionality reduction, namly Linear Discriminant Analysis (LDA), Principle Component Analysis (PCA) and Nonnegative Matrix Factorization (NMF). LDA1. Definition Can be either a predictive modeling algorithm for multi-class classification or a dimensionality reduction technique A Generative Learning Algorithm based on labeled data Assumes Gaussian Distribution for ; Each attribute has the same variance (Mean removal/Feature Engineering with Log/Root functions/Box-Cox transformation needed) The class calculated from the discrimination function as having the largest value will be the output classification () LDA creates a new axis based on Maximize the distance between means Minimize the variations within each categories Procedure For Dimensionality Reduction (reduced-rank LDA) Compute the d-dimensional mean vectors for the different classes from the dataset. Compute the scatter matrices (in-between-class and within-class scatter matrix). Compute the eigenvectors (e1,e2,‚Ä¶,ed) and corresponding eigenvalues () for the scatter matrices. Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d√ók dimensional matrix W (where every column represents an eigenvector). Use this d√ók eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: Y=XW (where X is a n√ód-dimensional matrix representing the n samples, and y are the transformed n√ók-dimensional samples in the new subspace). For classification (Essentially, LDA classifies the sphered data to the closest class mean.) Perform eigen-decomposition on the pooled covariance matrix Spheres the data: to produce an identity covariance matrix in the transformed space Obtain group means in the transformed space: Classify according to : where is the group's prior probability Extensions to LDA Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables). Flexible Discriminant Analysis (FDA): Where non-linear combinations of inputs is used such as splines. Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA. 2. Pros &amp; ConsPros Need Less Data Simple prototype classifier: Distance to the class mean is used, it's simple to interpret. The decision boundary is linear: It's simple to implement and the classification is robust. Cons Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired. In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired. Support for more complex prototype classification is desired. 3. Application Bankruptcy prediction Facial recognition 4. Code implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from sklearn.datasets import load_wineimport pandas as pdimport numpy as npnp.set_printoptions(precision=4)from matplotlib import pyplot as pltimport seaborn as snssns.set()from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_split# Loading Datawine = load_wine()X = pd.DataFrame(wine.data, columns=wine.feature_names)y = pd.Categorical.from_codes(wine.target, wine.target_names)# Merge X and y (Training set)df = X.join(pd.Series(y, name='class'))## The Simply Way: using sklearnfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminantAnalysis()X_lda = lda.fit_transform(X, y)## The hard way: Understand the logic here# Compute means for each classclass_feature_means = pd.DataFrame(columns=wine.target_names)for c, rows in df.groupby('class'): class_feature_means[c] = rows.mean()class_feature_means# Compute the Within Class Scatter Matrix using the Mean Vectorwithin_class_scatter_matrix = np.zeros((13,13))for c, rows in df.groupby('class'): rows = rows.drop(['class'], axis=1) s = np.zeros((13,13))for index, row in rows.iterrows(): x, mc = row.values.reshape(13,1), class_feature_means[c].values.reshape(13,1) s += (x - mc).dot((x - mc).T) within_class_scatter_matrix += s# Compute the Between Class Scatter Matrix:feature_means = df.mean()between_class_scatter_matrix = np.zeros((13,13))for c in class_feature_means: n = len(df.loc[df['class'] == c].index) mc, m = class_feature_means[c].values.reshape(13,1), feature_means.values.reshape(13,1) between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)# Compute the Eigenvalues &amp; Eigenvectors, then sort accordinglyeigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]pairs = sorted(pairs, key=lambda x: x[0], reverse=True)# Print the Explained Varianceeigen_value_sums = sum(eigen_values)print('Explained Variance')for i, pair in enumerate(pairs): print('Eigenvector {}: {}'.format(i, (pair[0]/eigen_value_sums).real))# Identify the Principle Eigenvalues (here k = 2); Compute the new feature space X_ldaw_matrix = np.hstack((pairs[0][1].reshape(13,1), pairs[1][1].reshape(13,1))).realX_lda = np.array(X.dot(w_matrix))le = LabelEncoder()y = le.fit_transform(df['class']) # Here the y is just the encoded label set# Visualizeplt.xlabel('LD1')plt.ylabel('LD2')plt.scatter( X_lda[:,0], X_lda[:,1], c=y, cmap='rainbow', alpha=0.7, edgecolors='b') PCA1. Definition A linear model Basic intuition: projecting data onto its orthogonal feature subspace It is a technique for feature extraction ‚Äî it combines our input variables in a specific way, then we can drop the \"least important\" variables while still retaining the most valuable parts of all of the variables (high variance, independent, few number) Each of the \"new\" variables after PCA are all independent of one another (due to the linear model assumption) PCA effectively minimizes error orthogonal to the model itself It can only be applied to datasets which are linearly separable Complete procedure Standardize the data. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix (by performing eigendecomposition), or perform Singular Value Decomposition (SVD) [SVD preferred due to efficiency and automatic eigenvalue sorting] Find the hyperplane that accounts for most of the variation using SVD (that hyperplane represents 1st componet); This basically means it tries to minimize the points‚Äô distance to the hyperplane/maximize the distance from the projected point to the origin Find the orthogonal subspace, get from the subspace a best hyperplane with largest variation using SVD again (2nd component which is clearly independent of 1st component) Repeat to find all the components Use the eigenvalues to determine the proportion of variation that each component account for; construct the new system using the principle (top k) components; plot the samples using the projections on components as coordinates We should keep the component if it contributes substantially to the variation It eventually cluster samples that are highly correlated; It is possible to restore all the samples if each component correspond to one distinct variable (Note that # of PC &lt; # of Samples) Warning: scaling and centering data is very Important!!! Kernel PCA an extension of PCA into non-linear dataset by project dataset into a higher dimensional feature space using the kernel trick (recall SVM) Some popular kernels are Gaussian/RBF Polynomial Hyperbolic tangent: Note that the kernel matrix still need to be normalized for PCA to use kernel PCA so kernel PCA will have difficulties if we have lots of data points. Robust PCA an extension of PCA to deal with sparsity in the matrix It factorizes a matrix into the sum of 2 matrices, , where is the original matrix, is the low-rank (with lots of redundant information) matrix and is a sparse matrix (In the case of corrupted data, often captures the corrupted entries) Application: Latent semantic indexing =&gt; captures all common words while captures all key words that best identify each document. The minimization is over subject to . Minimizing L1-norm results in sparse values, while minimizing nuclear norm (sometimes also use Frobenious norm ) leads to sparse singular values (hence low rank) 2. Pros &amp; ConsPros Removes Correlated Features Improves Algorithm Performance Reduces Overfitting Cons Independent variables become less interpretable Data standardization is must before PCA Information Loss (if PCs chosen are not sufficient) 3. Application When interpretability is not an issue, use pca When the dimension is too large or you want to identify features that are independent, use pca 4. ComparisonPCA vs LDA Not as good as LDA in clustering/classification effect, yet idea for Factor analysis PCA projects the entire dataset onto a different feature (sub)space, and LDA tries to determine a suitable feature (sub)space in order to distinguish between patterns that belong to different classes PCA vs ICA In PCA the basis you want to find is the one that best explains the variability of your data; In ICA the basis you want to find is the one in which each vector is an independent component of your data (which usually has mixed signals/mixed features) 5. Code implementation12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns; sns.set(style='white')%matplotlib inline%config InlineBackend.figure_format = 'retina'from sklearn import decompositionfrom sklearn import datasetsfrom mpl_toolkits.mplot3d import Axes3Ddigits = datasets.load_digits()X = digits.datay = digits.target# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))plt.figure(figsize=(16, 6))for i in range(10): plt.subplot(2, 5, i + 1) plt.imshow(X[i,:].reshape([8,8]), cmap='gray') pca = decomposition.PCA(n_components=2)X_reduced = pca.fit_transform(X)print('Projecting %d-dimensional data to 2D' % X.shape[1])plt.figure(figsize=(12,10))plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, edgecolor='none', alpha=0.7, s=40, cmap=plt.cm.get_cmap('nipy_spectral', 10))plt.colorbar()plt.title('MNIST. PCA projection')pca = decomposition.PCA().fit(X)plt.figure(figsize=(10,7))plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)plt.xlabel('Number of components')plt.ylabel('Total explained variance')plt.xlim(0, 63)plt.yticks(np.arange(0, 1.1, 0.1))plt.axvline(21, c='b')plt.axhline(0.9, c='r')plt.show() NMF1. Definition It automatically extracts sparse and meaningful (easily interpretable) features from a set of nonnegative data vectors. We basically factorize into 2 smaller matrices non-negative and such that and ( low-rank approximate factorizations) Interpretation of and : Basically, we can interpret to be a weighted sum of some components, where each row in is a component, and each row in contains the weights of each component Idea of the algo: Formalize an objective function and iteratively optimize it A local minima is sufficient for the solution Objective function to minimize : Frobenius norm: w.r.t. s.t. Generalized Kullback-Leibler divergence: Choices of Optimization technique used: Coordinate descent (alternative: gradient descent which fix and optimize , then fix and optimize until tolerance is met) Multiplicative Update , Method to choose the optimal factorisation rank, : General guideline: Trial and error, Estimation using SVD based of the decay of the singular values Insights from experts Tricks Initialization: uses SVD to compute a rough estimate of the matrices and . If the non-negativity condition did not exist, taking the top k singular values and their corresponding vectors would construct the best rank k estimate, measured by the frobenius norm. Since and must be non-negative, we must slightly modify the vectors we use. Regularization: Since the represents weights of a component, it may produces weights that are too high/low. The classical way is to use or regularization losses 2. Application NMF is suited for tasks where the underlying factors can be interpreted as non-negative Image processing Topic Modeling Text mining Hyperspectral unmixing 3. Code Implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135from operator import itemgetterfrom concurrent.futures import ProcessPoolExecutor from time import timeimport os import gensimimport pandas as pdimport itertoolsimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefrom nltk.stem import WordNetLemmatizerfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.decomposition import NMFfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoderlemmatizer = WordNetLemmatizer()data_path = 'matrix_factorization_arxiv_query_result.json'articles_df = pd.read_json(data_path)articles_df.head()def stem(text): return lemmatizer.lemmatize(text)def map_parallel(f, iterable, **kwargs): with ProcessPoolExecutor() as pool: result = pool.map(f, iterable, **kwargs) return resultdef retrieve_articles(start, chunksize=1000): return arxiv.query( search_query=search_query, start=start, max_results=chunksize )def vectorize_text(examples_df, vectorized_column='summary', vectorizer=CountVectorizer): vectorizer = vectorizer(min_df=2) features = vectorizer.fit_transform(examples_df[vectorized_column]) le = LabelEncoder() ohe = OneHotEncoder() labels = le.fit_transform(valid_example_categories).reshape(-1, 1) labels_ohe = ohe.fit_transform(labels).todense() vectorized_data = { 'features': features, 'labels': labels, 'labels_onehot' : labels_ohe } return vectorized_data, (vectorizer, ohe, le)def extract_keywords(text): \"\"\" Use gensim's textrank-based approach \"\"\" return gensim.summarization.keywords( text=stem(text), lemmatize=True )def filter_out_small_categories(df, categories, threshold=200): class_counts = categories.value_counts() too_small_classes = class_counts[class_counts &lt; threshold].index too_small_classes valid_example_indices = ~categories.isin(too_small_classes) valid_examples = df[valid_example_indices] valid_example_categories = categories[valid_example_indices] return valid_examples, valid_example_categoriesdef print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): message = \"Topic #%d: \" % (topic_idx + 1) message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]) print(message) print()categories = articles_df['arxiv_primary_category'].apply(itemgetter('term'))main_categories = categories.apply(lambda s: s.split('.')[0].split('-')[0])main_categories_counts = main_categories.value_counts(ascending=True)main_categories_counts.plot.barh()plt.show()main_categories_counts[main_categories_counts &gt; 200].plot.barh()plt.show()categories.value_counts(ascending=True)[-10:].plot.barh()plt.show()articles_df['summary_keywords'] = [extract_keywords(i) for i in articles_df['summary']]article_keyword_lengths = articles_df['summary_keywords'].apply(lambda kws: len(kws.split('\\n')))valid_examples, valid_example_categories = filter_out_small_categories(articles_df, main_categories)vectorized_data, (vectorizer, ohe, le) = vectorize_text( valid_examples, vectorized_column='summary_keywords', vectorizer=TfidfVectorizer)x_train, x_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split( vectorized_data['features'], vectorized_data['labels_onehot'], vectorized_data['labels'], stratify=vectorized_data['labels'], test_size=0.2, random_state=0)nmf = NMF(n_components=5, solver='mu', beta_loss='kullback-leibler')topics = nmf.fit_transform(x_train)n_top_words = 10tfidf_feature_names = vectorizer.get_feature_names()print_top_words(nmf, tfidf_feature_names, n_top_words)dominant_topics = topics.argmax(axis=1) + 1categories = le.inverse_transform(y_train_labels[:,0])pd.crosstab(dominant_topics, categories)","link":"/post/dimentionality-reduction/"},{"title":"Ensemble Models: Overview","text":"OverviewAn important techinque in machine learning is ensemble models. It includes some very popular techniques like bootstraping and boosting. In the upcoming blogs, I will outline these models in detail, and give comparisons when necessary. The mathematical proofs are omitted for simplicity. However, I highly recommend interested readers to take a look at the theoretical foundations of these models to gain great intuitions about the ideas behind ensemble models. Intro An ensemble model is a composite model which combines a series of low performing or weak classifiers with the aim of creating a strong classifier. Here, individual classifiers vote and final prediction label returned that performs majority voting. Now, these individual classifiers are combined according to some specific criterion to create an ensemble model. These ensemble models offer greater accuracy than individual or base classifiers. These models can parallelize by allocating each base learner to different mechanisms. So, we can say that ensemble learning methods are meta-algorithms that combine several machine learning algorithms into a single predictive model to increase performance. Ensemble models are created according to some specific criterion as stated below: Bagging - They can be created to decrease model variance using bagging approach. Boosting - They can be created to decrease model bias using a boosting approach. Stacking - They can be created to improve model predictions using stacking approach. It can be depicted with the help of following diagram. Ensemble Machine Learning 1. Bagging Bagging stands for bootstrap aggregation. It combines multiple learners in a way to reduce the variance of estimates. For example, random forest trains N Decision Trees where we will train N different trees on different random subsets of the data and perform voting for final prediction. Bagging ensembles methods are Random Forest and Extra Trees. 2. Boosting Boosting algorithms are a set of the weak classifiers to create a strong classifier. Strong classifiers offer error rate close to 0. Boosting algorithm can track the model who failed the accurate prediction. Boosting algorithms are less affected by the overfitting problem. The following three algorithms have gained massive popularity in data science competitions AdaBoost (Adaptive Boosting) Gradient Tree Boosting (GBM) XGBoost We will discuss AdaBoost in this kernel and GBM and XGBoost in future posts. 3. Stacking Stacking (or stacked generalization) is an ensemble learning technique that combines multiple base classification models predictions into a new data set. This new data are treated as the input data for another classifier. This classifier employed to solve this problem. Stacking is often referred to as blending. Blending (average) ensemble model: Fits the base learners to the training data and then, at test time, average the predictions generated by all the base learners. Use VotingClassifier from sklearn that: fit all the base learners on the training data at test time, use all base learners to predict test data and then take the average of all predictions. Stacked ensemble model: Fits the base learners to the training data. Next, use those trained base learners to generate predictions (meta-features) used by the meta-learner (assuming we have only one layer of base learners). There are few different ways of training stacked ensemble model: Fitting the base learners to all training data and then generate predictions using the same training data it was used to fit those learners. This method is more prune to overfitting because the meta learner will give more weights to the base learner who memorized the training data better, i.e. meta-learner won't generate well and would overfit. Split the training data into 2 to 3 different parts that will be used for training, validation, and generate predictions. It's a suboptimal method because held out sets usually have higher variance and different splits give different results as well as learning algorithms would have fewer data to train. Use k-folds cross validation where we split the data into k-folds. We fit the base learners to the (k - 1) folds and use the fitted models to generate predictions of the held out fold. We repeat the process until we generate the predictions for all the k-folds. When done, refit the base learners to the full training data. This method is more reliable and will give models that memorize the data less weight. Therefore, it generalizes better on future data. 4. How are base-learners classified Base-learners are classified into two types. On the basis of the arrangement of base learners, ensemble methods can be divided into two groups. Parallel ensemble: base learners are generated in parallel for example - Random Forest. Sequential ensemble: base learners are generated sequentially for example AdaBoost. On the basis of the type of base learners, ensemble methods can be divided into two groups. Homogenous ensemble: uses the same type of base learner in each iteration. Heterogeneous ensemble: uses the different type of base learner in each iteration. Bagging vs Boosting1. Selecting the best technique- Bagging or Boosting Depends on the data, the simulation and the circumstances. Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability. If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model. By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn‚Äôt help to avoid over-fitting. In fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting. 2. Similarities between Bagging and Boosting Both are ensemble methods to get N learners from 1 learner. Both generate several training data sets by random sampling. Both make the final decision by averaging the N learners (or taking the majority of them i.e Majority Voting). Both are good at reducing variance and provide higher stability. 3. Differences between Bagging and Boosting Bagging is the simplest way of combining predictions that belong to the same type while Boosting is a way of combining predictions that belong to the different types. Bagging aims to decrease variance, not bias while Boosting aims to decrease bias, not variance. In Baggiing each model receives equal weight whereas in Boosting models are weighted according to their performance. In Bagging each model is built independently whereas in Boosting new models are influenced by performance of previously built models. In Bagging different training data subsets are randomly drawn with replacement from the entire training dataset. In Boosting every new subsets contains the elements that were misclassified by previous models. Bagging tries to solve over-fitting problem while Boosting tries to reduce bias. If the classifier is unstable (high variance), then we should apply Bagging. If the classifier is stable and simple (high bias) then we should apply Boosting. Bagging is extended to Random forest model while Boosting is extended to Gradient boosting.","link":"/post/ensemble-0/"},{"title":"Ensemble Models: Bagging Techniques","text":"OverviewWe have learnt about what bagging is in Ensemble Models: Overview, to recap, bagging is: In bagging (Bootstrap Aggregating), a set of weak learners are combined to create a strong learner that obtains better performance than a single one. Bagging helps to decrease the model‚Äôs variance. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.In this blog, we will use random forest as an example to illustrate how bagging worksBagging works as follows: Multiple subsets are created from the original dataset, selecting observations with replacement. A base model (weak model) is created on each of these subsets. The models run in parallel and are independent of each other. The final predictions are determined by combining the predictions from all the models. Next let's consider random forest, a model that fully utilized the idea of bagging in its procedure. Random Forest1. Definition A random forest consists of multiple random decision trees. Two types of randomnesses are built into the trees. First, each tree is built on a random sample from the original data. Second, at each tree node, a subset of features are randomly selected to generate the best split. (Key difference from Bagging algorithms) An ensemble model that is widely applied (as it can be parallelized) Designed to solve the overfitting issue in Decision Tree. The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias. Procedure Execute until every last combination is exhausted Bootstrapping: Create a bootstraped dataset: randomly select samples from the dataset until it reaches the same size as the original sample (we're allowed to pick the same sample more than once); Decision Tree Construction: Create a decision tree using the bootstraped dataset, but only use a random subset of variables/features at each step (i.e each decision node selection); Bagging: defined as bootstrapping the data plus using the aggregation to make a decision given a new instance, run through all the decision trees (the enitre random forest) and obtain the sum of votes for y = 1 and y = 0; (this step is called ‚Äúaggregation‚Äù) decide on result of aggregation using the result with higher vote; Choose the most accurate random forest: Measure Accuracy based on the Out-of-Bag samples (CV) compute the Out-of-Bag Error as the # of samples which Bagging classifies wrongly Choice of number of variable used per step affects accuracy (optimized during CV): Usually choose the square root of the # of total variables and try a few settings above and below that value The low correlation between models is the key WARNING: RF is often considered as Bagging model while it is not always true, see this link 2. Pros &amp; ConsPros The power of handle large data sets with higher dimensionality (as each tree select much less features in its construction) The model outputs importance of variable, which can be a very handy feature (rf.feature_importance_) Balancing errors in data sets where classes are imbalanced. It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing. Using the out of bag error estimate for selection the most accurate random forest removes the need for a set aside test set. Cons It has very poor interpretability Does not work well for extrapolation to predict for data that is outside of the bounds of your original training data Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds. 3. Simple ImplementationThis is a template inspired by the Kaggle notebooks. I shall thank those writers whose code I borrowed from. Also note that here an aws s3 connection is made, which automatically makes the process parallelized. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierRSEED = 50# Load in datadf = pd.read_csv('https://s3.amazonaws.com/projects-rf/clean_data.csv')# Full dataset: https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system# Extract the labelslabels = np.array(df.pop('label'))# 30% examples in test datatrain, test, train_labels, test_labels = train_test_split(df, labels, stratify = labels, test_size = 0.3, random_state = RSEED)# Imputation of missing valuestrain = train.fillna(train.mean())test = test.fillna(test.mean())# Features for feature importancesfeatures = list(train.columns)# Create the model with 100 treesmodel = RandomForestClassifier(n_estimators=100, random_state=RSEED, max_features = 'sqrt', n_jobs=-1, verbose = 1)# Fit on training datamodel.fit(train, train_labels)n_nodes = []max_depths = []# Stats about the trees in random forestfor ind_tree in model.estimators_: n_nodes.append(ind_tree.tree_.node_count) max_depths.append(ind_tree.tree_.max_depth) print(f'Average number of nodes {int(np.mean(n_nodes))}')print(f'Average maximum depth {int(np.mean(max_depths))}')# Training predictions (to demonstrate overfitting)train_rf_predictions = model.predict(train)train_rf_probs = model.predict_proba(train)[:, 1]# Testing predictions (to determine performance)rf_predictions = model.predict(test)rf_probs = model.predict_proba(test)[:, 1]from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curveimport matplotlib.pyplot as plt# Plot formattingplt.style.use('fivethirtyeight')plt.rcParams['font.size'] = 18def evaluate_model(predictions, probs, train_predictions, train_probs): \"\"\"Compare machine learning model to baseline performance. Computes statistics and shows ROC curve.\"\"\" baseline = {} baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))]) baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))]) baseline['roc'] = 0.5 results = {} results['recall'] = recall_score(test_labels, predictions) results['precision'] = precision_score(test_labels, predictions) results['roc'] = roc_auc_score(test_labels, probs) train_results = {} train_results['recall'] = recall_score(train_labels, train_predictions) train_results['precision'] = precision_score(train_labels, train_predictions) train_results['roc'] = roc_auc_score(train_labels, train_probs) for metric in ['recall', 'precision', 'roc']: print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}') # Calculate false positive rates and true positive rates base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))]) model_fpr, model_tpr, _ = roc_curve(test_labels, probs) plt.figure(figsize = (8, 6)) plt.rcParams['font.size'] = 16 # Plot both curves plt.plot(base_fpr, base_tpr, 'b', label = 'baseline') plt.plot(model_fpr, model_tpr, 'r', label = 'model') plt.legend(); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves'); plt.show();evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)","link":"/post/ensemble-1/"},{"title":"Ensemble Models: Boosting Techniques","text":"Overview Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model. Let‚Äôs understand the way boosting works in the below steps. A subset is created from the original dataset. Initially, all data points are given equal weights. A base model is created on this subset. This model is used to make predictions on the whole dataset. Errors are calculated using the actual values and predicted values. The observations which are incorrectly predicted, are given higher weights. (Here, the three misclassified blue-plus points will be given higher weights) Another model is created and predictions are made on the dataset. (This model tries to correct the errors from the previous model) Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble. We will discuss 3 major boosting models: AdaBoost, Gradient Boost and XGBoost. AdaBoost1. Definition AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Stump: a tree with only 1 node and 2 leaves; Generally stumps does not perform as good as forest does; The AdaBoost uses the forest of stumps AdaBoost should meet two conditions: The classifier should be trained interactively on various weighed training examples. In each iteration, it tries to provide an excellent fit for these examples by minimizing training error. Complete Procedure Assign each sample with a weight (initially set to equal weight) each row in Dataframe has a equal weight Use the feature selection in decision node method to choose the first stump; Measure how well a stump classifies the samples using: where is the weight of and is the set of misclassified datapoints Determine the vote significance for the stump using Laplace smoothing for the vote significance: in case Total Error = 1 or 0, the formula will return error, we add a small value in the formula Modify the weight of samples so that next stump will take the errors that current stump made into account: 6.1 Run each sample down the stump 6.2 Compute new weight using: Formula: = New Sample Weight = Current Sample weight. = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and = place holder for 1 if stump correctly classified, -1 if misclassified. 6.3 Normalize the new weights With the new sample weight we can either: Use Weighted Gini Index to construct the next stump (Best feature for split) Use a new set of sample derived from the previous sample: pick until number of samples reach the size of original set construct an interval-selection scheme using the sum of new sample weight as cutoff value if a number falls in i-th interval between (0,1), choose i-th sample; - e.g (0-0.07:1; 0.07-0.14:2; 0.14-0.60:3;0.60-0.67:4; etc) randomly generate a number x between 0 and 1 pick the sample according to the scheme (note that the same sample can be repeatly picked) Repeat Step 1 to 7 until the entire forest is built 2. Pros and ConsPros Achieves higher performance than bagging when hyper-parameters tuned properly. Can be used for classification and regression equally well. Easily handles mixed data types. Can use \"robust\" loss functions that make the model resistant to outliers. AdaBoost is easy to implement. We can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. Cons Difficult and time-consuming to properly tune hyper-parameters. Cannot be parallelized like bagging (bad scalability when vast amounts of data). More risk of overfitting compared to bagging. AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. Slower as compared to XGBoost 3. Comparison with Random Forest Random Forest VS AdaBoost (Bagging vs Boosting) Random Forest uses full grown trees while Adaboost uses stumps (one root node with two leafs) In a Random Forest all the trees have similar amount of say, while in Adaboost some trees have more say than the other. In a random forest the order of the tree does not matter, while in Adaboost the order is important (especially since each tree is built by taking the error of the previous error). 4. Sample Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pandas as pdimport numpy as npfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.model_selection import train_test_split,GridSearchCV, StratifiedShuffleSplitfrom sklearn.metrics import accuracy_scorefrom sklearn.preprocessing import StandardScalertrain = pd.read_pickle(\"train.pkl\")X = train.drop(['Survived'], axis = 1)y = train[\"Survived\"]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state=0)# Feature Scaling## We will be using standardscaler to transformst_scale = StandardScaler()## transforming \"train_x\"X_train = st_scale.fit_transform(X_train)## transforming \"test_x\"X_test = st_scale.transform(X_test)adaBoost = AdaBoostClassifier(base_estimator=None, learning_rate=1.0, n_estimators=100)adaBoost.fit(X_train, y_train)y_pred = adaBoost.predict(X_test)accuracy_score(y_test, y_pred)n_estimators = [100,140,145,150,160, 170,175,180,185];cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)learning_r = [0.1,1,0.01,0.5]parameters = {'n_estimators':n_estimators, 'learning_rate':learning_r }grid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree. ), param_grid=parameters, cv=cv, n_jobs = -1)grid.fit(X,y) print (grid.best_score_)print (grid.best_params_)print (grid.best_estimator_)adaBoost_grid = grid.best_estimator_adaBoost_grid.score(X,y) GBM (Gradient Boosting) Gradient Boosting trains many models in a gradual, additive and sequential manner (sequential + homogeneous). Major Motivation: allows one to optimise a user specified cost function, instead of a loss function that usually offers less control and does not essentially correspond with real world applications. Main logic: utilizes the gradient descent to pinpoint the challenges in the learners' predictions used previously. The previous error is highlighted, and, by combining one weak learner to the next learner, the error is reduced significantly over time. Procedure: For Regression Start by Compute the average of the , this is our 'initial prediction' for every sample Then compute the ; : Pseudo Residual at i-th sample : True value of i-th sample : Estimated value of i-th sample (here in first iteration) Construct a new decision tree (fixed size) with the goal of predicting the residuals (a DT of , not the true value!!!) If in a leaf, # of leaves &lt; # of samples, then put the of samples that fall into same category into the same leaf; Then take average of all values on that leaf as output values; Compute the new predicted value () : Newly Estimated value of i-th sample : learning rate, usually between 0 ~ 0.1 : Estimated pseudo residual values (deduced from the decision tree) Compute the new of each sample = Construct the new tree with the new pseudo residual in step 5: Repeat step 2, 3 Compute the new predicted value (here is deduced from a new DT): Compute the new pseudo residual of each sample Loop through the process UNTIL: adding additional trees does not significantly reduce the size of the pseudo residuals For Classification Set the initial prediction for every sample using ( is th probability of a sample being classified as 1) : (same for every sample) : log(odds) prediction for i-th sample, initially the same value, but value for each sample will change upon future iterations Using logistic function for classification: ; Decide on the classification: if &gt; threshold, then \"Yes\"; else \"No\"; here the threshold may not be 0.5 (AUC and ROC to decide on the value); Compute Build a DT using the pseudo residual Transformation of the pseudo residual to obtain the output values on each leaf: e.g: if a leaf has (0.3, -0.7), then the leaf output value Compute the new prediction : log(odds) prediction for i-th sample in new iteration : learning rate, usually between 0 ~ 0.1 Compute the new Probability Compute the new predicted value for each sample; Compute the new pseudo residual for each sample; Build the new tree; Loop until the pseudo residual does not change significantly; Early Stopping: Early Stopping performs model optimisation by monitoring the model‚Äôs performance on a separate test data set and stopping the training procedure once the performance on the test data stops improving beyond a certain number of iterations. It avoids overfitting by attempting to automatically select the inflection point where performance on the test dataset starts to decrease while performance on the training dataset continues to improve as the model starts to overfit. In the context of gbm, early stopping can be based either on an out of bag sample set (\"OOB\") or cross- validation (\"cv\"). 1. Pros &amp; ConsPros Robust against bias/outliers GBM can be used to solve almost all objective function that we can write gradient out, some of which RF cannot resolve Able to reduce bias and remove some extreme variances Cons More sensitive to overfitting if the data is noisy GBDT training generally takes longer because of the fact that trees are built sequentially Prone to overfitting, but can be overcame by parameter optimization 2. AdaBoost vs GBM Both AdaBoost and Gradient Boosting build weak learners in a sequential fashion. Originally, AdaBoost was designed in such a way that at every step the sample distribution was adapted to put more weight on misclassified samples and less weight on correctly classified samples. The final prediction is a weighted average of all the weak learners, where more weight is placed on stronger learners. Later, it was discovered that AdaBoost can also be expressed as in terms of the more general framework of additive models with a particular loss function (the exponential loss). So, the main differences between AdaBoost and GBM are as follows: The main difference therefore is that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function (Exponential loss function). Hence, gradient boosting is much more flexible. AdaBoost can be interepted from a much more intuitive perspective and can be implemented without the reference to gradients by reweighting the training samples based on classifications from previous learners. In Adaboost, shortcomings are identified by high-weight data points while in Gradient Boosting, shortcomings of existing weak learners are identified by gradients. Adaboost is more about 'voting weights' and Gradient boosting is more about 'adding gradient optimization'. Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances. AdaBoost use simple stumps as learners, while the fixed size trees of GBM are usually of maximum leaf number between 8 and 32; Adaboost corrects its previous errors by tuning the weights for every incorrect observation in every iteration, but gradient boosting aims at fitting a new predictor in the residual errors committed by the preceding predictor. 3. Random Forest vs GBM GBMs are harder to tune than RF. There are typically three parameters: number of trees, depth of trees and learning rate, and each tree built is generally shallow. RF is harder to overfit than GBM. RF runs in parallel while GBM runs in sequence 4. Application A great application of GBM is anomaly detection in supervised learning settings where data is often highly unbalanced such as DNA sequences, credit card transactions or cybersecurity. 5. Sample Code implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auctrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")train.set_index(\"PassengerId\", inplace=True)test.set_index(\"PassengerId\", inplace=True)# generate training target set (y_train)y_train = train[\"Survived\"]# delete column \"Survived\" from train settrain.drop(labels=\"Survived\", axis=1, inplace=True)train_test = train.append(test)# delete columns that are not used as features for training and predictioncolumns_to_drop = [\"Name\", \"Age\", \"SibSp\", \"Ticket\", \"Cabin\", \"Parch\", \"Embarked\"]train_test.drop(labels=columns_to_drop, axis=1, inplace=True)# convert objects to numbers by pandas.get_dummiestrain_test_dummies = pd.get_dummies(train_test, columns=[\"Sex\"])train_test_dummies.fillna(value=0.0, inplace=True)# generate feature sets (X)X_train = train_test_dummies.values[0:891]X_test = train_test_dummies.values[891:]scaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)X_test_scale = scaler.transform(X_test)X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0)learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]for learning_rate in learning_rates: gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0) gb.fit(X_train_sub, y_train_sub) print(\"Learning rate: \", learning_rate) print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub))) print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub))) print() gb = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 2, random_state = 0)gb.fit(X_train_sub, y_train_sub)predictions = gb.predict(X_validation_sub)print(\"Confusion Matrix:\")print(confusion_matrix(y_validation_sub, predictions))print()print(\"Classification Report\")print(classification_report(y_validation_sub, predictions))y_scores_gb = gb.decision_function(X_validation_sub)fpr_gb, tpr_gb, _ = roc_curve(y_validation_sub, y_scores_gb)roc_auc_gb = auc(fpr_gb, tpr_gb) XGBoost An optimized GBM Evolution of XGBoost from Decision Tree Procedure: For Regression Set initial value (by default is 0.5 [for both regression and classification]) Build the first XGBoost Tree (a unique type of regression tree): Start with a root containing all the residuals ; Compute similarity score where is the Regularization parameter. Make a decision on spliting condition: For each consecutive samples, compute the mean k of 2 input as the threshold for decision node; then split by the condition feature_value &lt; k Decide the best thresold for spliting: Adopt the threshold that gives the largest Gain For example: we have points {}: Step 1: set the first threshold be Step 2: now left node has {x1}, right node has {x2, x3}, we have and Step 3: Compute Step 4: Compute the second threshold and new Gain using this thresold Step 5: Since gain of threshold 1 is greater than that of threshold 2, we use as the spliting threshold If the leaf after spliting has &gt; 1 residual, consider whether to split again (based on the residuals in the leaf); continue until it reaches the max_depth (default is 6) or no more spliting is possible Notes on A larger leads to greater likelihood of prunning as the are lower; The reduce the prediction‚Äôs sensitivity to nodes with low # of observations Prune the Tree From bottom branch up, decide on whether to prune the node/branch : The threshold to determine if a Gain is large enough to be kept if then prune (remove the branch); Note that setting does not turn off prunnig!!! If we prune every branch until it reaches the root, then remove the tree; Compute Compute New prediction : Learning rate, default value = 0.3 : Output value of in each residual tree Compute the new residuals for all samples, build the next tree and prune the tree; Repeat the process just like Gradient Boost does; As more trees are built, the Gains will decease; We stop until the Gain &lt; terminating value For Classification Set initial value (by default is 0.5) Build the first XGBoost Tree: Start with a root containing all the residuals Compute similarity score where is the Regularization parameter. Repeat the same procedure as the regression does; Compute all the Gains Warning of Cover: defined for the minimum number of residuals in each leaf (by default is 1) in Regression: = # of Residuals in the leaf (always &gt;= 1) in Classification: = (not necessarily &gt;= 1), hence some leafs violating the Cover threshold will be removed. Here Cover needs to be carefully chosen (like 0, 0.1, etc) Prune the tree: same procedure as Regression case Compute Compute New prediction : Learning rate, default value = 0.3 : Output value of in each residual tree (here is the first tree) Convert into using logistic regression: Compute the new residuals for all samples, build the next tree and prune the tree; Repeat the process just like Gradient Boost does; As more trees are built, the Gains will decease; We stop until the Gain &lt; terminating value; Prediction: ; 1. Advantage of XGBoost Parallelized Tree Building Unlike GBM, XGBoost is able to build the sequential tree using a parallelized implementation This is possible due to the interchangeable nature of loops used for building base learners: the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation. Tree Pruning using depth-first approach The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses 'max_depth' parameter as specified instead of criterion first, and starts pruning trees backward. (This 'depth-first' approach improves computational performance significantly.) Cache awareness and out-of-core computing allocating internal buffers in each thread to store gradient statistics. Further enhancements such as 'out-of-core' computing optimize available disk space while handling big data-frames that do not fit into memory. Regularization It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting. Efficient Handling of missing data XGboost decides at training time whether missing values go into the right or left node. It chooses which to minimise loss. If there are no missing values at training time, it defaults to sending any new missings to the right node. In-built cross-validation capability The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run. LightGBM A follow-up (and competitor) from XGBoost Generally the same as GBM, except that a lot of optimizations are done, see this page to view all of them Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. It is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. 1. Advantages of Light GBM Faster training speed and higher efficiency: Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. Lower memory usage: Replaces continuous values to discrete bins which result in lower memory usage. Better accuracy than any other boosting algorithm: It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter. Compatibility with Large Datasets: It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. Parallel learning supported 2. Code sample: XGBoost vs LightGBM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#importing standard libraries import numpy as np import pandas as pd from pandas import Series, DataFrame #import lightgbm and xgboost import lightgbm as lgb import xgboost as xgb #loading our training dataset 'adult.csv' with name 'data' using pandas data=pd.read_csv('adult.csv',header=None) #Assigning names to the columns data.columns=['age','workclass','fnlwgt','education','education-num','marital_Status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','Income'] #glimpse of the dataset data.head() # Label Encoding our target variable from sklearn.preprocessing import LabelEncoder,OneHotEncoderl=LabelEncoder() l.fit(data.Income) l.classes_ data.Income=Series(l.transform(data.Income)) #label encoding our target variable data.Income.value_counts() #One Hot Encoding of the Categorical features one_hot_workclass=pd.get_dummies(data.workclass) one_hot_education=pd.get_dummies(data.education) one_hot_marital_Status=pd.get_dummies(data.marital_Status) one_hot_occupation=pd.get_dummies(data.occupation)one_hot_relationship=pd.get_dummies(data.relationship) one_hot_race=pd.get_dummies(data.race) one_hot_sex=pd.get_dummies(data.sex) one_hot_native_country=pd.get_dummies(data.native_country) #removing categorical features data.drop(['workclass','education','marital_Status','occupation','relationship','race','sex','native_country'],axis=1,inplace=True) #Merging one hot encoded features with our dataset 'data' data=pd.concat([data,one_hot_workclass,one_hot_education,one_hot_marital_Status,one_hot_occupation,one_hot_relationship,one_hot_race,one_hot_sex,one_hot_native_country],axis=1) #removing dulpicate columns _,i = np.unique(data.columns, return_index=True) data=data.iloc[:, i] #Here our target variable is 'Income' with values as 1 or 0. #Separating our data into features dataset x and our target dataset y x=data.drop('Income',axis=1) y=data.Income #Imputing missing values in our target variable y.fillna(y.mode()[0],inplace=True) #Now splitting our dataset into test and train from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3)#The data is stored in a DMatrix object #label is used to define our outcome variabledtrain=xgb.DMatrix(x_train,label=y_train)dtest=xgb.DMatrix(x_test)#setting parameters for xgboostparameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}#training our model num_round=50from datetime import datetime start = datetime.now() xg=xgb.train(parameters,dtrain,num_round) stop = datetime.now()#Execution time of the model execution_time_xgb = stop-start print(f'execution_time_xgb: {execution_time_xgb}')#datetime.timedelta( , , ) representation $\\implies$ (days , seconds , microseconds) #now predicting our model on test set ypred=xg.predict(dtest) display(ypred)#Converting probabilities into 1 or 0 for i in range(0, 9769): if ypred[i] &gt;= .5: ypred[i] = 1 else: ypred[i]=0 #calculating accuracy of our model from sklearn.metrics import accuracy_score accuracy_xgb = accuracy_score(y_test,ypred) print(f'accuracy_xgb: {accuracy_xgb}')train_data=lgb.Dataset(x_train,label=y_train)#setting parameters for lightgbmparam = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}param['metric'] = ['auc', 'binary_logloss']#Here we have set max_depth in xgb and LightGBM to 7 to have a fair comparison between the two.#training our model using light gbmnum_round=50start=datetime.now()lgbm=lgb.train(param,train_data,num_round)stop=datetime.now()#Execution time of the modelexecution_time_lgbm = stop-startprint(f'execution_time_lgbm: {execution_time_lgbm}')#predicting on test setypred2=lgbm.predict(x_test)display(ypred2[0:5]) # showing first 5 predictions#converting probabilities into 0 or 1for i in range(0,9769): if ypred2[i]&gt;=.5: # setting threshold to .5 ypred2[i]=1 else: ypred2[i]=0 #calculating accuracyaccuracy_lgbm = accuracy_score(ypred2,y_test)print(f'accuracy_lgbm: {accuracy_lgbm}')display(y_test.value_counts())from sklearn.metrics import roc_auc_score#calculating roc_auc_score for xgboostauc_xgb = roc_auc_score(y_test,ypred)print(f'auc_xgb: {auc_xgb}')#calculating roc_auc_score for light gbm. auc_lgbm = roc_auc_score(y_test,ypred2)print(f'auc_lgbm: {auc_lgbm}')comparison_dict = {'accuracy score':(accuracy_lgbm, accuracy_xgb),'auc score':(auc_lgbm,auc_xgb),'execution time':(execution_time_lgbm, execution_time_xgb)}#Creating a dataframe ‚Äòcomparison_df‚Äô for comparing the performance of Lightgbm and xgb. comparison_df = DataFrame(comparison_dict) comparison_df.index= ['LightGBM','xgboost'] display(comparison_df) 3. General Pros and cons of boostingPros Achieves higher performance than bagging when hyper-parameters tuned properly. Can be used for classification and regression equally well. Easily handles mixed data types. Can use ‚Äúrobust‚Äù loss functions that make the model resistant to outliers. Cons Difficult and time consuming to properly tune hyper-parameters. Cannot be parallelized like bagging (bad scalability when huge amounts of data). More risk of overfitting compared to bagging. ConclusionHere we end the discussion about ensemble models. It was a fun and challenging topic. While most users of these model won't need to understand every nitty-gritty of these models, these profound theories laid significant foundations for future research on supervised ensemble learning models (and even meta-learning). In the next month, I'll share some posts about unsupervised learning. This is even large a topic, and I expect the content to be even deeper. Good luck, me and everyone!","link":"/post/ensemble-2/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it‚Äôs the task of minimizing the cost/loss function parameterized by the model‚Äôs parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That‚Äôs usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters‚Äô initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we‚Äôll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let‚Äôs first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let‚Äôs assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn‚Äôt converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it‚Äôs on very different scales. If we don‚Äôt scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don‚Äôt have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it‚Äôs negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn‚Äôt change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it‚Äôs perpendicular to level curves at each step. Now let‚Äôs discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter‚Äôs update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don‚Äôt contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don‚Äôt contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won‚Äôt converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don‚Äôt usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can‚Äôt utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent‚Äôs variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn‚Äôt take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients‚Äô norm is increasing, we‚Äôre able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent%20copy%202/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it‚Äôs the task of minimizing the cost/loss function parameterized by the model‚Äôs parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That‚Äôs usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters‚Äô initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we‚Äôll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let‚Äôs first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let‚Äôs assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn‚Äôt converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it‚Äôs on very different scales. If we don‚Äôt scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don‚Äôt have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it‚Äôs negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn‚Äôt change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it‚Äôs perpendicular to level curves at each step. Now let‚Äôs discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter‚Äôs update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don‚Äôt contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don‚Äôt contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won‚Äôt converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don‚Äôt usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can‚Äôt utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent‚Äôs variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn‚Äôt take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients‚Äô norm is increasing, we‚Äôre able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent%20copy%203/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it‚Äôs the task of minimizing the cost/loss function parameterized by the model‚Äôs parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That‚Äôs usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters‚Äô initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we‚Äôll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let‚Äôs first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let‚Äôs assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn‚Äôt converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it‚Äôs on very different scales. If we don‚Äôt scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don‚Äôt have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it‚Äôs negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn‚Äôt change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it‚Äôs perpendicular to level curves at each step. Now let‚Äôs discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter‚Äôs update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don‚Äôt contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don‚Äôt contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won‚Äôt converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don‚Äôt usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can‚Äôt utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent‚Äôs variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn‚Äôt take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients‚Äô norm is increasing, we‚Äôre able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent%20copy/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it‚Äôs the task of minimizing the cost/loss function parameterized by the model‚Äôs parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That‚Äôs usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters‚Äô initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we‚Äôll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let‚Äôs first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let‚Äôs assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn‚Äôt converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it‚Äôs on very different scales. If we don‚Äôt scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don‚Äôt have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it‚Äôs negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn‚Äôt change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it‚Äôs perpendicular to level curves at each step. Now let‚Äôs discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter‚Äôs update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don‚Äôt contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don‚Äôt contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won‚Äôt converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don‚Äôt usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can‚Äôt utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent‚Äôs variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn‚Äôt take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients‚Äô norm is increasing, we‚Äôre able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent/"},{"title":"manual","text":"","link":"/post/manual/"},{"title":"Recommender Systems: I. Content-Based Filtering And Collaborative Filtering","text":"OverviewThe rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user. They are used to predict the rating or preference that a user would give to an item. Almost every major tech company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. Moreover, companies like Netflix and Spotify depend highly on the effectiveness of their recommendation engines for their business and success. Traditional recommender system modelsThere are basically three types of traditional recommender systems, let's use the example of movie recommendation (e.g. Netflix): Demographic Filtering: They offer generalized recommendations to every user, based on movie popularity and/or genre. The System recommends the same movies to users with similar demographic features. Since each user is different , this approach is considered to be too simple. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience. Content Based Filtering: They suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it. Collaborative Filtering: This system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like its content-based counterparts. In later blogs, we will talk about more recent models for recommender systems, including factorization machines and deep learning based models. Content Based Filtering1. Definition Use additional information about users and/or items. Example: User features: age, the sex, the job or any other personal information Exmaple: Item features: the category, the main actors, the duration or other characteristics for the movies. Main idea: given the set of features (both User and Item), apply a method to identify the model that explain the observed user-item interactions. Content Flow Little concern about \"Cold Start\": new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities One key tool used: Term Frequency-Inverse Document Frequency (TF-IDF): TF: the frequency of a word in a document IDF: the inverse of the document frequency among the whole corpus of documents. log: log function is taken to dampen the effect of high frequency word (0 vs 100 0 vs 2 (log100)) Note that normalization is needed before we apply TF-IDF because the initial feature map are all 1's and 0's, but the log function will remove all these differentiation. In the end the TF score will just be 1/0 2. Limitation They are not good at capturing inter-dependence or complex behaviours. For example: A user may prefer gaming + tv the most while a pure tv is not really his favourate. 3. Code Sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as np # linear algebraimport pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.pairwise import linear_kernelbooks = pd.read_csv('goodread/books.csv', encoding = \"ISO-8859-1\")ratings = pd.read_csv('goodread/ratings.csv', encoding = \"ISO-8859-1\")book_tags = pd.read_csv('goodread/book_tags.csv', encoding = \"ISO-8859-1\")tags = pd.read_csv('goodread/tags.csv')tags_join_DF = pd.merge(book_tags, tags, left_on='tag_id', right_on='tag_id', how='inner')to_read = pd.read_csv('goodread/to_read.csv')tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix = tf.fit_transform(books['authors'])cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)titles = books['title']indices = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of book authorsdef authors_recommendations(title): idx = indices[title] sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]books_with_tags = pd.merge(books, tags_join_DF, left_on='book_id', right_on='goodreads_book_id', how='inner')tf1 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix1 = tf1.fit_transform(books_with_tags['tag_name'].head(10000))cosine_sim1 = linear_kernel(tfidf_matrix1, tfidf_matrix1)# Build a 1-dimensional array with book titlestitles1 = books['title']indices1 = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of books tagsdef tags_recommendations(title): idx = indices1[title] sim_scores = list(enumerate(cosine_sim1[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]temp_df = books_with_tags.groupby('book_id')['tag_name'].apply(' '.join).reset_index()books = pd.merge(books, temp_df, left_on='book_id', right_on='book_id', how='inner')books['corpus'] = (pd.Series(books[['authors', 'tag_name']] .fillna('') .values.tolist() ).str.join(' '))tf_corpus = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix_corpus = tf_corpus.fit_transform(books['corpus'])cosine_sim_corpus = linear_kernel(tfidf_matrix_corpus, tfidf_matrix_corpus)# Build a 1-dimensional array with book titlestitles = books['title']indices = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of books tagsdef corpus_recommendations(title): idx = indices1[title] sim_scores = list(enumerate(cosine_sim_corpus[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]corpus_recommendations(\"The Hobbit\")corpus_recommendations(\"Twilight (Twilight, #1)\") Collaborative Filtering1. Definition Based solely on the past interactions recorded between users and items in order to produce new recommendations Main idea: Past user-item interactions are sufficient to detect similar users and/or similar items and make predictions based on these estimated proximities. Every user and item is described by a feature vector or embedding. It creates embedding for both users and items on its own. It embeds both users and items in the same embedding space. 2 Major Types: Memory Based Users and items are represented directly by their past interactions (large sparce vector) Recommendations are done following nearest neighbour information No latent model is assumed Theoretically a low bias but a high variance. Usualy recommend those items with high rating for a user : : the rating given to by user : users similar to / items similar to : respective ratings : similarity score for -th item/user similar to / (deduced using the similarity metircs shown below) Similarity Metrics: Cosine Similarity Dot Product Euclidean distance Pearson Similarity: Limitations: Don't scale easily KNN algorithm has a complexity of O(ndk) Users may easily fall into a \"information confinement area\" which only give too precise/general information Overcome Limitation: Use Approximate nearest neighbour (ANN) or take advantage of sparse matrix 2 Types: User-User: Identify users with the most similar \"interactions profile\" (nearest neighbours) in order to suggest items that are the most popular among these neighbours (and that are \"new\" to our user). We consider that two users are similar if they have interacted with a lot of common items in the same way (similar rating, similar time hovering‚Ä¶). Prevents overfitting As, in general, every user have only interacted with a few items, it makes the method pretty sensitive to any recorded interactions (high variance) Only based on interactions recorded for users similar to our user of interest, we obtain more personalized results (low bias) Item-Item: Find items similar to the ones the user already \"positively\" interacted with Two items are considered to be similar if most of the users that have interacted with both of them did it in a similar way. A lot of users have interacted with an item, the neighbourhood search is far less sensitive to single interactions (lower variance) Interactions coming from every kind of users are then considered in the recommendation, making the method less personalised (more biased) VS User-User: Less personalized, but more robust Model Based New reprensentations of users and items are build based on a model (small dense vectors) The model \"derives\" the relevant features of the user-item interactions Recommendations are done following the model information May contain interpretability issue Theoretically a higher bias but a lower variance 3 Types: Clustering Simple KNN/ANN will do on these metrices Matrix Factorization Main assumption: There exists a very low dimensional latent space of features in which we can represent both users and items and such that the interaction between a user and an item can be obtained by computing the dot product of corresponding dense vectors in that space. Generate the factor matrices as feature matrices for users and items. Idea: : Interaction matrix of ratings, usually sparse : User matrix : Item matrix : the dimension of the latent space Advanced Factorization methods: SVD: Not so well due to the sparsity of matrix : is item matrix; is user matrix WMF (Weighted Matrix Factorization) Weight applied to rated/non-rated entries Similar to NMF but also consider non-rated ones by associating a weight to each entry NMF: Uses only the observed or rated one Performs well with sparse matrices where indicates the -th item rated by -th user Minimizing the objective function Most common: Weighted Alternating Least Squares Formula: Regularized minimization of ‚Äúrating reconstruction error‚Äù Optimization process via Gradient Descent (Reduce runtime by batch running) Instead of solving for and together, we alternate between the above two equations. Fixing and solving for Fixing and solving for This algorithm gives us an approximated result (two equations are not convex at the same time can't reach a global minimum local minimum close to the global minimum) For a fixed set of users and items, new interactions recorded over time bring new information and make the system more and more effective. Solution to \"Cold Start\" problem: Heuristics to generate embeddings for fresh items Recommending random items to new users/recommend new item to random users Recommending popular items to new usres/recommend new items to most active users Recomeending a set of various items to new users or a new item to a set of various users Use a non collaborative method for early life of the user/item Projection in WALS (given current optimal and ) 2. Pros &amp; ConsPros Require no information about users or items (more versatile) Cons Cold Start problem: Impossible to recommend anything to new users or to recommend a new item to any users Many users or items have too few interactions to be efficiently handled. 3. Comparison with Content Based Method Content based methods suffer far less from the cold start problem than collaborative approaches CB is much more constrained (because representation of users and/or items are given) CB tends to have the highest bias but the lowest variance 4. Code samples123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Import librariesimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlineimport warningswarnings.filterwarnings('ignore')import osfrom textwrap import wrap# Read the input training datainput_data_file_movie = \"movie.csv\"input_data_file_rating = \"rating.csv\"movie_data_all = pd.read_csv(input_data_file_movie)rating_data_all = pd.read_csv(input_data_file_rating)# Keep only required columnsmovie_data_all = movie_data_all.drop(['genres'], axis=1)rating_data_all = rating_data_all.drop(['timestamp'], axis=1)# Pick all ratings#num_ratings = 2000000rating_data = rating_data_all.iloc[:, :]movie_rating_merged_data = movie_data.merge(rating_data, on='movieId', how='inner')movie_rating_merged_pivot = pd.pivot_table(movie_rating_merged_data, index=['title'], columns=['userId'], values=['rating'], dropna=False, fill_value=0 )# Create a matrix R, such that, R(i,j) = 1 iff User j has selected a rating for Movie i. R(i,j) = 0 otherwise.R = np.ones(Y.shape)no_rating_idx = np.where(Y == 0.0)# Assign n_m (number of movies), n_u (number of users) and n_f (number of features)n_u = Y.shape[1]n_m = Y.shape[0]n_f = 2 # Because we want to cluster movies into 2 genres# Setting random seed to reproduce results laternp.random.seed(7)Initial_X = np.random.rand(n_m, n_f)Initial_Theta = np.random.rand(n_u, n_f)# Cost Functiondef collabFilterCostFunction(X, Theta, Y, R, reg_lambda): cost = 0 error = (np.dot(X, Theta.T) - Y) * R error_sq = np.power(error, 2) cost = np.sum(np.sum(error_sq)) / 2 cost = cost + ((reg_lambda/2) * ( np.sum(np.sum((np.power(X, 2)))) + np.sum(np.sum((np.power(Theta, 2)))))) return cost# Gradient Descentdef collabFilterGradientDescent(X, Theta, Y, R, alpha, reg_lambda, num_iters): cost_history = np.zeros([num_iters, 1]) for i in range(num_iters): error = (np.dot(X, Theta.T) - Y) * R X_grad = np.dot(error, Theta) + reg_lambda * X Theta_grad = np.dot(error.T, X) + reg_lambda * Theta X = X - alpha * X_grad Theta = Theta - alpha * Theta_grad cost_history[i] = collabFilterCostFunction(X, Theta, Y, R, reg_lambda) return X, Theta, cost_history# Tune hyperparametersalpha = 0.0001num_iters = 100000reg_lambda = 1# Perform gradient descent to find optimal parametersX, Theta = Initial_X, Initial_ThetaX, Theta, cost_history = collabFilterGradientDescent(X, Theta, Y, R, alpha, reg_lambda, num_iters)cost = collabFilterCostFunction(X, Theta, Y, R, reg_lambda)print(\"Final cost =\", cost)user_idx = np.random.randint(n_u)pred_rating = []print(\"Original rating of an user:\\n\", Y.iloc[:,user_idx].sort_values(ascending=False))predicted_ratings = np.dot(X, Theta.T)predicted_ratings = sorted(zip(predicted_ratings[:,user_idx], Y.index), reverse=True)print(\"\\nPredicted rating of the same user:\")_ = [print(rating, movie) for rating, movie in predicted_ratings]","link":"/post/recommender-1/"},{"title":"Recommender Systems: II. Factorization Machine","text":"Factorization Machine1. Definition In essence, a generalized matrix factorization method Field: A type/column in the original dataset Feature: A value in the Field (Nike is a feature, Brand is a field) Movitation: Traditional regression methods cannot handle sparse matrix very well (too much waste in computation time on null values) FM solves the problem of considering pairwise feature interactions (linear time complexity). It allows us to train, based on reliable information (latent features) from every pairwise combination of features in the model. Main logic: Instead of using field as column, each feature has a column. So the columns are basically one-hot-encoding for each value in the field and the row is user id each row covers all the information a user has log-loss function to minimize: w_i: feature parameter vector (to be optimized) x_i: feature vector (column, given) v_i: latent vector of predefined low dimension k (to be optimized) The idea here is that except for individual feature, it consider the combination of 2 features (hence a degree = 2) as a factor Extension: Field-aware FM For each feature, the parameter vector is no longer unique A feature may interact with other features with different fields. Hence we differentiate the parameter vector for a feature based on the field of its interacting feature E.g: Gaming is an activity, Make is a gender; The parameter vector for Male may also be a if it is interacting with a brand like Nike Important note on numerical features Numerical features either need to be discretized (transformed to categorical features by breaking the entire range of a particular numerical feature into smaller ranges and label encoding each range separately). Another possibility is to add a dummy field which is the same as feature value will be numeric feature for that particular row (For example a feature with value 45.3 can be transformed to 1:1:45.3). However, the dummy fields may not be informative because they are merely duplicates of features. 2. Code Sample Note that the code below will faill because we haven‚Äôt installed the xlearn package (too tedious) Refer to the code to get an inspiration Only apply the code if you have the need to use FM or FFM in your model Note that usually DL method works better for the FM-integrated recommender 123456789101112131415import pandas as pdimport xlearn as xltrain = pd.read_csv('loan prediction/train_u6lujuX_CVtuZ9i.csv')import warningswarnings.filterwarnings('ignore')cols = ['Education','ApplicantIncome','Loan_Status','Credit_History']train_sub = train[cols]train_sub['Credit_History'].fillna(0, inplace = True)dict_ls = {'Y':1, 'N':0}train_sub['Loan_Status'].replace(dict_ls, inplace = True)## train-test splitfrom sklearn.model_selection import train_test_splitX_train, X_test = train_test_split(train_sub, test_size = 0.3, random_state = 5) Next, we need to convert the dataset to libffm format which is necessary for xLearn to fit the model. Following function does the job of converting dataset in standard dataframe format to libffm format. df = Dataframe to be converted to ffm format Type = 'Train' / 'Test'/ 'Valid' Numerics = list of all numeric fields Categories = list of all categorical fields Features = list of all features except the Label and Id 12345678910111213141516171819202122232425262728293031323334353637383940414243def convert_to_ffm(df,type,numerics,categories,features): currentcode = len(numerics) catdict = {} catcodes = {} # Flagging categorical and numerical fields for x in numerics: catdict[x] = 0 for x in categories: catdict[x] = 1 nrows = df.shape[0] ncolumns = len(features) with open(str(type) + \"_ffm.txt\", \"w\") as text_file: # Looping over rows to convert each row to libffm format for n, r in enumerate(range(nrows)): datastring = \"\" datarow = df.iloc[r].to_dict() datastring += str(int(datarow['Loan_Status'])) # Set Target Variable here # For numerical fields, we are creating a dummy field here for i, x in enumerate(catdict.keys()): if(catdict[x]==0): datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x]) else: # For a new field appearing in a training example if(x not in catcodes): catcodes[x] = {} currentcode +=1 catcodes[x][datarow[x]] = currentcode #encoding the feature # For already encoded fields elif(datarow[x] not in catcodes[x]): currentcode +=1 catcodes[x][datarow[x]] = currentcode #encoding the feature code = catcodes[x][datarow[x]] datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\" datastring += '\\n' text_file.write(datastring) the xLearn library can handle csv as well as libsvm format for implementation of FMs while we necessarily need to convert it to libffm format for using FFM. Once we have the dataset in libffm format, we could train the model using the xLearn library. xLearn can automatically performs early stopping using the validation/test logloss and we can also declare another metric and monitor on the validation set for each iteration of the stochastic gradient descent. 1234567891011121314151617181920212223ffm_model = xl.create_ffm()ffm_model.setTrain(\"train_ffm.txt\")param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'}# Start to train# The trained model will be stored in model.outffm_model.fit(param, './model.out')# The library also allows us to use cross-validation using the cv() function:ffm_model.cv(param)# Prediction taskffm_model.setTest(\"test_ffm.txt\") # Test dataffm_model.setSigmoid() # Convert output to 0-1# Start to predict# The output result will be stored in output.txtffm_model.predict(\"./model.out\", \"./output.txt\")","link":"/post/recommender-2/"},{"title":"Recommender Systems: III. Deep-learning Methods","text":"A brief introThere are a wide variety of DL tools used for recommendation systems, we will outline a few below. We cite various information from the paper Deep Learning based Recommender System: A Survey and New Perspectives. You may find more details from that paper. Multilayer Perceptron (MLP) is a feed-forward neural network with multiple (one or more) hidden layers between the input layer and output layer. Here, the perceptron can employ arbitrary activation function and does not necessarily represent strictly binary classi¬Äer. MLPs can be intrepreted as stacked layers of nonlinear transformations, learning hierarchical feature representations. MLPs are also known to be universal approximators. Autoencoder (AE) is an unsupervised model a¬äempting to reconstruct its input data in the output layer. In general, the bottleneck layer (the middle-most layer) is used as a salient feature representation of the input data. ¬åere are many variants of autoencoders such as denoising autoencoder, marginalized denoisingautoencoder, sparse autoencoder, contractive autoencoder and variational autoencoder (VAE). Convolutional Neural Network (CNN) is a special kind of feedforward neural network with convolution layers and pooling operations. It can capture the global and local features and significantly enhancing the e¬Åciency and accuracy. It performs well in processing data with grid-like topology. Recurrent Neural Network (RNN) is suitable for modelling sequential data. Unlike feedforward neural network, there are loops and memories in RNN to remember former computations. Variants such as Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) network are o¬âen deployed in practice to overcome the vanishing gradient problem. Restricted Boltzmann Machine (RBM) is a two layer neural network consisting of a visible layer and a hidden layer. It can be easily stacked to a deep net. Restricted here means that there are no intra-layer communications in visible layer or hidden layer. Neural Autoregressive Distribution Estimation (NADE) is an unsupervised neural network built atop autoregressive model and feedforward neural networks. It is a tractable and efficient estimator for modelling data distribution and densities. Adversarial Networks (AN) is a generative neural network which consists of a discriminator and a generator. The two neural networks are trained simultaneously by competing with each other in a minimax game framework. Attentional Models (AM) are di¬Çerentiable neural architectures that operate based on soft content addressing over an input sequence (or image). Attention mechanism is typically ubiquitous and was incepted in Computer Vision and Natural Language Processing domains. However, it has also been an emerging trend in deep recommender system research. Deep Reinforcement Learning (DRL) . Reinforcement learning operates on a trial-and-error paradigm. The whole framework mainly consists of the following components: agents, environments, states, actions and rewards. The combination between deep neural networks and reinforcement learning formulate DRL which have achieved human-level performance across multiple domains such as games and selfdriving cars. Deep neural networks enable the agent to get knowledge from raw data and derive efficient representations without handcrafted features and domain heuristics. Pros and consPros Nonlinear Transformation: Contrary to linear models, deep neural networks is capable of modelling the non-linearity in data with nonlinear activations such as relu, sigmoid, tanh, etc. This property makes it possible to capture the complex and intricate user item interaction patterns. The linear assumption, acting as the basis of many traditional recommenders, is oversimplified and will greatly limit their modelling expressiveness. It is well-established that neural networks are able to approximate any continuous function with an arbitrary precision by varying the activation choices and combinations. Representation Learning: Deep neural networks is efficacious in learning the underlying explanatory factors and useful representations from input data. In general, a large amount of descriptive information about items and users is available in real-world applications. Making use of this information provides a way to advance our understanding of items and users, thus, resulting in a better recommender. As such, it is a natural choice to apply deep neural networks to representation learning in recommendation models. The advantages of using deep neural networks to assist representation learning are in two-folds: it reduces the efforts in hand-craft feature design. Feature engineering is a labor intensive work, deep neural networks enable automatically feature learning from raw data in unsupervised or supervised approach; it enables recommendation models to include heterogeneous content information such as text, images, audio and even video. Deep learning networks have made breakthroughs in multimedia data processing and shown potentials in representations learning from various sources. Sequence Modelling: Deep neural networks have shown promising results on a number of sequential modelling tasks such as machine translation, natural language understanding, speech recognition, chatbots, and many others. RNN and CNN play critical roles in these tasks. RNN achives this with internal memory states while CNN achieves this with filters sliding along with time. Both of them are widely applicable flexible in mining sequential structure in data. Modelling sequential signals is an important topic for mining the temporal dynamics of user behaviour and item evolution. For example, next-item/basket prediction and session based recommendation are typical applications. As such, deep neural networks become a perfect fit for this sequential pattern mining task FlexibilityÔºö Deep learning techniques possess high flexibility, especially with the advent of many popular deep learning frameworks. Cons Interpretability: Despite its success, deep learning is well-known to behave as black boxes, and providing explainable predictions seem to be a really challenging task. A common argument against deep neural networks is that the hidden weights and activations are generally non-interpretable, limiting explainability. However, this concern has generally been eased with the advent of neural attention models and have paved the world for deep neural models that enjoy improved interpretability. While interpreting individual neurons still pose a challenge for neural models (not only in recommender systems), present state-of-the-art models are already capable of some extent of interpretability, enablingexplainable recommendation. We discuss this issue in more detail in the open issues section. Data Requirement: A second possible limitation is that deep learning is known to be data-hungry, in the sense that it requires sufficient data in order to fully support its rich parameterization. However, as compared with other domains (such as language or vision) in which labeled data is scarce, it is relatively easy to garner a significant amount of data within the context of recommender systems research. Million/billion scale datasets are commonplace not only in industry but also released as academic datasets. Extensive Hyperparameter Tuning: A third well-established argument against deep learning is the need for extensive hyperparameter tuning. However, we note that hyperparameter tuning is not an exclusive problem of deep learning but machine learning in general (e.g., regularization factors and learning rate similarly have to be tuned for traditional matrix factorization etc) Granted, deep learning may introduce additional hyperparameters in some cases.","link":"/post/recommender-3/"},{"title":"Regression Models: Linear Regression and Regularization","text":"Definition It is used for predicting the continuous dependent variable with the help of independent variables. The goal is to find the best fit line that can accurately predict the output for the continuous dependent variable. The model is usually fit by minimizing the sum of squared errors (OLS (Ordinary Least Square) estimator for regression parameters) Major algorithm is gradient descent: the key is to adjust the learning rate Explanation in layman terms: - provides you with a straight line that lets you infer the dependent variables - estimate the trend of a continuous data by a straight line. using input data to predict the outcome in the best possible way given the past data and its corresponding past outcomes Various RegulationsRegularization is a simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression. Convergence conditions differ note that regularization only apply on variables (hence is not regularized!) L2 norm: Euclidean distance from the origin L1 norm: Manhattan distance from the origin Elastic Net: Mixing L1 and L2 norms Ridge regression: where is cofficient; more widely used as compared to Ridge when number of variables increases Lasso regression: ; better when the data contains suspicious collinear variables Comparison with Logistic Regression Linear Regression: the outcomes are continuous (infinite possible values); error minimization technique is ordinary least square. Logistic Regression: outcomes usually have limited number of possible values; error minimization technique is maximal likelihood. ImplementationsBasic operations using sklearn packages 1234567891011from sklearn.linear_model import LinearRegressionX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])y = np.dot(X, np.array([1, 2])) + 3reg = LinearRegression(normalize=False, fit_intercept = True).fit(X, y)display(reg.score(X, y))display(reg.coef_) # regression coefficientsdisplay(reg.intercept_) # y-intercept / offsetreg.predict(np.array([[3, 5]])) Common Questions Is Linear regression sensitive to outliers? Yes! Is a relationship between residuals and predicted values in the model ideal? No, residuals should be due to randomness, hence no relationship is an ideal property for th model What is the range of learning rate? 0 to 1 Advanced: Analytical solutionsHere let's discuss some more math-intensive stuff. Those who are not interested can ignore this part (though it gives a very important guide on regression models) 1. A detour into Hypothesis representationWe will use to denote the independent variable and to denote dependent variable. A pair of is called training example. The subscripe in the notation is simply index into the training set. We have training example then . The goal of supervised learning is to learn a hypothesis function , for a given training set that can used to estimate based on . So hypothesis fuction represented as where are parameter of hypothesis.This is equation for Simple / Univariate Linear regression. For Multiple Linear regression more than one independent variable exit then we will use to denote indepedent variable and to denote dependent variable. We have independent variable then . The hypothesis function represented as where are parameter of hypothesis, Number of training exaples, Number of independent variable, is training exaple of feature. 2. Matrix FormulationIn general we can write above vector as Now we combine all aviable individual vector into single input matrix of size and denoted it by input matrix, which consist of all training exaples, We represent parameter of function and dependent variable in vactor form as So we represent hypothesis function in vectorize form . 3. Cost functionA cost function measures how much error in the model is in terms of ability to estimate the relationship between and .We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference of observed dependent variable in the given the dataset and those predicted by the hypothesis function. To implement the linear regression, take training example add an extra column that is feature, where . ,where and input matrix will become as Each of the m input samples is similarly a column vector with n+1 rows being 1 for our convenience, that is . Now we rewrite the ordinary least square cost function in matrix form as Let's look at the matrix multiplication concept,the multiplication of two matrix happens only if number of column of firt matrix is equal to number of row of second matrix. Here input matrix of size , parameter of function is of size and dependent variable vector of size . The product of matrix will return a vector of size , then product of will return size of unit vector. 4. Normal EquationThe normal equation is an analytical solution to the linear regression problem with a ordinary least square cost function. To minimize our cost function, take partial derivative of with respect to and equate to . The derivative of function is nothing but if a small change in input what would be the change in output of function. where Now we will apply partial derivative of our cost function, I will throw part away since we are going to compare a derivative to . And solve , Here because of unit vector. Partial derivative , ,, hence this is the normal equation for linear regression. Advanced: Model Evaluation and Model Validation1. Model evaluationWe will predict value for target variable by using our model parameter for test data set. Then compare the predicted value with actual valu in test set. We compute Mean Square Error using formula is statistical measure of how close data are to the fitted regression line. is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it's mean. 100% indicated that model explains all the variablity of the response data around the mean. where = Sum of Square Error, = Sum of Square Total. Here is predicted value and is mean value of .Below is a sample code for evaluation 12345678910111213141516171819202122232425262728# Normal equationy_pred_norm = np.matmul(X_test_0,theta)#Evaluvation: MSEJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]# R_square sse = np.sum((y_pred_norm - y_test)**2)sst = np.sum((y_test - y_test.mean())**2)R_square = 1 - (sse/sst)print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)print('R square obtain for normal equation method is :',R_square)&gt;&gt;&gt; The Mean Square Error(MSE) or J(theta) is: 0.17776161210877062&gt;&gt;&gt; R square obtain for normal equation method is : 0.7886774197617128# sklearn regression moduley_pred_sk = lin_reg.predict(X_test)#Evaluvation: MSEfrom sklearn.metrics import mean_squared_errorJ_mse_sk = mean_squared_error(y_pred_sk, y_test)# R_squareR_square_sk = lin_reg.score(X_test,y_test)print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)print('R square obtain for scikit learn library is :',R_square_sk)&gt;&gt;&gt; The Mean Square Error(MSE) or J(theta) is: 0.17776161210877925&gt;&gt;&gt; R square obtain for scikit learn library is : 0.7886774197617026 The model returns value of 77.95%, so it fit our data test very well, but still we can imporve the the performance of by diffirent technique. Please make a note that we have transformer out variable by applying natural log. When we put model into production antilog is applied to the equation. 2. Model ValidationIn order to validated model we need to check few assumption of linear regression model. The common assumption for Linear Regression model are following Linear Relationship: In linear regression the relationship between the dependent and independent variable to be linear. This can be checked by scatter ploting Actual value Vs Predicted value The residual error plot should be normally distributed. The mean of residual error should be 0 or close to 0 as much as possible The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot. Linear regession assumes that there is little or no *Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor VIF identifies correlation between independent variables and strength of that correlation. , If VIF &gt;1 &amp; VIF &lt;5 moderate correlation, VIF &lt; 5 critical level of multicollinearity. Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern. The model assumption linear regression as follows In our model the actual vs predicted plot is curve so linear assumption fails The residual mean is zero and residual error plot right skewed Q-Q plot shows as value log value greater than 1.5 trends to increase The plot is exhibit heteroscedastic, error will insease after certian point. Variance inflation factor value is less than 5, so no multicollearity. Linearity plot and Residual plot. Q-Q Plot and HomoScedasticity plot","link":"/post/regressions-1/"},{"title":"Regression Models: Logistic Regression","text":"Definition We have a mathematical function which gives a value between and , and to convert it to a value between (0,1), we need a Sigmoid function or a logistic function We can visualize it as a boundary (the decision boundary) to separate 2 categories on a hyperplane, where each dimension is a variable (a certain type of information) The algorithm used is also gradient descent Common Questions What is a logistic function? Answer: . What is the range of values of a logistic function? Answer: The values of a logistic function will range from 0 to 1. The values of Z will vary from to . What are the cost functions of logistic function? Answer: The popular 2 are Cross-entropy or log loss. Note that MSE is not used as squaring sigmoid violates convexity (cause local extrema to appear). Basic Implementation12345678from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionX, y = load_iris(return_X_y=True)clf = LogisticRegression(random_state=2).fit(X, y)clf.predict(X[:2, :])clf.predict_proba(X[:2, :])clf.score(X, y) NotesIn fact, logistic regression is simple, but the key thing here is actually on the mathematics behind gradient descent and its multi-dimensional variations. I'll discuss about them in future posts.","link":"/post/regressions-2/"},{"title":"Regression Models: GAM, GLM and GLMM","text":"OverviewGeneralized linear model (GLM) is a cure to some issues posted by ordinary linear regression. In the well-known linear regression model, we often assume . However, it often assumes that is not bounded when is not bounded. However, very often, we must restrict the values of within a fixed range. This may invalidated the ordinary linear model as the function behaviors near the boundary points can be very off. Generalized linear models aim to deal with this issue by allowing for that have arbitrary distributions (not just gaussian distribution), a function of (the link function) to vary linearly with (rather than assuming that a direct linear relationship between and ). Generalized Additive Model (GAM) and Generalized Linear Mixed Model (GLMM) are extensions to GLM with special functions applied to differenet elements in . GLMWe note that GLM has three major parts: An exponential family of probability distributions: , some examples include: normal exponential gamma chi-squared beta Dirichlet Bernoulli categorical Poisson A function of predictor (in GLM it is , in extended models, it can be other things, see GAM and GLMM), we can estimate via maximum likelihood or Bayesian methods like laplace approximation and Gibbs sampling, etc. A link function such that (sometime we may have tractable distribution for variance 1. Pros and Cons for GLM and GLMM Pros: Easy to interpret Easy to grasp Coefficients can be further used in numerical models Easy to extend: link functions, fixed and random effects, correlation structures Cons: Not good for dynamic models (the model is not linear and transformation may not help or would loose information Generalized additive models (GAMs) GAMs are extensions to GLMs in which the linear predictor is not restricted to be linear in the covariates but is the sum of smoothing functions applied to the each . For example, Is useful if relationship between Y and X is likely to be non-linear but we don't have any theory or any mechanistic model to suggest a particular functional form Each is linked with by a smoothing function instead of a coefficient GAMS are data-driven rather than model-driven, that is, the resulting fitted values do not come from an a priori model (non-parametric) All of the distribution families allowed with GLM are available with GAM 1. Pros and Cons for GAM Pros: By combining the basis functions GAMs can represent a large number of functional relationship (to do so they rely on the assumption that the true relationship is likely to be smooth, rather than wiggly) Particularly useful for uncovering nonlinear effects of numerical covariates, and for doing so in an \"automatic\" fashion More Flexible as now each sample's Y is associated with its X by a smoothing function instead of a coefficient Cons: Interpretability of the coefficient need to be estimated graphically Coefficients are not easily transferable to other datasets and parameterization Very sensitive to gaps in the data and outliers Lack underlying theory for the use of hypothesis tests one solution is to do bootstrapping and get aggregated result for more reliable confidence bands 2. Examples of GAM (different predictor representation functions): Loess (Locally weighted regression smoothing) The key factor is the span width (usually set to be a proportion of the data set: 0.5 as a standard starting point) Main idea: Split the data into separate blobs using sliding windows and fit linear regressions in each blob/interval Pros: Easily interpretable. At each test case, a local linear model is fit (eventually explained by linear behaviours) a popular way to see smooth trends on scatterplots Cons: If there are a lot of data points, fitting a LOESS over the entire range of the predictor can be slow because so many local linear regressions must be fit. Regression Splines (piecewise polynomials over usually a finite range) Main constraint is that the splines must remain smooth and continuous at knots To avoid overfitting of splines, penalty terms are added The penalty term also reflects the degree of smoothness in the regression The less smooth the regression is (after fitting the spline functions), the higher the penalty terms Pros: cover all sorts of nonlinear trends and are computationally very attractive because spline terms fit exactly into a least squares linear regression framework. Least squares models are very easy to fit computationally Cons: It is possible to create multidimensional splines by creating interactions between spline terms for different predictors. This suffers from the curse of dimensionality like KNN because we are trying to estimate a wavy surface in a large dimensional (many variable) space where data points will only sparsely cover the many many regions of the space GLMMThe model has the form: where is the design matrix for the random effects (the random complement to the fixed ). is a vector of the random effects (the random complement to the fixed ). The random effects are just deviations around the value in , which is the mean. Usually is a sparse matrix that assigns random effects to each element. We nearly always assume that with being the covariance matrix of the random effects. Assuming that the random effects are independent, we can have being a diagonal matrix with entries and . 1. Code implementationI recommend beginners to use statsmodels package because the output via .summary() function is very clear to read. For advanced users, you may implement the function yourself by referring to the mathematical expressions and package documentations from the following statsmodels: statsmodels.formula.api.mixedlm pymc3 theano pystan tensorflow keras 2. A sample code using statsmodels12345678910111213141516171819202122import statsmodels.formula.api as smffrom patsy import dmatricesformula = \"rt ~ group*orientation*identity\"#formula = \"rt ~ -1 + cbcond\"md = smf.mixedlm(formula, tbltest, groups=tbltest[\"subj\"])mdf = md.fit()print(mdf.summary())fe_params = pd.DataFrame(mdf.fe_params,columns=['LMM'])random_effects = pd.DataFrame(mdf.random_effects)random_effects = random_effects.transpose()random_effects = random_effects.rename(index=str, columns={'groups': 'LMM'})#%% Generate Design Matrix for later useY, X = dmatrices(formula, data=tbltest, return_type='matrix')Terms = X.design_info.column_names_, Z = dmatrices('rt ~ -1+subj', data=tbltest, return_type='matrix')X = np.asarray(X) # fixed effectZ = np.asarray(Z) # mixed effectY = np.asarray(Y).flatten()nfixed = np.shape(X)nrandm = np.shape(Z)","link":"/post/regressions-3/"},{"title":"Some Supervised Learning Models","text":"OverviewAlmost everyone who learned about data science or machine learning knows what supervised learning is. However, not many have dived deep into the details of those well-known models. In this blog, I will share some critical aspects of these models (mainly mathematical) that will become helpful in both research and practical work. One note on functionality is that these models work for both regression and classification problems. KNN1. Definition K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions) Non-parametric technique Distance functions can be Euclidean: Manhattan (Or Hamming in the case of Classification): Minkowski: Preprocessing Standardized Distance: One major drawback in calculating distance measures directly from the training set is in the case where variables have different measurement scales or there is a mixture of numerical and categorical variables. The solution is to do standardization on each variable Dimension Reduction: Usually KNN's speed gets much slower when number of attributes increase. Hence we need to reduce the number of dimensions using techniques such as PCA and SVD 2. Choice of K In general, a large K value is more precise as it reduces the overall noise; however, the compromise is that the distinct boundaries within the feature space are blurred (Lower prediction accuracy if K is too large). Need to use cross validation to determine an optimal K 3. Strength and Weakness Advantage The algorithm is simple and easy to implement. There's no need to build a model, tune several parameters, or make additional assumptions. The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section). Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates \"We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset\". Disadvantages The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase. KNN doesn't know which attributes are more import ant Doesn't handle missing data gracefully Slow during prediction (not training) 4. Suitable scenario KNN is bad if you have too many data points and speed is important In ensemble model: k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending When you are solving a problem which directly focusses on finding similarity between observations, K-NN does better because of its inherent nature to optimize locally (i.e: KNN-search) Real Life Example: a simple recommender system (e.g: Given our movies data set, what are the 5 most similar movies to a movie query) 5. Interview Questions Use 1 line to describe KNN Answer: KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression). 6. Simple implementations123456789101112131415161718192021222324252627282930313233343536373839404142434445## For Regressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neighbors import KNeighborsRegressorfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import confusion_matrix,mean_squared_error,accuracy_scorefrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_split##Randomly generate some datadata = pd.DataFrame(np.random.randint(low = 2,high = 100,size = (1000, 4)), columns=[\"Target\",\"A\",\"B\",\"C\"])data.head()train_x,test_x,train_y,test_y = train_test_split(data.iloc[:,1:],data.Target,test_size = 0.2)print(train_x.shape, test_x.shape)scaler = MinMaxScaler(feature_range=(0,1))scaler.fit(train_x)scaled_train_x = pd.DataFrame(scaler.transform(train_x),columns=['A','B','C'])scaled_test_x = pd.DataFrame(scaler.transform(test_x),columns=[\"A\",\"B\",\"C\"])###Basic Performance testingknn_regressor = KNeighborsRegressor(n_neighbors=3,algorithm=\"brute\",weights=\"distance\")knn_regressor.fit(scaled_train_x, train_y)train_pred = knn_regressor.predict(scaled_train_x)test_pred = knn_regressor.predict(scaled_test_x)print(mean_squared_error(train_y,train_pred))print(mean_squared_error(test_y,test_pred))###Grid Search to determine Kknn_regressor = KNeighborsRegressor(algorithm=\"brute\",weights=\"distance\")params = {\"n_neighbors\": [1,3,5],\"metric\": [\"euclidean\", \"cityblock\"]}grid = GridSearchCV(knn_regressor,param_grid=params,scoring=\"neg_mean_squared_error\",cv=5)grid.fit(scaled_train_x, train_y)print(grid.best_params_)print(grid.best_score_)best_knn = grid.best_estimator_train_pred = best_knn.predict(scaled_train_x)test_pred = best_knn.predict(scaled_test_x) SVM1. Definition Normally a binary classification Basic Ideology: SVM is based on the idea of finding a hyperplane that best separates the features into different domains. Both for Regression and classification SVR SVC Support vectors: The points closest to the hyperplane margin maximizing hyperplane: the bound that maximize the distances from support vectors hard margin SVM: If the points are linearly separable then only our hyperplane is able to distinguish between them. Then we have very strict constraints to correctly classify each and every datapoint Soft margin SVM: If the points are not linearly separable then we need an update so that our function may skip few outliers and be able to classify almost linearly separable points. For this reason, we introduce a new Slack variable(Œæ) We use CV to determine whether allowing certain amount of misclassification results in better classification in the long run Kernel: used to systematically find the specfic transformation that leads to class separation Polynomial Kernel: where = constant term and = degree of kernel done via Dot Product of a Feature Engineered Matrix Radial basis function kernel (RBF)/ Gaussian Kernel: where = Euclidean distance between &amp; Œ≥: As the value of increases the model gets overfits. As the value of decreases the model underfits For Gaussian kernel: Most Important Idea abut Kernel: Our powerful kernel function actually calculate the high-dimensional relationships WITHOUT actually transforming the data to higher dimensions Multiclass classification: 2 types of strategy One vs. All: N-class instances then N binary classifier models, then pick the prediction of a non-zero class which is the most certain. One-vs-Rest classification One vs. One: N-class instances then N* (N-1)/2 binary classifier models (adopted in SVM). At prediction time, a voting scheme is applied: all C(C‚àí1)/2 classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier. decision_function_shape='ovo' in the parameter to specify one-vs-one, else default is ovr 2. Pros &amp; ConsPros It is really effective in the higher dimension. Its solution is global optimal Effective when the number of features are more than training examples. Great when the data is noise-free and separable Less affected by outliers (if they are not the support vectors) SVM is suited for extreme case binary classification. Cons For larger dataset, it requires a large amount of time to process. Does not perform well in case of overlapped classes Cannot handle categorical data must convert via proper encoding Selection of hyperparameter/Kernel can be difficult resulting boundary plane are very difficult to interpret 3. Application When you need a non-linear approximator, use it When your dataset has a lot of features, use it When the matrix is sparse, use it When the data is unstructured, it is not used 4. Simple Implementation1234567from sklearn.svm import SVCsvc=SVC(kernel='linear') # Choices include 'rbf', 'poly', 'sigmoid'svc.fit(X_train,y_train)y_pred=svc.predict(X_test)print('Accuracy Score:')print(metrics.accuracy_score(y_test,y_pred)) Decision Tree1. Definition Decision Tree is a tree-based model that predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data). use a layered splitting process, where at each layer they try to split the data into two or more groups, so that data that fall into the same group are most similar to each other (homogeneity), and groups are as different as possible from each other (heterogeneity). It apples a top-down approach to data, so that given a data set, DTs try to group and label observations that are similar between them, and look for the best rules that split the observations that are dissimilar between them until they reach certain degree of similarity. Non-parametric technique Pruning: a technique used to deal with overfitting, that reduces the size of DTs by removing sections of the Tree that provide little predictive or classification power. Simpler trees prefered (according to Occam's Razor) Post-prune: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made. 2. Types of DT CHAID (Chi-squared Automatic Interaction Detection) multiway DT chooses the independent variable that has the strongest interaction with the dependent variable. The selection criteria: For regression: F-test For classification: chi-square test Has no pruning function CART (Classification And Regression Tree) binary DT handles data in its raw form (no preprocessing needed), can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables. The selection metric: For Classification: Gini Impurity Index where is the % of data with label in the split The lower value indicates a better spliting For Regression: Least Square Deviation (LSD) the sum of the squared distances (or deviations) between the observed values and the predicted values. Often refered as 'sqaured residual', lower LSD means better split doesn't use an internal performance measure for Tree selection/testing Iterative Dichotomiser 3 (ID3) classification DT Entropy: Single Attribute: Multiple Attribute: where ‚Üí Current state and ‚Üí Selected attribute The higher the entropy, the harder it is to draw any conclusions from that information. Follows the rule ‚Äî A branch with an entropy of zero is a leaf node and A brach with entropy more than zero needs further splitting The selection metric: Information Gain: where is number of splits and is a particular split The higher the gain, the better the split Limitation: it can't handle numeric attributes nor missing values C4.5 The successor of ID3 and represents an improvement in several aspects can handle both continuous and categorical data (regression + classification) can deal with missing values by ignoring instances that include non-existing data The selection metric: Gain ratio: a modification of Information gain that reduces its bias and is usually the best option Windowing: the algorithm randomly selects a subset of the training data (called a \"window\") and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. It captures all the \"rare\" instances together with sufficient \"ordinary\" cases. Can be pruned: pruning method is based on estimating the error rate of every internal node, and replacing it with a leaf node if the estimated error of the leaf is lower. 3. Strength and Weakness Advantage The algorithm is simple and easy to implement. Require very little data preparation The cost of using the tree for inference is logarithmic in the number of data points used to train the tree. Hence the training speed is high Good interpretability. Disadvantages Overfitting is quite common with decision trees simply due to the nature of their training. It's often recommended to perform some type of dimensionality reduction such as PCA so that the tree doesn't have to learn splits on so many features high variance, which means that a small change in the data can result in a very different set of splits, making interpretation somewhat complex. vulnerable to becoming biased to the classes that have a majority in the dataset. It's always a good idea to do some kind of class balancing such as class weights, sampling, or a specialised loss function. In more technical terms: it always look for a greedy option to split, thus more inclined towards a locally optimal split instead of a gloablly optimal one 4. Suitable scenarioConsideration: If the goal is better predictions, we should prefer RF, to reduce the variance. If the goal is exploratory analysis, we should prefer a single DT , as to understand the data relationship in a tree hierarchy structure. If there is a high non-linearity &amp; complex relationship between dependent &amp; independent variables, a tree model will outperform a classical regression method. When computational power is low, DT should be used When important features in the attributes are already identified, DT can be used When you demand more interpretability, DT should be used Use cases: healthcare industry: the screening of positive cases in the early detection of cognitive impairment Environment/Agriculture: DTs are used in agriculture to classify different crop types and identify their phenological stages/recognize different causes of forest loss from satellite imagery Sentiment Analysis: identify emotion from text Finance: Fraud Detection 5. Simple implementations123456789from sklearn import treedt = tree.DecisionTreeClassifier(random_state=1, max_depth=4)dt.fit(data_train, label_train)dt_score_train = dt.score(data_train, label_train) print(\"Training score: \",dt_score_train)dt_score_test = dt.score(data_test, label_test)print(\"Testing score: \",dt_score_test)dt2.predict(data_pred) Naive Bayes1. Definition The Na√Øve Bayes Classifier belongs to the family of probability classifier, using Bayesian theorem. The reason why it is called 'Na√Øve' because it requires rigid independence assumption between input variables. The classification formula is simple: Why is it called 'Naive': It is naive because while it uses conditional probability to make classifications, the algorithm simply assumes that all features of a class are independent. This is considered naive because, in reality, it is not often the case. Laplace Smoothing is also applied in some cases to solve the problem of zero probability. Different types of NB: Gaussian: It is used in classification and it assumes that features follow a normal distribution. Multinomial: It is used for discrete counts. For example, let's say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of 'word occurring in the document', we have 'count how often word occurs in the document', you can think of it as 'number of times outcome number x_i is observed over the n trials'. Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with 'bag of words' model where the 1s &amp; 0s are 'word occurs in the document' and 'word does not occur in the document' respectively. You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, \"ensembling, boosting, bagging\" won't help since their purpose is to reduce variance. Naive Bayes has no variance to minimize. 2. Pros &amp; ConsPros It is easy and fast to predict the class of the test data set. It also performs well in multi-class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption). Cons Can't learn the relationship among the features because assumes feature independence the assumption of independent predictors unlikely to hold. In real life, it is almost impossible that we get a set of predictors which are completely independent. 3. Applications Realtime prediction (because it's fast) When Dataset is Huge (high-dimension) When training dataset is small Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments) Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not. 4. Simple Implementation123456789from sklearn.naive_bayes import GaussianNBmodel = GaussianNB()# fit the model with the training datamodel.fit(train_x,train_y)# predict the target on the train datasetpredict_train = model.predict(train_x)print('Target on train data',predict_train)","link":"/post/supervised-learning/"},{"title":"Topic Modeling with Latent Dirichlet Allocation","text":"OverviewTopic modeling: Topic modelling refers to the task of identifying topics that best describes a set of documents. In this blog, we discuss about an popular advanced model that Definition To explain in plain word: LDA imagines a fixed set of topics. Each topic represents a set of words. The goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics. An important note to take is that LDA aims to explain the document-level idea, meaning it has less focus on the meaning of each word/phrase in the document, but rather the topic the document falls under Dirichlet Process: A family of stochastic process to produce a probability distribution Used in Bayesian Inference to describe the prior knowledge about the distribution of random variables Dirichlet Distribution: Basically a multivariate generalisation of the Beta distribution: where is a beta distribution Outputs: where Often called ‚Äúa distribution of distribution‚Äù symmetric Dirichlet distribution: a special case in the Dirichlet distribution where all are equal, hence use a single scalar in the model representation Impact of : (a scaling vector for each dimension in ) : Sparsity increases The distribution is likely bowl-shaped (most probable vectors are sparse vectors like or In LDA, it means a document is likely to be represented by just a few of the topics Sparcity decreases We will have a unimodel distribution (most probable vectors are in the center) In LDA, it means a document is likely to contain most of the topics makes documents more similar to each other The conjugate prior of multinomial distribution is a Dirichlet distribution LDA\\s keywords k: Number of topics a document belongs to (a fixed number) V : Size of the vocabulary M: Number of documents N: Number of words in each document w: A word in a document. This is represented as a one hot encoded vector of size V W: represents a document (i.e. vector of \"w\"s) of N words D: Corpus, a collection of M documents z: A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil) Œ∏: The topic distribution for each of the document based on a parameter Œ± Œ≤: The Dirichlet distribution based on parameter Œ∑ LDA's procedure This is quite complicated LDA's document generation Œ± has a topic distribution for each document (Œ∏ ground for each document) a (M x K) shape matrix Œ∑ has a parameter vector for each topic. Œ∑ will be of shape (k x V) In the above drawing, the constants actually represent matrices, and are formed by replicating the single value in the matrix to every single cell. Œ∏ is a random matrix based on dirichlet distribution, where represents the probability of the th document to containing words belonging to the th topic a relatively low Œ≤ is also a dirichlet distribution as Œ∏, represents the probability of the th topic containing the th word in a vocabulary of size ; The higher the , the more th topic is likely to contain more of the words, and makes the topics more similar to each other Detailed steps: For each topic, draw a distribution over words For each document Draw a vector of topic proportions . E.g: [climate = 0.7, trade = 0.2, housing = 0.1, economy = 0] For each word slot allocated, draw a topic assignment , then draw a word We want to infer the join probability given our observations, We infer the hidden variables or latent factors by observing the corpse of documents, i.e. finding The learning part Idea 1: Gibbs sampling: A point-wise method (Possible but not optimal) Intuition: The setting which generates the original document with the highest proability is the optimal machine The mathematics of collapsed gibbs sampling (cut back version) Recall that when we iterate through each word in each document, we unassign its current topic assignment and reassign the word to a new topic. The topic we reassign the word to is based on the probabilities below. where - number of word assignments to topic in document - number of assignments to topic in document - smoothing parameter (hyper parameter - make sure probability is never 0) - number of words in document - don‚Äôt count the current word you‚Äôre on - total number of topics - number of assignments, corpus wide, of word to topic - number of assignments, corpus wide, of word to topic - smoothing parameter (hyper parameter - make sure probability is never 0) - sum over all words in vocabulary currently assigned to topic size of vocabulary i.e. number of distinct words corpus wide Done with each word in a document (to classify them into a topic) Done in an iterative way (different topics for same words in a document: 1st \"happy\" may be topic 1, which affects 2nd \"happy\" to be topic 2 in the same document) Main steps: For each word in a document : The word will be allocated to Note that is the one used in the original and in Iterate until each document &amp; word's topic is upadted Aggregate the results from all documents to update the word distribution for each topic Repeat the previous steps until corpus objective converges Idea 2: variational inference: The key concept of variance inference is approximate posterior with a distribution using some known families of distribution that is easy to model and to analyze. Then, we train the model parameters to minimize the KL-divergence between q and p. KL-divergence: ,also called \"relative entropy\" Further reduction in complexity for high dimensional distribution is possible Idea 3: Mean-field variational inference breaks up the joint distribution into distributions of individual variables that are tractable and easy to analyze It is not easy to optimize KL-divergence directly. So let us introduce the Evidence lower bound (ELBO) by maximizing ELBO, we are minimizing KL-divergence: view explanation here When minimizing ELBO, we don‚Äôt need Z. No normalization is needed. In contrast, KL‚Äôs calculation needs the calculated entity to be a probability distribution. Therefore, we need to compute the normalization factor Z if it is not equal to one. Calculating Z is hard. This is why we calculate ELBO instead of KL-divergence. There are a lot of math details involving exponential family operations, but the general picutre is captured by the graph below Evaluation using similarity query Ok, now that we have a topic distribution for a new unseen document, let's say we wanted to find the most similar documents in the corpus. We can do this by comparing the topic distribution of the new document to all the topic distributions of the documents in the corpus. We use the Jensen-Shannon distance metric to find the most similar documents. What the Jensen-Shannon distance tells us, is which documents are statisically \"closer\" (and therefore more similar), by comparing the divergence of their distributions. Jensen-Shannon is symmetric, unlike Kullback-Leibler on which the formula is based. This is good, because we want the similarity between documents A and B to be the same as the similarity between B and A. The formula is described below. For discrete distirbutions and , the Jensen-Shannon divergence, is defined as where and is the Kullback-Leibler divergence The square root of the Jensen-Shannon divergence is the Jensen-Shannon Distance: The smaller the Jensen-Shannon Distance, the more similar two distributions are (and in our case, the more similar any 2 documents are) Pros &amp; ConsPros An effective tool for topic modeling Easy to understand/interpretable variational inference is tractable Œ∏ are document-specific, so the variational parameters of Œ∏ could be regarded as the representation of a document , hence the feature set is reduced. z are sampled repeatedly within a document ‚Äî one document can be associated with multiple topics. Cons Must know the number of topics K in advance Hard to know when LDA is working - topics are soft-clusters so there is no objective metric to say \"this is the best choice\" of hyperparameters LDA does not work well with very short documents, like twitter feeds Dirichlet topic distribution cannot capture correlations among topics Stopwords and rare words should be excluded, so that the model doesnt overcompensate for very frequent words and very rare words, both of which do not contribute to general topics. Real-word application Text classification Book recommender Article clustering/image clustering understanding the different varieties topics in a corpus (obviously) getting a better insight into the type of documents in a corpus (whether they are about news, wikipedia articles, business documents) quantifying the most used / most important words in a corpus document similarity and recommendation. Long Code example123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235# import dependencies%matplotlib inlineimport pandas as pdimport numpy as npimport nltkfrom nltk.corpus import stopwordsimport gensimfrom gensim.models import LdaModelfrom gensim import models, corpora, similaritiesimport refrom nltk.stem.porter import PorterStemmerimport timefrom nltk import FreqDistfrom scipy.stats import entropyimport matplotlib.pyplot as pltimport seaborn as sns# Read in data; only keep essential columns and English language articlesdf = pd.read_csv('lda_fake.csv', usecols = ['uuid','author','title','text','language','site_url','country'])df = df[df.language == 'english']df = df[df['text'].map(type) == str]df['title'].fillna(value=\"\", inplace=True)df.dropna(axis=0, inplace=True, subset=['text'])# shuffle the datadf = df.sample(frac=1.0)df.reset_index(drop=True,inplace=True)# Define some functions to clean and tokenize the datadef initial_clean(text): \"\"\" Function to clean text of websites, email addresess and any punctuation We also lower case the text \"\"\" text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text) text = re.sub(\"[^a-zA-Z ]\", \"\", text) text = text.lower() # lower case the text text = nltk.word_tokenize(text) return textstop_words = stopwords.words('english')def remove_stop_words(text): \"\"\" Function that removes all stopwords from text \"\"\" return [word for word in text if word not in stop_words]stemmer = PorterStemmer()def stem_words(text): \"\"\" Function to stem words, so plural and singular are treated the same \"\"\" try: text = [stemmer.stem(word) for word in text] text = [word for word in text if len(word) &gt; 1] # make sure we have no 1 letter words except IndexError: # the word \"oed\" broke this, so needed try except pass return textdef apply_all(text): \"\"\" This function applies all the functions above into one \"\"\" return stem_words(remove_stop_words(initial_clean(text)))# clean text and title and create new column \"tokenized\"t1 = time.time()df['tokenized'] = df['text'].apply(apply_all) + df['title'].apply(apply_all)t2 = time.time()print(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")# We'll use nltk to get a word frequency (by count) here and only keep the top most used words to train the LDA model on# first get a list of all wordsall_words = [word for item in list(df['tokenized']) for word in item]# use nltk fdist to get a frequency distribution of all wordsfdist = FreqDist(all_words)len(fdist) # number of unique words# choose k and visually inspect the bottom 10 words of the top kk = 50000top_k_words = fdist.most_common(k)top_k_words[-10:]# choose k and visually inspect the bottom 10 words of the top kk = 15000top_k_words = fdist.most_common(k)top_k_words[-10:]# k = 50,000 is too high, as the bottom words aren't even real words and are very rarely used (once in entire corpus)# k = 15,000 is much more reasonable as these have been used at least 13 times in the corpus# define a function only to keep words in the top k wordstop_k_words,_ = zip(*fdist.most_common(k))top_k_words = set(top_k_words)def keep_top_k_words(text): return [word for word in text if word in top_k_words] df['tokenized'] = df['tokenized'].apply(keep_top_k_words)# document lengthdf['doc_len'] = df['tokenized'].apply(lambda x: len(x))doc_lengths = list(df['doc_len'])df.drop(labels='doc_len', axis=1, inplace=True)print(\"length of list:\",len(doc_lengths), \"\\naverage document length\", np.average(doc_lengths), \"\\nminimum document length\", min(doc_lengths), \"\\nmaximum document length\", max(doc_lengths)) # plot a histogram of document lengthnum_bins = 1000fig, ax = plt.subplots(figsize=(12,6));# the histogram of the datan, bins, patches = ax.hist(doc_lengths, num_bins)ax.set_xlabel('Document Length (tokens)', fontsize=15)ax.set_ylabel('Normed Frequency', fontsize=15)ax.grid()ax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))plt.xlim(0,2000)ax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-', label='average doc length')ax.legend()ax.grid()fig.tight_layout()plt.show()### Drop short articlesLDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.From the histogram above, droping all articles less than 40 tokens seems appropriate.# only keep articles with more than 30 tokens, otherwise too shortdf = df[df['tokenized'].map(len) &gt;= 40]# make sure all tokenized items are listsdf = df[df['tokenized'].map(type) == list]df.reset_index(drop=True,inplace=True)print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df), \"articles\")# create a mask of binary valuesmsk = np.random.rand(len(df)) &lt; 0.999train_df = df[msk]train_df.reset_index(drop=True,inplace=True)test_df = df[~msk]test_df.reset_index(drop=True,inplace=True)def train_lda(data): \"\"\" This function trains the lda model We setup parameters like number of topics, the chunksize to use in Hoffman method We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize \"\"\" num_topics = 100 chunksize = 300 dictionary = corpora.Dictionary(data['tokenized']) corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']] t1 = time.time() # low alpha means each document is only represented by a small number of topics, and vice versa # low eta means each topic is only represented by a small number of words, and vice versa lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2) t2 = time.time() print(\"Time to train LDA model on \", len(df), \"articles: \", (t2-t1)/60, \"min\") return dictionary,corpus,lda dictionary,corpus,lda = train_lda(train_df)# show_topics method shows the the top num_words contributing to num_topics number of random topicslda.show_topics(num_topics=10, num_words=20)# select and article at random from train_dfrandom_article_index = np.random.randint(len(train_df))bow = dictionary.doc2bow(train_df.iloc[random_article_index,7])# get the topic contributions for the document chosen at random abovedoc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])# bar plot of topic distribution for this documentfig, ax = plt.subplots(figsize=(12,6));# the histogram of the datapatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)ax.set_xlabel('Topic ID', fontsize=15)ax.set_ylabel('Topic Contribution', fontsize=15)ax.set_title(\"Topic Distribution for Article \" + str(random_article_index), fontsize=20)ax.set_xticks(np.linspace(10,100,10))fig.tight_layout()plt.show()# select and article at random from test_dfrandom_article_index = np.random.randint(len(test_df))print(random_article_index)new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])print(test_df.iloc[random_article_index,3])new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])# bar plot of topic distribution for this documentfig, ax = plt.subplots(figsize=(12,6));# the histogram of the datapatches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)ax.set_xlabel('Topic ID', fontsize=15)ax.set_ylabel('Topic Contribution', fontsize=15)ax.set_title(\"Topic Distribution for an Unseen Article\", fontsize=20)ax.set_xticks(np.linspace(10,100,10))fig.tight_layout()plt.show()def jensen_shannon(query, matrix): \"\"\" This function implements a Jensen-Shannon similarity between the input query (an LDA topic distribution for a document) and the entire corpus of topic distributions. It returns an array of length M where M is the number of documents in the corpus \"\"\" # lets keep with the p,q notation above p = query[None,:].T # take transpose q = matrix.T # transpose matrix m = 0.5*(p + q) return np.sqrt(0.5*(entropy(p,m) + entropy(q,m))) def get_most_similar_documents(query,matrix,k=10): \"\"\" This function implements the Jensen-Shannon distance above and retruns the top k indices of the smallest jensen shannon distances \"\"\" sims = jensen_shannon(query,matrix) # list of jensen shannon distances return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances # this is surprisingly fastmost_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)most_similar_df = train_df[train_df.index.isin(most_sim_ids)]most_similar_df['title']","link":"/post/topic-modeling/"},{"title":"Unsupervised Learning: Measures about Clustering","text":"OverviewUnsupervised learning is a vast topic, and clustering is really a big part (yet not all) of it. Whenever we have some ideas about clusteirng, we should first ask: is this idea comparable to some existing works? Now to answer this, we need some evaluation strategies and choose measures for such evaluations. This is what today's blog will talk about. Distance MetricsWe have four most popular distance metrics outlined below. In essense, one should understand the structure of each metric, and when to use them. Minkowski Distance: Minkowski distance is a metric in Normed vector space. Formula p = 1, Manhattan Distance p = 2, Euclidean Distance p = ‚àû, Chebychev Distance Manhattan Distance: We use Manhattan Distance if we need to calculate the distance between two data points in a grid like path. Euclidean Distance: Euclidean distance formula can be used to calculate the distance between two data points in a plane. Cosine Distance: Mostly Cosine distance metric is used to find similarities between different documents. In cosine metric we measure the degree of angle between two documents/vectors(the term frequencies in different documents collected as metrics). This particular metric is used when the magnitude between vectors does not matter but the orientation. Formula: Evaluation Methods1. Clustering Tendency Before evaluating the clustering performance, making sure that data set we are working has clustering tendency and does not contain uniformly distributed points is very important. Hopkins Test: a statistical test for spatial randomness of a variable, can be used to measure the probability of data points generated by uniform data distribution. Null Hypothesis () : Data points are generated by non-random, uniform distribution (implying no meaningful clusters) Alternate Hypothesis (): Data points are generated by random data points (presence of clusters) If the H value is between {0.01, ‚Ä¶,0.3}, the data is regularly spaced. If the H value is around 0.5, it is random. If the H value is between {0.7, ‚Ä¶, 0.99}, it has a high tendency to cluster. The Hopkins Test is highly influenced by outliers Sample Code123456789101112131415161718import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)from sklearn.decomposition import PCAfrom sklearn import datasetsfrom sklearn.preprocessing import scalefrom pyclustertend import hopkins ## the hopkins testfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltheart_df = pd.read_csv(\"heart.csv\")X = heart_df[heart_df.columns[~heart_df.columns.isin([\"target\"])]].valuesy = heart_df[heart_df.columns[heart_df.columns.isin([\"target\"])]].values.flatten()display(hopkins(X, X.shape[0]))display(hopkins(scale(X),X.shape[0])) 2. Number of Optimal ClustersMainly 2 Direction Domain knowledge ‚Äî Domain knowledge might give some prior knowledge on finding number of clusters. For example, in case of clustering iris data set, if we have the prior knowledge of species (sertosa, virginica, versicolor) , then k = 3. Domain knowledge driven k value gives more relevant insights. Data driven approach ‚Äî If the domain knowledge is not available, mathematical methods help in finding out right number of clusters. Mainly 2 Methods Statistical approach: Gap statistic is a powerful statistical method to find the optimal number of clusters, k. Sum of within-cluster (intra-cluster) variance is calculated for different values of k. : Sum-of-within-Cluster variance of original data set for k clusters : Sum-of-within-cluster variance of reference data set (null reference data set of uniform distribution) of k clusters Formula: As Gap statistic quantifies this deviation, More the Gap statistic means more the deviation. Cluster number with maximum Gap statistic value corresponds to optimal number of cluster. Sample Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Gap Statistics%matplotlib inlineimport timeimport hashlibimport scipyimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom sklearn.cluster import KMeansfrom sklearn.datasets.samples_generator import make_blobsplt.rcParams['figure.figsize'] = 10, 10x, y = make_blobs(750, n_features=2, centers=12)plt.scatter(x[:, 0], x[:, 1])plt.show()def optimalK(data, nrefs=3, maxClusters=15): \"\"\" Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie Params: data: ndarry of shape (n_samples, n_features) nrefs: number of sample reference datasets to create maxClusters: Maximum number of clusters to test for Returns: (gaps, optimalK) \"\"\" gaps = np.zeros((len(range(1, maxClusters)),)) resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]}) for gap_index, k in enumerate(range(1, maxClusters)): # Holder for reference dispersion results refDisps = np.zeros(nrefs) # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop for i in range(nrefs): # Create new random reference set randomReference = np.random.random_sample(size=data.shape) # Fit to it km = KMeans(k) km.fit(randomReference) refDisp = km.inertia_ refDisps[i] = refDisp # Fit cluster to original data and create dispersion km = KMeans(k) km.fit(data) origDisp = km.inertia_ # Calculate gap statistic gap = np.log(np.mean(refDisps)) - np.log(origDisp) # Assign this loop's gap statistic to gaps gaps[gap_index] = gap resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True) return (gaps.argmax() + 1, resultsdf) # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal k, gapdf = optimalK(x, nrefs=5, maxClusters=15)print(f'Optimal k is: {k}') Elbow method: Within-cluster variance is a measure of compactness of the cluster. Lower the value of within cluster variance, higher the compactness of cluster formed. Sample Code12345678910111213141516171819202122232425262728293031# Elbow Methodimport matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn.datasets.samples_generator import (make_blobs, make_circles, make_moons)from sklearn.cluster import KMeans, SpectralClusteringfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import silhouette_samples, silhouette_score# Import the datadf = pd.read_csv('old_faithful.csv')# Standardize the dataX_std = StandardScaler().fit_transform(df)sse = []list_k = list(range(1, 10))for k in list_k: km = KMeans(n_clusters=k) km.fit(X_std) sse.append(km.inertia_)# Plot sse against kplt.figure(figsize=(6, 6))plt.plot(list_k, sse, '-o')plt.xlabel(r'Number of clusters $k$')plt.ylabel('Sum of squared distance') 3. Clustering QualityThere are majorly two types of measures to assess the clustering performance. For more details, check sklearn document on cluster performance evaluation. Extrinsic Measures: Require ground truth labels. Examples are Adjusted Rand index, Fowlkes-Mallows scores, Mutual information based scores, Homogeneity, Completeness and V-measure. Intrinsic Measures: Does not require ground truth labels. Examples are Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index etc. 123456789101112131415161718192021222324252627282930313233343536373839404142434445# silhouette analysisfor i, k in enumerate([2, 3, 4]): fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) # Run the Kmeans algorithm km = KMeans(n_clusters=k) labels = km.fit_predict(X_std) centroids = km.cluster_centers_ # Get silhouette samples silhouette_vals = silhouette_samples(X_std, labels) # Silhouette plot y_ticks = [] y_lower, y_upper = 0, 0 for i, cluster in enumerate(np.unique(labels)): cluster_silhouette_vals = silhouette_vals[labels == cluster] cluster_silhouette_vals.sort() y_upper += len(cluster_silhouette_vals) ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1) ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1)) y_lower += len(cluster_silhouette_vals) # Get the average silhouette score and plot it avg_score = np.mean(silhouette_vals) ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green') ax1.set_yticks([]) ax1.set_xlim([-0.1, 1]) ax1.set_xlabel('Silhouette coefficient values') ax1.set_ylabel('Cluster labels') ax1.set_title('Silhouette plot for the various clusters', y=1.02); # Scatter plot of data colored with labels ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels) ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250) ax2.set_xlim([-2, 2]) ax2.set_xlim([-2, 2]) ax2.set_xlabel('Eruption time in mins') ax2.set_ylabel('Waiting time to next eruption') ax2.set_title('Visualization of clustered data', y=1.02) ax2.set_aspect('equal') plt.tight_layout() plt.suptitle(f'Silhouette analysis using k = {k}', fontsize=16, fontweight='semibold', y=1.05);","link":"/post/unsupervised-learning/"},{"title":"Variational Inference","text":"Introduction1. Background of Bayesian methodsIn the field of machine learning, most would agree that frequentist approaches played a critical role in the development of early classical models. Nevertheless, we are witnessing the increasing significance of Bayesian methods in modern study of machine learning and data modelling. The simple-looking Bayes' rule has inspired a lot wonderful models in areas like topic modelling, representation learning and hyperparameter optimization. In these models, the latent variables are the focus of the study. By analysing several data on the observed variables , we hope to get some meaningful information (for example, a point estimate or an entire distribution) about these latent variables. 2. Problem with Bayesian methods: intractable integralWhile the rule looks easily understandable, the numerical computation is hard in reality. One major issue is the intractable integral we need to compute in order to get the , which is often called the \"model evidence\". This is often because the search space for is combinatorially too large, making the computation extremely expensive. A common approach to deal with this problem is to approximate the posterior probability directly. Some popular choices include Monte Carlo Sampling methods and variational inference. In this report, we will introduce the variational methods, which are perhaps the most widely used inference technique in machine learning. We will analyse a particularly famous technique in variational methods, mean-field variational inference. 3. Main idea of variational inferenceIn variational inference, we can avoid computing the intractable integral by magically modelling the posterior directly. The main trick here is to approximate the unknown distribution with some similar distribution q. Since we can choose the q to belong to a certain family of distribution (hence tractable), the problem is now transformed into an optimization problem about the parameters of . Understanding Variational Bayesian methodIn this section, we demonstrate the theory behind variational Bayesian methods. 1. Kullback-Leibler DivergenceAs mentioned above, variational inference needs a distribution to approximate the posterior distribution . Therefore we need to gauge how well a candidate approximates the posterior. A common measure is Kullback-Leibler Divergence (often called KL divergence). KL divergence is defined as Where means the expected value with respect to distribution . The formula can be interpreted as follows: if is low, the divergence is generally low. if is high and is high, the divergence is low. if is high and is low, the divergence is high, hence the approximation is not ideal. Take note of the following about use of KL divergence in Variational Bayes: KL divergence is not symmetric, it's easy to see from the formula that as the approximation distribution is usually different from the target distribution . In general, we focus on approximating some regions of as good as possible (Figure 1 (a)). It is not necessary for the to nicely approximate every part of As a result (usually called forward divergence) is not ideal. Because for some regions which we don't want to care, if , the KL divergence will be very large, forcing to take a different form even if it fits well with other regions of (refer to Figure 1(b)). On the other hand, (usually called reverse KL divergence) has the nice property that only regions where requires and to be similar. Consequently, reverse KL divergence is more commonly used in Variational Inference. 2. Evidence lower boundUsually we don't directly minimizing KL divergence to obtain a good approximated distribution. This is because computing divergence still depends on the posterior . The computation involves the \"evidence\" term which is expensive to compute, as shown in the formula below: The approximation using reverse KL divergence usually gives good empirical results, even though some regions of may be compromised Figure 1: Reverse KL vs Forward KL divergence: The left image has a better approximation on part of . (a) Reverse KL simulation (b) Forward KL simulation We can directly conclude by the fact that the term is than the log of evidence. We can also proof this result using Jensen's inequality as follows: By the definition of marginal probability, we have , take log on both side we have: The last 2 lines follow from Jensen's Inequality which states that for a convex function , we have This term is known as the Evidence Lower Bound, or Since log does not depend on , we can treat it as a constant from the perspective of optimizing . Hence, minimizing is now equivalent to maximizing General procedureIn general, a variational inference starts with a family of variational distribution (such as the mean-field family described below) as the candidate for . We can then use the manually chosen to compute the ELBO terms and . Afterwards, we optimize the parameters in to maximize the ELBO value using some optimization techniques (such as coordinate ascent and gradient methods). Mean Field Variational Family1. The \"Mean Field\" AssumptionsAs shown above, the particular variational distribution family we use to approximate the posterior is chosen by ourselves. A popular choice is called the mean-field variational family. This family of distribution assumes joint approximation distribution to be factorized over some partition of the latent variables. This implies mutual independence among the n fractions in the partition. In particular: we have where is factorized into . For simplicity, we assume that each fraction only contains 1 latent variable , it is often referred as \"naive mean field\". This family is nice to analyse because we can model each distribution with a tractable distribution based on the problem set-up. Do note that a limitation of this family is that we cannot easily capture the interdependence among the latent variables. 2. Derivation of optimal Now in order to derive the the optimal form of distribution for and thus the overall , we need to go back to the ELBO optimization with this mean-field family assumption. Recall the formula for ELBO (we use here as it is the convention): We express this formula in terms of as using functional integral (see appendix A): With this new expression, we can consider maximizing with respect to each of the . The optimal form of is the one which maximizes , that is: We take the derivative with respect to using Lagrange multipliers and set to 0 yields: where is a normalization constant that plays minimal role in the variable update. The funtional derivative of this expression actually requires some knowledge about calculus of variations, specifically Euler-Lagrange equation. 3. Variable update with Coordinate AscentFrom equation we found that . Therefore iterative optimization algorithms like Coordinate Ascent can be applied to update the latent variables to reach their optimal form. Note that all the 's are interdependent during the update, hence in each iteration, we need to update all the 's. As short description for the coordinate ascent in this setup will be: Compute values (if any) that can be directly obtained from data and constants Initialize a particular to an arbitrary value Update each variable with the step function Repeat step 3 until the convergence of ELBO A more detailed example of coordinate ascent will be shown in next section with the univariate gaussian distribution example. A point to take note that in general, we cannot guarantee the convexity of ELBO function. Hence, the convergence is usually to a local maximum. Example with Univariate GaussianWe demonstrate the mean-field variational inference with a simple case of observations from univariate Gaussian model. We first assume there are observations from a Gaussian distribution satisfying: Here is inverse of variance (hence one-to-one correspondence). From the derivation of we know we need to compute the log joint probability . We will first derive an explicit formula for it by expanding the join probability into conditional probability: where is a constant. Note that sometimes some latent variable has higher priority that others. The choice of this variable depends on the exact question in hand. 1. Compute independent and Next, we apply approximation via . By the mean-field assumption, we have . We proceed to find the optimal form of and : Compute the expression for : Note that here is a shortcut representation for , and all are constant terms not involved in the optimization update. From the expression above, it's easy to observe that follows a Gaussian distribution with , where: Compute the expression for A closer look at the result suggest that follows a Gaussian distribution with Gamma , where: 2. Variable update until ELBO convergence}Now that we have and , we only need to update their parameters: Using the updated and , we can then compute with Hence the coordinate ascent algorithm can be applied here: Compute and as they can be derived directly from the data and constants based on their formula Initialize to some random value Update with current values of and Update with current values of and Compute ELBO value with the variables &amp; updated with the parameters in step 1 - 4 Repeat the last 3 steps until ELBO value doesn't vary by much As a result of the algorithm, we obtain an approximation for the posterior distribution of and given observations . Extension and Further resultIn this section, we briefly outline some more theory and reflection about general variational Bayesian methods. Due to space limitations, we only provide a short discussion on each of these. Exponential family distributions in Varational InferenceA nice property of the exponential family distribution is the presence of conjugate priors in closed forms. This allows for less computationally intensive approaches when approximating posterior distributions (due to reasons like simpler optimization algorithm applicable and better analytical forms). Further more, Gharamani &amp; Beal even suggested in 2000 that if all the belong to the same exponential family, the update of latent variables in the optimization procedure can be exact. A great achievement in the field of variational inference is the generalized update formula for Exponentialfamily-conditional models. These models has conditional densities that are in exponential family. The nice property of exponential family leads to an amazing result that the optimal approximation form for posteriors are in the same exponential family as the conditional. This has benefits a lot of well-known models like Markov random field and Factorial Hidden Markov Model. Comparison to other Inference methodsThe ultimate results of variational inference are the approximation for the entire posterior distribution about the parameters and variables in the target problem with some observations instead of just a single point estimate. This serves the purpose of further statistical study of these latent variables, even if their true distributions are analytically intractable. Another group of inference methods commonly used to achieve the similar aim is Markov chain Monte Carlo (MCMC) methods like Gibbs sampling, which seeks to produce reliable resampling of given observations that help to approximate latent variables well. Another common Bayesian method that has a similar iterative variable update procedure is Expectation Maximization (EM). For EM, however, only point estimates of posterior distribution are obtained. The estimates are \"Expectation maximizing\" points, which means any information about the distribution around these points (or the parameters they estimate) are not preserved. On the other hand, despite the advantage of \"entire distribution\" Variational inference has, its point estimates are often derived just by the mean value of the approximated distributions. Such point estimates are often less significant compared to those derived using EM, as the optimum is not directly achieved from the Bayesian network itself, but the optimal distributions inferred from the network. Popular algorithms applying variational inferenceThe popularity of variational inference has grown to even surpass the classical MCMC methods in recent years. It is particularly successful in generative modeling as a replacement for Gibbs sampling. The methods often show better empirical result than Gibbs sampling, and are thus more well-adopted. We here showcase some popular machine learning models and even deep learning models that heavily rely on variational inference methods and achieved great success: Latent Dirichlet Allocation: With the underlying Dirichlet distribution, the model applies both variational method (for latent variable distribution) and EM algorithm to obtain an optimal topic separation and categorization. variational autoencoder: The latent Gaussian space (a representation for the input with all the latent variables and parameters) is derived from observations, and fine-tuned to generate some convincing counterparts (a copy for instance) of the input. These models often rely on a mixture of statistical learning theories, but variational inference is definitely one of the key function within them.","link":"/post/variational-inference/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Clustering","slug":"Clustering","link":"/tags/Clustering/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","link":"/tags/Unsupervised-Learning/"},{"name":"Matrix Computation","slug":"Matrix-Computation","link":"/tags/Matrix-Computation/"},{"name":"Dimensionality Reduction","slug":"Dimensionality-Reduction","link":"/tags/Dimensionality-Reduction/"},{"name":"Ensemble","slug":"Ensemble","link":"/tags/Ensemble/"},{"name":"Boosting","slug":"Boosting","link":"/tags/Boosting/"},{"name":"Supervised Learning","slug":"Supervised-Learning","link":"/tags/Supervised-Learning/"},{"name":"Bagging","slug":"Bagging","link":"/tags/Bagging/"},{"name":"Random Forest","slug":"Random-Forest","link":"/tags/Random-Forest/"},{"name":"Gradient Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"front-end","slug":"front-end","link":"/tags/front-end/"},{"name":"blog-building","slug":"blog-building","link":"/tags/blog-building/"},{"name":"Recommender Systems","slug":"Recommender-Systems","link":"/tags/Recommender-Systems/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Regularization","slug":"Regularization","link":"/tags/Regularization/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Topic Modeling","slug":"Topic-Modeling","link":"/tags/Topic-Modeling/"},{"name":"Evaluation Methods","slug":"Evaluation-Methods","link":"/tags/Evaluation-Methods/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Bayesian Statistics","slug":"Bayesian-Statistics","link":"/tags/Bayesian-Statistics/"}],"categories":[{"name":"Blogs","slug":"Blogs","link":"/categories/Blogs/"},{"name":"Anime","slug":"Anime","link":"/categories/Anime/"},{"name":"Enlightenment","slug":"Enlightenment","link":"/categories/Enlightenment/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"}],"pages":[{"title":"404","text":"I don‚Äôt know how you ended up here, but you have jumped over the edge of this blog. Maybe it‚Äôs the end of the internet and you can power off your machine now‚Ä¶ /@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@ @@@@@@@@@ ,@@@@@ @@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@ ,@@@@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@@@ @@@@@@@@@@@@ @@@@@@@@@ (@@@@@@@@@@@@ #@@@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@, @@@@@@@@@. @@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@ @@@@@@@@@ @@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@@@@@ @@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@ ‚Ä¶ or you climb back and read one of my recent posts :)","link":"/404.html"},{"title":"About Me","text":"Who am IMy name is Zhenlin Wang. I'm from Ningbo, China. Currently I'm a master student in the Machine Learning Department in Carnegie Mellon University. My academic interest and work passion lie across the fields of Data Science, Machine Learning/AI and Software Engineering (though they are somewhat connected under the field of Computer Science‚Ä¶). The path to my passion Before university, I was quite obsessed with applying mathematical models to the financial industry. I dreamed of being an equity researcher and predicting flows in the stock and derivatives market. In pursuit of this goal, I read an entire textbook on corporate finance and written all kinds of note on accounting models. However, I was wrong. When I entered the equity research team in the investment club in my university, I quickly realized that people were not \"relying\" on the models for judgement. They \"abuse\" these models to fit their assumptions in order to sell their investment plans to customers. This is not what I wanted. So I tried an alternative path - quantitative research. Being an applied math student, it\\'s fairly easy to imagine that I would be fascinated by the significant amount of math used in quantitative research team in the investment club. This stood up as a completely different track from equity research. I was learning so much data science and machine learning knowledge in this team. I knew I\\'ve found my true passion. In the following semester, I enrolled in a second major in Computer Science to learn more about fundamental CS knowledge. I started building up my statistics/data science skills. A youtube channel, StatQuest with Josh Starmer really helped me a lot. Until today, I\\'m glad to see it being supported by the community and is still diligently uploding new tutorials on various concepts in stats/DS/ML. Further down the path, I started exploring boundaries in some ML sub-fields, namly Bayesian Optimization, Bandit and Reinforcement Learning. I conducted several researches under the guidance of my supervisors, and luckily published some papers. I believe in the motto of Learn to live, Live to learn. No matter how old I am and where I am, I\\'ll keep myself challenged with learning new things. At current stage, expanding my passion from theoretical knowledge to real-life application is my major goal. Solving real-world problems with ML breakthroughs require the careful construction of tools and technologies. Thus, learning and practice software engineering has also become part of my life nowadays. Looking backI received my high school and undergradute education in Singapore. It is a second hometown to me. I have no words but only genuine gratitude and respect for this city state. In Singapore, I've met lots of excellent people, learnt countless lessons, and found my true love . My colorful youth life remains in Singapore forever (Ëä≥ÂçéÊ∞∏È©ªÊ≠§ÁãÆÂüé~). Leisure timeI listen and play musics sometimes. I used to play violin, but nowadays, I prefer harmonica as I can carry it anywhere easily. I go workout regularly. Since Pittsburgh has lots of tennis court, I'm planning on playing tennis with my friends as well. I love eating Thai food and Japanese food, my favorites are Pat Thai, Basil Chicken, Sushi/sashimi and Suki Yaki. An ACG FanI'm a Japanese culture lover, specifically it's ACG culture and it's history. I read manga, watch anime and used to play some galgame. I used to be quite into the anime merchandise (an otaku) but now I just treat it as an interest and watch some old animes for chilling, I'll casually write some reviews on animes I watched again recently. My favorite anime is Steins;Gate. It tells a story of a man leaping through time challenging destiny using a time machine. My favicon is also a tribute to this series. The perseverence of the main protagonist, Okabe Rintarou, motivated me to keep on despite various setbacks from time to time. Steins Gate - Okabe Rintarou This is the end this self-introduction (you call this an intro???), have fun browsing through my blogs. El Psy Congroo!","link":"/intro/index.html"},{"title":"Publication","text":"Best Arm Identification with Safety Constraints. Zhenlin Wang, Andrew Wagenmaker, Kevin Jamieson.The 25th International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.[arXiv] Max-min Grouped Bandits. Zhenlin Wang, Jonathan Scarlett.Association for the Advancement of Artificial Intelligence (AAAI), 2022.[arXiv]","link":"/publication/index.html"}]}