{"posts":[{"title":"About","text":"About MeI'm Zhenlin Wang (Criss). I graduated as an MS student from Machine Learning Department @ Carnegie Mellon University. I have a strong passion for machine learning and AI-powered softwares. Recently I'm drawn to performance optimization of deep learning systems and AI tools. In particular, I've been studying and playing around with distributed systems and MLOps for foundation models and large language models. At CMU, I collaborated with Bryan Lu, Yilong Qin and Andrew Shen on the problem of Out-of-Distribution Detection for deep learning models. Before CMU, I graduated with First Class Honor from National University of Singapore (NUS) with a B.S. double major in Applied Mathematics and Computer Science. There I was fortunate to work with Prof. Jonathan Scarlett and Bryan Low on multi-arm bandit, bayesian optimization and high-dimensional statistics. I also worked with Prof. Kevin Jamieson and a Phd advisor Andrew Wagenmaker from Computer Science Department @ University of Washington on Bandit and Reinforcement Learning problems. Engineering is an integral part of my life. I built an event-driven signal generation system at J.P. Morgan Securities LLC for Agency RMBS trading using Python and C++. I also led the design, implementation, deployment and monitoring of an ML-based price suggestion tool for eyos.one an IT-startup based in Singapore. About the blogsI started maintaining this website from my sophomore year in NUS. The original version used a minimal-mistake style and I've migrated into the hexo-icarus style on Aug 2022. The blogs here covers various topics in DS/AI/ML and software engineering are written here. The mathematical/technical details are presented. I often included some discussion about the pros/cons of the methodologies outlined in these blogs. Nonetheless, I don't think that any blog post can be mark &quot;DONE&quot; as new perspectives on these topics can always supplement what's on the posts. Thus, I consistently update these posts whenever I learn some new knowledge about the topics discussed in these posts. Disclaimer: I try my best to give credit for all sources I made referenecs to. If you found some parts in my post that were referenced from your work without credit, please kindly contact me so I can immediately correct my mistakes and make apology. Special thanksIn retrospect, I have to thank @Lei Mao for the inspiration to build a website from scratch. I followed him since 2018 when I read his motivating background story. When trying to customizing the pages, I learnt from @iMaeGoo, @Xinyu Liu and @Pengyuan Li, and I thank them for their great tutorials on website styling.","link":"/post/Zhenlin%20Wang/"},{"title":"CLI-nic","text":"IntroductionCLI-nic is an application to help medical supply managers keep track of medical products and storage. It is optimized for these managers to update product supply conditions and access critical product information quickly via fast typing and efficient Graphical User Interface interaction. It is my CS2103 Software Engineering project. I collaborated with Chan Qin Liang, Jeffrey Tan Jian Sheng, Lim Zheng Wei and Toh Yu Ting in an Agile style. We managed to build this app within 3 months with a sprint of 3 weeks. In addition to the app itself, we produced an extensive documentation (developer's guide and user's guide) for our app utilizing UML diagrams significantly. Major contributions: Implemented data models Implemented deletion and search modules factory method dependency injection adapter pattern DevOp related: Extensive unit testing/integration testing CI/CD Documentation Tech &amp; Methodology Iterative development Sprint Agile scrum and Kanban Practices Unit Testing / Integration Testing with JUnit Test CI/CD with GitHub Actions","link":"/post/projects/CLI-nic/"},{"title":"Code Analyzer","text":"IntroductionA Static Program Analyzer (SPA) is an interactive tool that automatically answers queries about programs. It is the final product of my CS3203 Software Engineering Project. Our team designed and implemented a SPA for a simplified programming language (SIMPLE) with specific query language (PQL). Here is a glimpse of our work. Major contributions: System design &amp; API specification Designed customized Postfix String Conversion algorithm for design extractor &amp; population Program Knowledge Base (PKB) development Fast lexical token querying via Indexing and Concurrency (C++ STL threads) DevOp related: Extensive unit testing/integration testing (100% line coverage) CI/CD management with CMake and Github workflow Documentation Tech &amp; Methodology C++ STL threads Unit Testing/Integration Testing with Catch System Testing Github workflow","link":"/post/projects/Code%20Analyzer/"},{"title":"Motion Prediction with Guided Diffusion","text":"IntroductionWe proposed a guided diffusion based method for Motion Forecasting task. The diffusion process uses the standard UNet architecture with 1D-convolution conditioned on past locations. We addressed the problem of a long-tailed data distribution using a max-norm scaling. Our model outperformed baseline methods in experiments using ArgoVerse 2 dataset. MembersAndrew Shen, Zhenlin Wang, Yilong Qin [Code][Report] Results Prediction Plots","link":"/post/projects/Diffusion/"},{"title":"MyPal","text":"IntroductionMyPal is a web application for students in campus to manage the interpersonal connections online. It was built by me and Ren Hao from National University of Singapore as part of the Orbital Program. We managed to design, implement, deploy and iterate our app during the summer of 2019 to improve our product's rating from Gemini (intermediate) to Apollo 11 (Advanced). [Certificate] Our initiative for building this app was to give college students the power to conveniently create, manage and safely store important details about the people they want to keep connected with in their ever-expanding social circle during their college life. Here is a glimpse of our application Major Features \"Relationship Info\" creation template Template to create a new info sheet about a person in one's social network Can be personlized by adding and removing elements in the template Automatic integration of information from the person's social media and contact book (through uploaded or online documents) Categorization and tagging Allow tags and categories to be applied on each connection for fast group viewing and relationship management Associate links to enable the user to switch to the social media to message a specific person Social Network Visualization Using network diagram to display various social circles the user is in （built with D3.js) Using Timeline to help the user recollect the events he/she had with certain people/group Weekly/Monthly Report Automatically generate a summary about the activities recorded on the website this week Produce reminder about friends' birthday and people they haven't got in touch with for a long time. Tech &amp; Methodology Specifically MERN stack (MongoDB, Express.js, ReactJS, Node.js) REST/GraphQL APIs","link":"/post/projects/MyPal/"},{"title":"Needle: High-performance DL System","text":"IntroductionNeedle is a Deep Learning framework with customized GPU and CPU backend in C++ and Python. This is an attempt to simulate PyTorch's imperative style, especially its way of auto-differentiation and computational graph traversal. In the meantime, we enable accelerated computing with custom ndarrays implementation via low level C++ CUDA programming. This enables tensor operations to run on GPUs and other specialized hardwares. Key contributions Modular DL framework Build models from scratch ResNet (Residual Blocks, Skip Connection, BatchNorm2D) LSTM (Cell State, Hidden State, Forget/Input/Output Gate, Activations) Transformer (Multi-Head Self/Cross Attention, LayerNorm, Dropout, Positional Encoding, FFN, Skip Connection, Attention Masking) Optimization with GPU &amp; CUDA SIMT Tensor Model Parallelism Register Tiling Block Tiling GPipe Best result: ~5x speedup in 4-core distributed training Tech Stack &amp; Methodology AcknowledgementThis project is inspired by 10-414/714 Deep Learning Systems by Carnegie Mellon University. Extensions based on this are built and are still under development (more to come!).","link":"/post/projects/Needle/"},{"title":"Starlink Tracking","text":"IntroductionStarlink Trajectory Visualization is a fun project I worked on to play around with front-end tools, especially D3.js. It allows users to input geographical locations and visualize the “real-time” trajectories of the satellites. Tech &amp; Methodology","link":"/post/projects/Starlink%20Trajectory/"},{"title":"SmartMall Discounted Electronic Shopping","text":"IntroductionA Spring Boot 3-powered, fully dockerized microservices application. [Code] Overall Structure Key services API Gateway (Spring Cloud) Service Discovery (Netflix Eureka) Inventory Service Notificaiton Service Order Service Product Service [TODO] Cart Service [TODO] Payment Service [TODO] Membership/Discount Service Key Features &amp; Technology Data Storage: MongoDB + PostgreSQL + Hibernate IPC: RestTemplate + WebClient Gateway: Spring Cloud Gateway Service Registration &amp; Discovery: Netflix Eureka Security: Keycloak + JWT Inter-service Communication: Kafka (Event-driven, asynchronous) + Resilience4J (Circuit Breaker, fault tolerance) Distributed Tracing: Zipkin + Sleuth Containerization: Docker + [TODO] Kubernetes Monitoring: Grafana + Prometheus Logging: Lombok SLF4J","link":"/post/projects/SmartMall/"},{"title":"Travel Planner","text":"IntroductionTravel Planner is a web application in Golang to help users automate travel plan suggestion, generation and archiving. I worked as the backend Tech Lead in a team of 10 people. I captured the business insight of leveraging GPT as a potentially reliable source of site recommendation and integrated it with TripAdvisor APIs to build a fully-automated site suggestion feature. [code][Slides] Features AI-powered Site Recommendation Google Map API powered route palnning Travel Plan generation and archiving Appealing UI Leading the Backend TeamAs a Tech Lead for backend, my role spans across setting up the development plan, communication and syncing, making decisions on tools and building the skeleton for the backend. To better manage the team, I also used Trello to ensure everyone is on track and bugs get fixed ASAP. We use a sprint of 1 week during Agile development, and have team meetings every Monday and Thursday. Here's an example of our Kanban Board Tech Stack &amp; Methodology API lib: TripAdvisor API, GoogleMap API System Design: N-Tier Architecture Security: JWT Authentication Data: Gorm + MySQL (on AWS RDS), Optimistic Concurrency Control Algorithms: All Pair Shortest Path (APSP) - Floyd-Warshall Deployment: Docker + Kubernetes, Horizontal Pod Autoscaling (HPA) and Istio service mesh","link":"/post/projects/Travel%20Planner/"},{"title":"Twitch+","text":"IntroductionTwitch+ is a web application to track favorite Twitch resources for users. I deployed it on both heroku (lightweight version) and on GCP (more scalable and stable). This is an ongoing project which I keep thinking of new featuers and iteratively update my work. Here are some features I have built. [code] Custom game search Multiple resource extraction (Stream/Video/Clip) Like/unlike &amp; Favorite panel Recommendations &amp; Hot games list Tech Stack &amp; Methodology REST APIs with Java servlets API lib: Twitch API Security :JWT Authentication Data: MySQL query optimization, HikariCP connection pool, Hibernate lazy loading AI: Recommender system - Content-based filtering, Matrix factorization and Approximate Nearest Neighbor (ANN) Cloud: GCP (Compute engine, Cloud Storage, BigQuery)","link":"/post/projects/Twitch_Plus/"},{"title":"A Regex Tutorial","text":"IntroductionRecently I've been working on software developement projects and learning some NLP algorithms. Then regex came to my attention as a powerful string processing tool. It is so useful that I have to utilize the techniques almost everyday in my learning and work. Nonetheless, I think I might have a chance of leaving it to rot after some time. To maintain good memory of the syntax, I decided to create this blog, to both teach my future self and all of you interested readers. Without further a do, let's begin. Regex Full name: regular expression Create/search for a specific pattern in a string Very useful for text editing/file searching/phrase grouping/etc terminalogies raw string: a raw string in python is just string prefixed with 'r', it tells python not to handle back slashes in any special way MetaCharacters: symbols in the search pattern to create variated pattern Useful website: [1] Regex101 In the following, I'll outline all keywords used in the expression 1. Special sequences with backslash \\d: digit(0-9) \\D: Not a Digit(0-9) \\w: Word character (a-z, A-Z, 0-9 _) \\W: Not a word character \\s: Whitespace(space, tab, blank line) \\b: Word boundary \\B: Not a word boundary \\A: Start of string, alternative representation: ^ \\Z: End of string, alternative representation: $ \\g&lt;id&gt;: Matches a previously defined group 2. MetaCharacters: \\: Escape special characters .: Matches any character $: Matches end of string ^: Matches start of string []: Matches characters in brackets [^ ]: Matches characters not in brackets |: Either or (): Group 3. Quantifiers: *: 0 or more +: 1 or more ?: 0 or 1 {m}: Exactly m times {n,}: Min n times {m,n}: From m to n times, as many as possible {m,n}?: From m to n times, as few as possible Demonstrate the idea of raw string12345print('\\tTab')print(r'\\tTab')# this gives the following output&gt;&gt;&gt; Tab&gt;&gt;&gt; \\tTab Let's try out some regex usage by initializing the following strings 12345678910111213141516171819202122232425262728293031323334353637383940414243import retext_to_search = '''abcdefghijklmnopqurtuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890Ha HaHaMetaCharacters (Need to be escaped):. ^ $ * + ? { } [ ] \\ | ( )coreyms.com321-555-4321123.555.1234123*555*1234800-555-1234900-555-1234somebody123@gamil.comtest@test.comsome@qq.eduMr. SchaferMr SmithMs DavisMrs. RobinsonMr. TFirstLastFirst.LastFirst LastFirst..LastFirst...LastFirst....Last'''urls = '''https://www.google.comhttp://baidu.com'''sentence = 'Start a sentence and then bring it to an end' Next let's apply 2 functions test and test2 that we use to apply regex on text_to_search and sentence respectively: 123456789101112131415def test(s): pattern = re.compile(s) matches = pattern.finditer(text_to_search) for match in matches: print(match) # output: span(x, y) -&gt; the location [x,y] in the string, match -&gt; matched string print(\"\")def test2(s): pattern = re.compile(s) matches = pattern.finditer(sentence) for match in matches: print(match) # output: span(x, y) -&gt; the location [x,y] in the string, match -&gt; matched string print(\"\") Then we can try the following code and play around with it 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# use of \\test(r'ms\\.com')test(r'\\\\')print(\"===========================\")# use of \\dtest(r\"\\d\")print(\"===========================\")# use of \\wtest(r\"\\w\")print(\"===========================\")# use of ^test2(r'^Start')test2(r'Start^a') # no matchtest2(r'^a') # no matchprint(\"===========================\")# use of $test2(r'end$') # no matchtest2(r'$Start') # no matchtest2(r'a$') # no matchprint(\"===========================\")# use word boundary usagetest(r'\\bHa')test(r'\\BHa')test(r'\\bHa\\b')print(\"===========================\")# use of .# warning: . does not takes \\n into accounttest(r'.')test(r'.*')print(\"===========================\")# usage of group ()test(r'\\d\\d\\d.\\d\\d\\d.\\d\\d\\d\\d')test(r'(\\d*).\\d\\d\\d.\\d\\d\\d\\d')print(\"===========================\")# use of |test(r'First( |\\.)Last')print(\"===========================\")# limit the value in pattern []test(r'([6-9]\\d*).(\\d*).(\\d*)')test(r'[a-zA-Z]')test(r'[^a-zA-Z]') # take NOTtest(r'First[ \\.]Last')print(\"===========================\")# limit the size in pattern using {}test(r'(\\d{3}).\\d{3}.\\d{4}')test(r'\\w{4,8}')test(r'\\w{4,8}?')print(\"===========================\")# use of ?test(r'First\\.?Last') # has 1 or 0 .test(r'First\\.?\\sLast')test(r'555-?')print(\"===========================\")# use of +test(r'First\\.+Last')print(\"===========================\")# use of *test(r'First\\.*Last')print(\"===========================\")# Now let's try a combination of the above methodstest(r'\\d{3}[-.]\\d{3}[-.]\\d{4}') SubstitutionTo substitute existing characters in a string, we need to specify a pattern directly, and we then use the .sub method. 123pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)')subbed_urls = pattern.sub(r'\\2\\3',urls)print(subbed_urls) The pattern can also be used to direcly find all the occurances, so that we don't need to define test and test2 by ouselves. 123456789101112131415161718192021222324252627282930313233pattern = re.compile(r'(\\w+)-(\\w+)-(\\w+)')matches1 = pattern.findall(text_to_search)for match in matches1: print(match)pattern2 = re.compile(r'\\d{3}-\\d{3}-\\d{4}')matches2 = pattern2.findall(text_to_search)for match in matches2: print(match)### Check match at the begin of the stringpattern = re.compile(r'Start')matches = pattern.match(sentence)print(matches)pattern = re.compile(r'sentence')matches = pattern.match(sentence)print(matches)### Case insensitive modepattern = re.compile(r'start', re.I) # re.I is a short version of re.IGNORECASEmatches = pattern.match(sentence)print(matches)### Actual functionality demo# Email searchtest(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9_.+-]+\\.[a-zA-Z0-9_.+-]+')test(r'(\\w+)@(\\w+)\\.(\\w+)')pattern = re.compile(r'(\\w+)@(\\w+)\\.(\\w+)')matches = pattern.finditer(text_to_search)for match in matches: print(match.group(1), match.group(2), match.group(3))","link":"/post/software/regex/"},{"title":"A good Python projecttemplate to use as starting point","text":"OverviewWhen you build up an open source Python project, the first thing to keep in mind is its usefulness. It should really solve some unsolved problems. And right after this, is how you plan and build up a solution step by step. Designing the codebase is one big part of the planning. Everyone open source contributer should have his or her own way of starting a project, but not everyone is good at maintaining the codebase in the long run. One big reason for that is they lack the mindset of building a robust architecture for their code. In this blog, I will cover the major parts every python project (at least for me) should include. These components can be redundant. But redundancy is good to me, as subtraction is easier than addition especially when it comes to simple things. Heads up that this is more for references. Most of the content are inspired from other coders on GitHub who have accumulated the good habit of building robust codebase template over the years. I'm here just to learn, ABSORB and implement them in my own style. Project structureThe first thing is actually codebase structure. My template structure looks like the following: 123456789101112131415├── .github│ └── workflows├── .pre-commit-config.yaml├── LICENSE├── README.md├── .gitignore├── pyproject.toml├── .readthedocs.yml├── MANIFEST.in├── Makefile├── requirements.txt├── setup.py├── docs├── tests└── src Here: src contains your project code, tests include all your test cases, docs include your documentation of the code (very often used as the api doc, a common choice is readthedoc with sphinx). .gitignore, README.md, pyproject.toml, .readthedocs.yml, MANIFEST.in are config and instruction files for you to setup your project and potentially make it into pypl. This also enables quick prototying of environment which I'll talk about later. requirements.txt keeps a list of major packages/dependencies. This can be split into prod/dev/etc as you want. Makefile is to automate many lengthy command-line code into short make xxx command. .github/workflow is the CI/CD pipeline based on GitHub Actions/Jenkins and other integration tools .pre-commit-config.yaml is the file the carries out pre-commit code validations LICENSE is just there in case your code will be used by many people (which is gooood!) EnvironmentA basic thing when it comes to setup environment is to always start with a new virtual environment. This helps separate dependencies for different projects. There are three major ways. direcly using virtualenv 12345678# Create a virtual environmentpython -m venv .venv# Activate the virtual environmentsource .venv/bin/activate# Upgrade pippip install --upgrade pip Use poetry dependency manager 1234567891011# Create a poetry projectpoetry init --no-interaction# Add numpy as dependencypoetry add numpy# Recreate the project based on the pyproject.tomlpoetry install# To get the path to poetry venv (for PyCharm)poetry env info With our (virtual) environment set up and activated, we can proceed to install python packages. To keep our production environment as light as possible, we’ll want to separate the packages needed for dev and prod: dev: These are only used for development (e.g., testing, linting, etc.) and are not required for production.prod: These are needed in production (e.g., data processing, machine learning, etc.). 1234567891011# Install dev packages which we'll use for testing, linting, type-checking etc.pip install pytest pytest-cov pylint mypy codecov# Freeze dev requirementspip freeze &gt; requirements.dev# Install prod packagespip install pandas# Freeze dev requirementspip freeze &gt; requirements.prod use Docker as a Dev Environment instead 1234567891011121314151617181920ARG BASE_IMAGE=python:3.8FROM ${BASE_IMAGE} as baseLABEL maintainer='eugeneyan &lt;dev@eugeneyan.com&gt;'# Use the opt directory as our dev directoryWORKDIR /optENV PYTHONUNBUFFERED TRUECOPY requirements.dev .COPY requirements.prod .# Install python dependenciesRUN pip install --upgrade pip \\ &amp;&amp; pip install --no-cache-dir wheel \\ &amp;&amp; pip install --no-cache-dir -r requirements.dev \\ &amp;&amp; pip install --no-cache-dir -r requirements.prod \\ &amp;&amp; pip list then run 1DOCKER_BUILDKIT=1 docker build -t dev -f Dockerfile . to setup the environment.Now we can run bash using the docker container (after mounting current dir to /opt folder) via 1docker run --rm -it -v $(pwd):/opt bash testing + coverageYou'll need to thoroughly test your code. In generally, when your codebase involve little infra dependency, and isn't heavy on data/model ingestion/output, most of tests will still be unit tests. pytest and unittests are your best friends. In the meantime, never forget to ensure high test coverage with the help of coveragepy and codecov. A little badge can be attached to README.md (check this guide) once you've robustly tested your code, which actually suggests that you're a responsible engineer! linting + code quality ensuring code consistency with lintingLinters analyze code to flag proramming errors, bugs, and deviations from standards. Linting leads to good code quality. As you use linting to correctly format your code, you are forming the good having of following a good coding style as well. Many people use either pylint and flake8 for linting. Note that very often you may want to ignore certain patterns in linting standard that your project doesn't agree with. There are many ways to configure it properly. In the current folder structure I suggested, you should edit in pyproject.toml. Note, for flake8, you need to install flake8-pyproject package independently for the configuration to work property. python coding style (PEP8)Sometimes linting also helps to check with particular function format. For example, pylint and flake8 both require function/class annotations. But you'll need to style your annotations correctly: 12345678910def sample_function(input1: int) -&gt; Any: \"\"\"Description of what the function does Args: input1: some input explanation Returns: Some output explanation \"\"\" ... typing checksThe Python runtime does not enforce type annotations; it’s dynamically typed and only verifies types (at runtime) via duck typing. Nonetheless, we can use type annotations and a static type checker to verify the type correctness of our code. mypy is the most widely adopted option from my impression. In the meantime, consider using pydantic for a more OOP styled typing. command automation with makeMakefile is a life-saver when we have many tasks to run before committing code. When each task is a long command-line code, it can be hard to remember and go through your terminal history to retrieve it. Hence we can just define tasks using the Makefile as follows: 12345678910111213141516171819202122232425262728293031323334353637383940.PHONY: refresh build install build_dist json release lint test cleanrefresh: clean build install lintbuild: python -m buildinstall: pip install .build_dist: make clean python -m build pip install dist/*.whl make testjson: python example/generate_examples.pyrelease: python -m twine upload dist/*lint: flake8 src/ tests/ example/ --exclude \"src/db/*\" --count --statistics mypy src/ --exclude 'src/.*'test: . .venv/bin/activate &amp;&amp; py.test tests --cov=src --cov-report=term-missing --cov-fail-under 95clean-pyc: find . -name '*.pyc' -exec rm -f {} + find . -name '*.pyo' -exec rm -f {} + find . -name '*~' -exec rm -f {} + find . -name '__pycache__' -exec rm -fr {} +clean-test: rm -f .coverage rm -f .coverage.*clean: clean-pyc clean-test and run any of make test, make clean, make lint tasks to complete the command-line code defined under it. pre-commit and pre-pushWhen it comes down to committing your code and pushing your code (via PR for e.g.), things get complicated as people often make mistakes in linting/typing/formatting. When it happends, pre-commit automations and checks help us resolve those problems to some extent. We can use a .pre-commit-config.yaml file to help us make corrections and find errors asap (may have overlap with the make-automation method) before we even commit our code to local branch. 1234567891011121314151617181920212223242526272829303132333435363738394041424344repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v4.5.0 hooks: - id: trailing-whitespace - id: end-of-file-fixer - id: check-yaml - id: debug-statements - id: double-quote-string-fixer - id: name-tests-test - id: requirements-txt-fixer - repo: https://github.com/asottile/setup-cfg-fmt rev: v2.5.0 hooks: - id: setup-cfg-fmt - repo: https://github.com/asottile/reorder-python-imports rev: v3.12.0 hooks: - id: reorder-python-imports exclude: ^(pre_commit/resources/|testing/resources/python3_hooks_repo/) args: [--py39-plus, --add-import, \"from __future__ import annotations\"] - repo: https://github.com/asottile/add-trailing-comma rev: v3.1.0 hooks: - id: add-trailing-comma - repo: https://github.com/asottile/pyupgrade rev: v3.15.1 hooks: - id: pyupgrade args: [--py39-plus] - repo: https://github.com/hhatto/autopep8 rev: v2.0.4 hooks: - id: autopep8 - repo: https://github.com/PyCQA/flake8 rev: 7.0.0 hooks: - id: flake8 - repo: https://github.com/pre-commit/mirrors-mypy rev: v1.9.0 hooks: - id: mypy additional_dependencies: [types-all] exclude: ^testing/resources/ For push actions, we can define a check in the .github/workflow/test.yml to run the checks: 1234567891011121314# .github/workflows/tests.ymlname: Testson: pushjobs: tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v1 with: python-version: 3.8 architecture: x64 - run: make setup-venv - run: make checks This helps us find problems before reviewing any code to start with. SummaryThese are the first few steps I take to build up a robust codebase. It came a long way from learning the codebase and blogs from smart and selfless engineers who're willing to share their experience and code online. I'm truly grateful of these people. References Eugene Yan Tian Gao","link":"/post/software/build-a-complete-python-project/"},{"title":"A fundamental course for Data Engineering","text":"OverviewIn this blog, I'll discuss about a variety of fundamental concepts in data processing. While remembering them is pointless (since the true data engineers learn by practice), I try to give a clear view of different aspects in handling the data, this includes: Data Storage Where data is stored Data ETL Extracting / Transforming / Loading Data Properties Source types and information system types Data Processing Types How different data are processed differently Data Cleansing What if the data is not well-organized Database Compliance ACID or BASE? Data Storage Data lake: a centralized repository that allows you to store structured, semistructured, and unstructured data at any scale. Single source of truth Store any type of data, regardless of structure Example: AWS Data Lake: Harness the power of purpose-built analytic services for a wide range of use cases, such as interactive analysis, data processing using Apache Spark and Apache Hadoop, data warehousing, real-time analytics, operational analytics, dashboards, and visualizations. Data warehouse: A data warehouse is a central repository of structured data from many data sources. This data is transformed, aggregated, and prepared for business reporting and analysis. ETL (Extract, Transform, Load) operations before stored into data warehouse Data is stored within the data warehouse using a schema. Data Mart: A subset of data from a data warehouse is called a data mart. Data marts only focus on one subject or functional area. A Data Warehouse might contain all relevant sources for an enterprise, but a Data Mart might store only a single department’s sources. Because data marts are generally a copy of data already contained in a data warehouse, they are often fast and simple to implement. Traditional data warehousing: pros and cons Pros Cons Fast data retrieval Costly to implement Curated data sets Maintenance can be challenging Centralized storage Security concerns Better business intelligence Hard to scale to meet demand Data Warehouse vs Data Lake Factors Data warehouse Data Lake Data Relational from transactional systems, operational databases, and line of business applications Non-relational and relational from IoT devices, websites, mobile apps, social media, and corporate applications Schema Designed prior to implementation (schema-on-write) Written at the time of analysis (schema-on-read) Price/performance Fastest query, higher cost storage Fast, low-cost storage Data quality Highly curated data that serves as the central version of the truth Any data, which may or may not be curated (e.g., raw data) Users Business analysts Data scientists, data developers, and BA Analytics Batch reporting, BI, and visualizations Machine learning, predictive analytics, data discovery, and profiling. ETL Basics Extracting data There are four key areas you must plan for. You must identify \"where\" all of the source data resides. This may be data stored on-premises by your company but can also include data that is found online. You must carefully plan \"when\" the extraction will take place due to the potential impact of the copy process on the source system. You must plan for \"where\" the data will be stored during processing. This is generally referred to as a staging location. You must plan for \"how often\" the extraction must be repeated. Transforming data This phase involves using a series of rules and algorithms to massage the data into its final form. Data cleansing also occurs during this part of the process. It can be basic or advanced: This could be replacing NULL values with a zero or replacing the word female with the letter F; Or applying business rules to the data to calculate new values. Filtering, complex join operations, aggregating rows, splitting columns Loading data The planning steps you took in the transformation phase will dictate the form the final data store must take. This could be a database, data warehouse, or data lake. Data Properties Data Source: In each of these data sources, data is stored in a specific way. Some data sources use a schema to organize content and indexes to improve performance. Others organize data in a more flexible way and are called schemaless. Schemaless data sources still use indexes to improve performance. Types of data source: Structured data: stored in a tabular format, often within a database management system (DBMS). Organized based on a relational data model Defines and standardize data elements The downside to structured data is its lack of flexibility: you must reconfigure the schema to allow for this new data, and you must account for all records that don’t have a value for this new field. Semistructured data (NoSQL) Stored in the form of elements within a file. (CSV, JSON, XML, etc) Organized based on elements and the attributes that define them. No pre-defined schemas. Semistructured data is considered Have a self-describing structure: Each element is a single instance of a thing, such as a conversation. The attributes within an element define the characteristics of that conversation. Each conversation element can track different attributes. The trade-off is with analytics. It can be more difficult to analyze semistructured data when analysts cannot predict which attributes will be present in any given data set. Unstructured data Stored in the form of files. This data doesn't conform to a predefined data model and isn't organized in a predefined manner. Can be text-heavy, photographs, audio recordings, or even videos. Need to be preprocessed to perform meaningful analysis. Types of information systems There are two main ways—known as information systems—of organizing data within a relational database. The data can be organized to focus on the storage of transactions or the process of analyzing transactions. Online transaction processing (OLTP) databases: operational databases, primary focus being on the speed of data entry These databases are characterized by a large number of insert, update, and delete operations. based on ensuring rapid data entry and updates. The effectiveness of an OLTP system is often measured by the number of transactions per second. Online analytical processing (OLAP) databases: data warehouses, primary focus being the speed of data retrieval through queries. These databases are characterized by a relatively low number of write operations and the lack of update and delete operations based on the types of queries and other analytics that will be performed using the data. The effectiveness of an OLAP system is often measured by the response time of query results. OLTP VS OLAP: Characteristic OLTP OLAP Nature Constant transactions (queries/updates) Periodiclarge updates, complex queries Examples Accounting database, online retail transactions Reporting, decision support Type Operational data Consolidated data Data retention Short-term (2-6 months) Long-term (2-5 years) Storage Gigabytes (GB) Terabytes (TB)/petabytes (PB) Users Many Few Protection Robust, constant data protection and fault tolerance Periodic protection Processing Types Categories and types: By Collection: Batch: Velocity is very predictable with batch processing. It amounts to large bursts of data transfer at scheduled intervals. Periodic: Velocity is less predictable with periodic processing. The loss of scheduled events can put a strain on systems and must be considered. Near real-time: Velocity is a huge concern with near real-time processing. These systems require data to be processed within minutes of the initial collection of the data. This can put tremendous strain on the processing and analytics systems involved. Real-time: Velocity is the paramount concern for real-time processing systems. Information cannot take minutes to process. It must be processed in seconds to be valid and maintain its usefulness. By Processing: Batch and periodic: Once the data has been collected, processing can be done in a controlled environment. There is time to plan for the appropriate resources. Near real-time and real-time: Collection of the data leads to an immediate need for processing. Depending on the complexity of the processing (cleansing, scrubbing, curation), this can slow down the velocity of the solution significantly. Plan accordingly. Data acceleration Another key characteristic of velocity on data is data acceleration, which means the rate at which large collections of data can be ingested, processed, and analyzed. Data acceleration is not constant. It comes in bursts. Take Twitter as an example. Hashtags can become hugely popular and appear hundreds of times in just seconds, or slow down to one tag an hour. That's data acceleration in action. Your system must be able to efficiently handle the peak of hundreds of tags a second and the lows of one tag an hour. Attributes of batch and stream processing Batch data processing Stream data processing Data scope over all or most of the data over data within a rolling time window, or on just the most recent data record Data size Large batches of data Individual records/ micro batches consisting of a few records Latency Minutes to hours Seconds or milliseconds Analysis Complex analytics Simple response functions, aggregates, and rolling metrics Processing big data streams There are many reasons to use streaming data solutions. In a batch processing system, processing is always asynchronous, and the collection system and processing system are often grouped together. With streaming solutions, the collection system (producer) and the processing system (consumer) are always separate. Streaming data uses what are called data producers. Each of these producers can write their data to the same endpoint, allowing multiple streams of data to be combined into a single stream for processing. Another huge advantage is the ability to preserve client ordering of data and the ability to perform parallel consumption of data. This allows multiple users to work simultaneously on the same data. Data Cleansing Curation: the action or process of selecting, organizing, and looking after the items in a collection. Data integrity: the maintenance and assurance of the accuracy and consistency of data over its entire lifecycle. Different types of integrity: Referential integrity: the process of ensuring that the constraints of table relationships are enforced. Domain integrity: the process of ensuring that the data being entered into a field matches the data type defined for that field.. Entity integrity: the process of ensuring that the values stored within a field match the constraints defined for that field. Maintaining Integrity accross steps of a data lifecycle: Creation phase: Ensure data accuracy. Mainly involves software audits/data generation audits/data Aggregation phase: Ensure the metrics computed are well-defined. Bad practice such as poor naming of metrics Storage phase: Ensure stable data are not changed and volatile data are only changed by authorized personels Access phase: System should be read-only and audited regularly for anomalies in access pattern Share pahse: The phase where veracity get truly examined Archive phase: Security of the data is the most important factor. Ensure limited access and read-only Data veracity: the degree to which data is accurate, precise, and trusted. A few best practices to help you identify data integrity issues Know what clean looks like Before you do anything else, you must come to a consensus on what clean looks like. Some businesses deem clean data to be data in its raw format with business rules applied. Some businesses deem clean data as data that has been normalized, aggregated, and had value substitutions applied to regulate all entries. These are two very different understandings of clean. Be sure to know which one you are aiming for. Know where the errors are coming from As you find errors in the data, trace them back to their likely source. This will help you to predict workloads that will have integrity issues. Know what acceptable changes look like From a purely data-centric view, entering a zero in an empty column may seem like an easy data cleansing decision to make, but beware the effects of this change. Know if the original data has value In some systems, the original data is no longer valuable once it has been transformed. However, in highly regulated data or highly volatile data, it is important that both the original data and the transformed data are maintained in the destination system. Database Schemas A data schema is the set of metadata used by the database to organize data objects and enforce integrity constraints. The schema defines attributes of the database, providing descriptions of each object and how it interacts with other objects within the database. One or more schemas can reside on the same database. Logical schemas: focus on the constraints to be applied to the data within the database. This includes the organization of tables, views, and integrity checks. Physical schemas: focus on the actual storage of data on disk or in a cloud repository. These schemas include details on the files, indices, partitioned tables, clusters, and more. Information Schemas: An information schema is a database of metadata that houses information on the data objects within a database. Given the proper permissions on the database, you can query the information schema to learn about the objects within the database. When queries are executed, this information is used to ensure the best optimization for the query. The information schema can also be used in maintenance of the database itself. Database Compliance ACID: ACID is an acronym for Atomicity, Consistency, Isolation, and Durability Atomicity: Ensures that your transactions either completely succeed or completely fail. No one statement can succeed without the others Consistency: Ensures that all transactions provide valid data to the database. If any single statement violates these checks, the whole transaction will be rolled back Isolation: Ensures that one transaction cannot interfere with another concurrent transaction Data durability: Ensures your changes actually stick. Once a transaction has successfully completed, durability ensures that the result of the transaction is permanent even in the event of a system failure. This means that all completed transactions that result in a new record or update to an existing record will be written to disk and not left in memory. Mainly to ensure veracity in a structured database The goal of an ACID-compliant database is to return the most recent version of all data and ensure that data entered into the system meets all rules and constraints that are assigned at all times. BASE: BASE is an acronym for Basically Available Soft state Eventually Consistent. Basically Available: allows for one instance to receive a change request and make that change available immediately. The system will always guarantee a response for every request. However, it is possible that the response may be a failure or stale data, if the change has not been replicated to all nodes. Soft State: In a BASE system, there are allowances for partial consistency across distributed instances. For this reason, BASE systems are considered to be in a soft state, also known as a changeable state. Eventually Consistency: The data will be eventually consistent. In other words, a change will eventually be made to every copy. However, the data will be available in whatever state it is during propagation of the change. BASE supports data integrity in non-relational databases This consistency is mostly concerned with the rapid availability of data To ensure the data is highly available, changes to data are made available immediately on the instance where the change was made. However, it may take time for that change to be replicated across the fleet of instances. ACID vs BASE: ACID BASE Strong consistency Weak consistency – stale data is OK Isolation is key Availability is key Focus on committed results Best effort results Conservative (pessimistic) availability Aggressive (optimistic) availability","link":"/post/blogs/data/big-data-guidelines/"},{"title":"Apache Spark: Only the simple answer","text":"OverviewIn this post, I'm just gonna discuss some fundational things I learned about big data with Apache Spark. Personally, I'm just a bit interested in this topic, and do not aim to really become a big data professional (not yet~). It does take tremendous effort to learn Spark well, not to mention the entire big data ecosystem. I'll update this post if I try out some new projects that really apply Spark and its APIs in a deep manner, but for now, let's just talk about some basics of Spark. Apache Spark vs Hadoop MapReduce MapReduce is a programming model, Spark is a processing framework Apache Spark MapReduce Processing Type Process in batches and in real-time Process in batches only Speed nearly 100x faster slower due to large scale data processing Storage store data in RAM i.e. in-memeory (easier to retrieve) Store in HDFS, longer time to retrieve Memory dependence caching (for RDD) and in-memory data storage disk-dependent Important Components of Spark Ecosystem Core components[MOST IMPORTANT]: Spark Core (Caching, RDD, DataFrames, Datasets || Transformations and Actions) Memory management Fault recovery Task dispatching Scheduling and monitoring jobs Spark SQL (Data Query) Used for structured and semi-structured data processing Usually used for in-memory processing Top level: DataFrame DSL(domain specific language) ; Spark SQl and HQL(Hive) Level 2: DataFrame API Level 3: Data Source API Base Level: CSV + JSON + JDBC(Java Database connectivity) + etc storage/query Spark Streaming (Stream data) Spark MLlib (Machine Learnig toolkits) GraphX (Graph Processing models) Langauge Support Java Python Scala R Spark Cluster Managers Standalone mode: Default choice, run in FIFO order, and each application will try to use all available nodes Apache YARN (Hadoop Integration): This is the resource manager of Hadoop, use this will help spark to connect to hdfs better Kubernetes: For deployment, scaling and management of containerized applications RDD Resilient Distributed Datasets RDDs are immutable, fault-tolerant distributed collections of objects that can be operated in parallel (split into partition and executed on different nodes of a cluster) 2 major types of operations: Transformation: map, filter, join, union, etc. Yield a new RDD containing the result Action: reduce, count, first, etc. Return a value after running a computation on RDD Works in a similar style as java Stream How Spark runs applications with the help of its architectureSTART EXECUTION In driver program Spark applications runs as independent processes (i.e. split tasks) running across different machines Spark sessions/context as the entry point of the application Driver: Record the flow of the application Resource manager/Cluster manager (DAG Scheduler at the backend) The driver program request resources from the clusters Assign task to workers, one task per partition Knows each step of the application for in-memory processing Worker node processing slave (node manager) grands the request to usage of resources from resource manager The request is called Container, within the Container, executor process is launched to apply tasks to its unit of work to the dataset in its partition and outputs a new partition dataset because iterative algorithms apply operations repeated to the data, the benefit from caching datasets across iterations END EXECUTION Results are sent back: worker node -&gt; container -&gt; manager -&gt; driver program/disk The entire execution is lazily evaluated (transformations not evaluated until an action is called) What is a Parquet file and what are its advantages Parquet is a columnar format that is supported byh several data processing systems (default data type for spark) Advisable to use if not all fields/columns in the data are used Advantages able to fetch specific columns for access consumes less space follow type-specific encoding limited I/O operations What is shuffling in Spark? When does it occur? Shuffling is the process of redistributing data across partitions that may lead to data movement across executors Occurs while joining two tables or while performing byKey operations such as GroupByKey or ReduceByKey Notes on Big Data Learning Journey (for those who truly want a Big Data job and for my future )To excel in the Big Data domain, you should master the following skills: Java &amp; Scala Understand source code for related package/API development Linux Everyone should know about shell scripts, bash and linux commands Hadoop It's a broad topic, but first of all, the ability to read whatever source code for an API is a must Hive Know how to use it, understand how the SQL is converted in base code and how to optimize the query process or MapReduce/Spark Operations Spark The core developement process (But honestly, most of the time it is still SQL) Kafka High-volumn stream data processing; Good to use when you have high concurrency Flink Faster than Spark sometimes. However, you should not discard Spark. Learn based on what you need. HBase Know your database knowledge. Understand its fundamental knowledge Zookeeper Distributation cluster data coordination services; Know how to use, better to understand the basic YARN Cluster resources management; Know how to use Sqoop, Flume, Oozie/Azkaban Know how to use Different cluster managers Spark Standalone mode by default, applications submitted to the standalone mode cluster will run in FIFO order, and each application will try to use all available nodes Apache Mesos an open sources project to manage computer clusters, and can also run Hadoop applications YARN Apache YARN is the cluster resource manager of Hadoop 2. Kubernetes an open-source system for automating deployment, scaling and management of containerized applications","link":"/post/blogs/data/big-data-spark/"},{"title":"An overview of Big Data Analytics","text":"OverviewData analysis is the process of compiling, processing, and analyzing data so that you can use it to make decisions. Let's start with the well-known 5 V's and proceed down to more concepts in Data analytics. The 5 V's Volume – data storage: When businesses have more data than they are able to process and analyze, they have a volume problem. There are three broad classifications of data source types: Structured data (10%):: Organized and stored in the form of values that are grouped into rows and columns of a table. Semistructured data (10%):: Often stored in a series of key-value pairs that are grouped into elements within a file. Unstructured data (80%):: Not structured in a consistent way. Some data may have structure similar to semi-structured data but others may only contain metadata. Velocity - data processing: When businesses need rapid insights from the data they are collecting, but the systems in place simply cannot meet the need, there's a velocity problem. 2 types of processing: Batch processing (Large burst of data): For initial insights and real-time feedback Stream Processing (Tiny burst of data): For deep insights using complex analysis Variety - data structure and types When your business becomes overwhelmed by the sheer number of data sources to analyze and you cannot find systems to perform the analytics, you know you have a variety problem. Veracity - data cleasing and transformation When you have data that is ungoverned, coming from numerous, dissimilar systems and cannot curate the data in meaningful ways, you know you have a veracity problem Value - data insight and business intelligence When you have massive volumes of data used to support a few golden insights, you may be missing the value of your data Data Analytics Definitions Information analyticsInformation analytics is the process of analyzing information to find the value contained within it. This term is often synonymous with data analytics. Operational analyticsOperational analytics is a form of analytics that is used specifically to retrieve, analyze, and report on data for IT operations. This data includes system logs, security logs, complex IT infrastructure events and processes, user transactions, and even security threats. 5 forms of analysis: Descriptive (Data Mining): Human Judgement Insight - Requires highest amount of human effort and interpretation - Focus on \"Whatdunit\" Diagnostic Human Judgement Insight - Used to compare historic data with other data to find dependencies and patterns that lead to answers - Focus on \"Whydunit\" Predictive Human Judgement Insight - Focus on Future prediction - Uses the results of descriptive and diagnostics analysis to predict future events and trends - Accuracy highly dependent on the quality of data and stability of environment setup Prescriptive Human Judgement Insight Decision Action - Focus on Solution finding - Used to prescribe actions to taken given the data provided - Requires input from all other forms of analytics, combined with rules and contraints-based optimization to make relevant suggestion. - This part is likely to be automated via machine learning Cognitive Human Judgement Insight Decision Action - Focus on recommended actions (not to be confused with \\\"solution\\\" to problems) - Try to mimic what the human brain does in problem solving - Generates hypothesis from the existing data, connections and contraints. Answers are provided in the form of recommendation and a confidence ranking Analytic services and velocity Batch analytics: Typically involves querying large amounts of “cold” data. Batch analytics are implemented on large data sets to produce a large number of analytical results on a regular schedule. Interactive analytics: Typically involves running complex queries across complex data sets at high speeds. This type of analytics is interactive in that it allows a user to query and see results right away. Batch analytics are generally run in the background, providing analytics in the form of scheduled report deliveries. Stream analytics: Requires ingesting a sequence of data and incrementally updating metrics, reports, and summary statistics in response to each arriving data record. This method is best suited for real-time monitoring and response functions. Streaming data processing requires two layers: a storage layer and a processing layer. The storage layer needs to support record ordering and strong consistency to enable fast, inexpensive, and re-playable reads and writes of large streams of data. The processing layer is responsible for consuming data from the storage layer, running computations on that data, and then notifying the storage layer to delete data that is no longer needed. Best practices for writing reports Gather the data, facts, action items, and conclusions. Identify problems and formats Identify the audience, expectations they have, and the proper method of delivery. Identify the visualization styles and report style that will best fit the needs of the audience. Create the reports and dashboards.","link":"/post/blogs/data/data-analytics/"},{"title":"The Data Mining Triology: II. Cleaning","text":"OverviewVery often, the data loaded into your notebooks are not entirely usable. There might be missing values, noisy data points, duplicates and outliers. Sometimes, data needs to be scaled up and down. Encoding and dimensionality reductions can be performed to make data cleaner and easier to operate on. Here we discuss about some essential ways to clean up the data Basic CleaningThe first step involves Detecting and handling missing or noisy data; Removal of outliers Minimizing duplication and computed biases within the data Missing Data Missing data is the entries with empty input or Null input. It can be handled in following ways: Ignore the Tuple Note: Suitable only when the dataset is quite large and multiple values are missing within a tuple 123456df2 = df[[column for column in df if df[column].count() / len(df) &gt;= 0.3]] # Drops columns with &gt;70% of rows missing value;print(\"List of dropped columns:\", end=\" \")for c in df.columns: if c not in df2.columns: print(c, end=\", \") # list out the dropped columns Fill the Missing Values using Manual imputation (via inspection &amp; domain knowledge) Mean value imputation Most Probable Value (Mode) imputation Noisy Data Noisy data is meaningless data that can't be interpreted by machines. It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways : Binning Method: [Sorted data in order to smooth it] [The whole data divided into segments of equal size] [Various methods are performed to complete the task. Each segmented is handled separately.] Regression: Fitting data to a regression function: ML Regression Algorithm can be used for smoothing of data. Interpolate using the regression. Clustering: Groups the similar data in a cluster and apply unsupervised learning. Detect and Remove Outliers: Detect Outliers (Some simple methods outlined below) 1. Using Boxplot12import seaborn as snssns.boxplot(...) 2. Using Scatterplot12345%matplotlib inlinefrom matplotlib import pyplot as pltfig, ax = plt.subplots(figsize=(16,8))ax.scatter(...)plot.show() 3. Using z score12345from scipy import statsimport numpy as npz = np.abs(stats.zscore(boston_df))threshold = 3print(np.where(z &gt; threshold)) 4. Using interquartile range (IQR) score1234Q1 = boston_df_o1.quantile(0.25)Q3 = boston_df_o1.quantile(0.75)IQR = Q3 - Q1print(boston_df_o1 &lt; (Q1 - 1.5 * IQR)) |(boston_df_o1 &gt; (Q3 + 1.5 * IQR)) Remove Outliers (Fixed value or interval methods) 5. Using column specific value threshold1boston_df_o = boston_df_o[(z &lt; 3).all(axis=1)] 6. Using value range (IQR in this case)1boston_df_out = boston_df_o1[~((boston_df_o1 &lt; (Q1 - 1.5 * IQR)) |(boston_df_o1 &gt; (Q3 + 1.5 * IQR))).any(axis=1)] Remove Duplicates Sometimes, there may exist duplicate data entries. Most of the time, this is undesirable. You may want to remove those entries (after carefully examine the problem setups) 123 s1_dup = s1_trans[s1_trans.duplicated()] # Identify Duplicatesprint(s1_dup.shape)s1_trans.drop_duplicates(subset = None, keep = 'first', inplace = True) # Remove Duplicates Transforming dataWe transform datasets in some situations to : Convert the raw data into a specified format according to the need of the model. Remove redundancy within the data (not duplicates, but unnecessary bytes that occupy the storage for no meaning) Efficiently organize the data Here we just present the method using sci-kit laern's preprocessing module. Data Conversion Normalization (Basically Data rescaling/mean removal):It is done in order to scale the data values in a specified range (-1.0 to 1.0 or 0.0 to 1.0) Attribute Selection (Usually for Aggregation purpose):In this strategy, new attributes are constructed from the given set of attributes to help the mining process. Discretization: (IMPT!!!)This is done to replace the raw values of numeric attribute by interval levels or conceptual levels. Using classes/ranges/bands mapping (given or need to design) Using Top-down approaches: Entropy-based Discretization Concept Hierarchy Generation:Here attributes are converted from level to higher level in hierarchy. For example, the attribute \"city\" can be converted to \"country\" in some scenarios. Encode Data:Machine learning algorithms cannot work with categorical data directly, categorical data must be converted to number. Label Encoding One hot encoding Dummy variable trap Label encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them. A One hot encoding is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1. The Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated in simple term one variable can be predicted from the others. By using pandas get_dummies function we can do all above three step in line of code. We will this fuction to get dummy variable for sex, children,smoker,region features. By setting drop_first =True function will remove dummy variable trap by droping one variable and original variable.The pandas makes our life easy. Advanced: Box-Cox transformationA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn't normal, applying a Box-Cox means that you are able to run a broader number of tests. All that we need to perform this transformation is to find lambda value and apply the rule shown below to your variable. 123## The trick of Box-Cox transformation is to find lambda value, however in practice this is quite affordable. The following function returns the transformed variable, lambda value,confidence interval. See the sample code below:from scipy.stats import boxcoxy_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05) box-cox Data Scaling / Standardizing / Mean RemovalNote: Don't use all of them, only use some selectively when you need to (which is usually the case after EDA)!!! Rescaling Data: scaled between the given range12data_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))data_scaled = data_scaler.fit_transform(input_data) Mean Removal: standardize input_data into mean = 0 and std = 1123data_standardized = preprocessing.scale(input_data)data_standardized.mean(axis = 0)data_standardized.std(axis = 0) Normalizing Data: values of a feature vector are adjusted so that they sum up to 11data_normalized = preprocessing.normalize(input_data, norm = 'l1') Binarizing Data: convert a numerical feature vector into a Boolean vector1data_binarized = preprocessing.Binarizer(threshold=1.4).transform(input_data) Label Encoding: changing the word labels into numbers 1234567891011### Encode:label_encoder = preprocessing.LabelEncoder()input_classes = ['suzuki', 'ford', 'suzuki', 'toyota', 'ford', 'bmw']label_encoder.fit(input_classes) #Generate Mappingprint(\"\\nClass mapping:\")for i, item in enumerate(label_encoder.classes_): print(item, '--&gt;', i)labels = ['toyota', 'ford', 'suzuki']encoded_labels = label_encoder.transform(labels) # Actual Encoding### Decode:decoded_labels = label_encoder.inverse_transform(encoded_labels) # Actual Decoding Warning: it assumes higher the categorical value, better the category (solved by one hot encoding) One Hot Encoding (often used together with argmax function): Dummy Variable Encoding123456789101112&gt;&gt;&gt; enc = OneHotEncoder(handle_unknown='ignore')&gt;&gt;&gt; X = [['Male', 1], ['Female', 3], ['Female', 2]]&gt;&gt;&gt; enc.fit(X)OneHotEncoder(handle_unknown='ignore')&gt;&gt;&gt; enc.categories_[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]&gt;&gt;&gt; enc.transform([['Female', 1], ['Male', 4]]).toarray()array([[1., 0., 1., 0., 0.], [0., 1., 0., 0., 0.]])&gt;&gt;&gt; enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])array([['Male', 1], [None, 2]], dtype=object) Scaler Comparison and choices MinMaxScaler Definition: Add or substract a constant. Then multiply or divide by another constant. MinMaxScaler subtracts the mimimum value in the column and then divides by the difference between the original maximum and original minimum. Preprocessing Type: Scale Range: 0 to 1 default, can override Mean: varies Distribution Characteristics: Bounded When Use: Use first unless have theoretical reason to need stronger scalers. Notes: Preserves the shape of the original distribution. Doesn't reduce the importance of outliers. Least disruptive to the information in the original data. Default range for MinMaxScaler is 0 to 1. RobustScaler Definition: RobustScaler standardizes a feature by removing the median and dividing each feature by the interquartile range. Preprocessing Type: Standardize Range: varies Mean: varies Distribution Characteristics: Unbounded When Use: Use if have outliers and don't want them to have much influence. Notes: Outliers have less influence than with MinMaxScaler. Range is larger than MinMaxScaler or Standard Scaler. StandardScaler Definition: StandardScaler standardizes a feature by removing the mean and dividing each value by the standard deviation. Preprocessing Type: Standardize Range: varies Mean: 0 Distribution Characteristics: Unbounded, Unit variance When Use: When need to transform a feature so it is close to normally distributed. Notes: Results in a distribution with a standard deviation equal to 1 (and variance equal to 1). If you have outliers in your feature (column), normalizing your data will scale most ofthe data to a small interval. Normalizer Definition: An observation (row) is normalized by applying 12 (Euclidian) normalization. If each element were squared and summed, the total would equal 1. Could also specify 11 (Manhatten) normalization. Preprocessing Type: Normalize Range: varies Mean: 0 Distribution Characteristics: Unit norm When Use: Rarely Notes: Normalizes each sample observation (row), not the feature (column)! Reduce DataThe aim of data reduction is to fit the data size to the question/make model more efficient. Usually there are 4 major methods as outlined below: 1. Data Cube Aggregation:Aggregation operation is applied to data for the construction of the data cube. The cube stores multidimensional aggregated information Ensures a smallest representation which is enough for the Task Base cuboid: individual entity of interest (e.g customers) Apex cuboid: total of all branches (e.g total sales for all item types) 2. Attribute Subset Selection:The highly relevant attributes should be used, rest all can be discarded.Significance Level and p-value of the attribute comparison: The attribute having p-value greater than significance level can be discarded. 3. Numerosity Reduction:This enable to store the model of data instead of whole data, for example: Regression Models. Parametric MethodRegressionlog-linear models Non-parametri Methodhistograms (for supervised learning binning)clustering (for unsupervised learning binning)sampling (best is simple random sampling without replacement)data cube aggregation (move from lowest level to highest level, data reduces when moving up the cube) 4. Dimensionality Reduction: This reduce the size of data by encoding mechanisms. lossy vs lossless:If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction Methods: Wavelet transforms: decompose a signal based on special bases (or basis functions), which have certain mathematical properties; Works well for image description PCA (Identify the components contributing to the most of the variances in the data) ICA (identify independent components that extract individual signals from a mixture) ConclusionThe above methods really focused a lot on numerical data and basic variables. There are tones of details I neglected (like errors in entries, how to detect them and how to fix them). Also, there are special treatments on words or images (like regex processing and image transformations). These are widely applied in the field of natural language processing and computer visions. We encourage interested readers to explore these ideas by reading the regex tutorials (I’ve also written a blog on it) and CV tutorials.","link":"/post/blogs/data/data-cleaning/"},{"title":"The Data Mining Triology: I. Preparation","text":"OverviewIn this Data Mining triology, I'm going to present the following critical steps each scientist should perform when handling the data: Data Preparation Load and integrate data sources Data Cleaning Prepocessing the data to make them meaningful and usable Exploratory Data Analysis Analyze the data and understand its pattern. Make corresponding adjustments to data along the way In this blog, let's talk about the first one – Data Preparation. Sources of dataThere are indeed a great variety of data sources since the age of machine learning started. While we may come across a wide variety of data types (image, video, text, sheet, signal… you name it), we often can get these data from some popular website: Google Datasets Search Pros: Wide coverage, can find whatever dataset you want Cons: Some datasets are not actually accessible, and the website does not indicate that at all! Government Datasets Pros: Most of them are publically available, which is good Most of these data are well preprocessed, so you don't have to worry about it Cons: Sometimes, the size of the dataset is not large enough for meaningful projects Governments may not release the most up-to-date datasets. Hence the effectiveness of prediction models may be questionable Kaggle Datasets Pros: Really easy to get the data: via command line. This often preferred by professional engineers. See tutorial on kaggle api Aside from the datasets themselves, you can often find a bunch of enthusiasts on machine learning in Kaggle and excellent tutorials on the datasets you found. Cons: It takes a bit of practice to get along with Kaggle. Passion and drive are the key to success in Kaggle UCI Machine Learning Repository Pros: One of the most widely used repository for machine learning datasets Very Often, the datasets are related to academic/industrial research projects, so it is extremely helpful to researchers Cons: As a well-aged repo, the datasetes there certainly have been studied extensively. So it may not be so useful for new breakthroughs (but still, it should be very helpful for beginners) Code for loading the dataThe most common format for machine learning data is CSV files, and we are using python 3.x here for actual code.This step should mark the start of your notebook (after np/pd/sklearn/plt). 123import pandas as pdtrain = pd.read_csv('../input/train.csv')test = pd.read_csv('../input/test.csv') Note that this is too widely abused that people forget about other ways to load data: Load CSV with Python Standard Library123456import csvimport numpy as npraw_data = open(\"your filename here\", 'rt')reader = csv.reader(raw_data, delimiter=',')x = list(reader)data = np.array(x).astype('float') Load CSV with Numpy123import numpyraw_data = open(\"your filename here\", 'rt')reader = numpy.loadtxt(raw_data, delimiter=\",\") Load CSV with URL12345from numpy import loadtxtfrom urllib.request import urlopenurl = 'URL to a dataset'raw_data = urlopen(url)dataset = loadtxt(raw_data, delimiter=\",\") Now be cautioned that this is just for CSV files. There are a lot of other data formats, and Google is always your best friend in finding methods to load datasets.{: .notice–info .notice–x-large} That was input, how about output?Converting data from python objects into byte streams is known as Pickling or Serialization. This allows your own data to be passed around efficiently. Very often, they are stored as .pkl or .json files. Python Pickle and JSONThe following table is inspired by this tutorial Python Pickle JSON Definition Python Pickle is the process of converting python objects (list, dict, tuples, etc.) into byte streams which can be saved to disks or can be transferred over the network. The byte streams saved on the file contains the necessary information to reconstruct the original python object. The process of converting byte streams back to python objects is called de-serialization. JSON stands for JavaScript Object Notation. Data Stored can be loaded without having the need to recreate the data again. Storage format Binary serialization format Simple text serialization format, human-readable Storage Versatility Not only data entries, but classes and methods can be serialized and de-serialized JSON is limited to certain python objects, and it cannot serialize every python object, such as classes and functions Language dependency Very reliant on the language (Python specific) and versions (2.x pickle files may not be compatible in 3.x env) JSON is supported by almost all programming languages. Speed Slower serialization and de-serialization in pickle Lightweights, much faster than pickle Security There is always security risks with pickle files JSON is generally secure With the above table in mind, one can choose their outputs accordingly. ConclusionLoading data is merely the first step and people can quickly learn to apply them. However, I/O choices does matter, and one should be cautious about them. Now, lets step into the second step in data mining: cleaning data.","link":"/post/blogs/data/data-preparation/"},{"title":"Database: Intro","text":"DefinitionA database is a collection of information that is usually stored in computer systems. The information are usually data of various forms. The main function of database is for the managers of these information to better store and manipulate the data efficiently. A database can be either relational or non-relational. Relational databaseIn a relational database, each row in the table is a record with a \"unique ID\" called the key. The columns of the table hold attribute of the data, and each record usually has a value for each attribute, making it easy to establish the relationships among data points. Non-relational databaseA non-relational database is a database that does not use the tabular schema of rows and columns found in most traditional database systems. Instead, non-relational databases use a storage model that is optimized for the specific requirements of the type of data being stored. For example, data may be stored as simple key/value pairs, as JSON documents, or as a graph consisting of edges and vertices.1 Note: The 3 points above are for interview preparation. They are succint and clearly explains each part. You can directly use them or shorten them in interview. 4. DBMS (Database management system)I assume that most of you have some prior knowledge about SQL. Otherwise, you are welcome to review my posts on SQL. So I'm not gonna explain what DBMS is and some examples of DBMS.To be updated…","link":"/post/blogs/data/database-introduction/"},{"title":"The Data Mining Triology: III. Analysis","text":"OverviewFinally we have come to the last part in fundamental data mining. This is where people's analytical power shine through. However, we also highlight some cautions engineers should practice in exploratory analysis. While data analysis is fascinating, I feel that building models based on the analysis to facilitate business decisions is even more exciting. This heavily relies on machine learning models and artificial intelligence toolkits. I've also written (and will write more in the future) blogs on these topics. Word of reminder: these models require some level of statistical and mathematical foundations, so it really depends on one's interests in developing these models. A general view of the datasetOne can always use an easy trick: YourDataFrameName.describe() to show the details about your data entries. This gives very good view of properties of your data. A sample output looks like 12345678910111213&gt;&gt;&gt; df.describe(include='all') categorical numeric objectcount 3 3.0 3unique 3 NaN 3top f NaN afreq 1 NaN 1mean NaN 2.0 NaNstd NaN 1.0 NaNmin NaN 1.0 NaN25% NaN 1.5 NaN50% NaN 2.0 NaN75% NaN 2.5 NaNmax NaN 3.0 NaN Next let's look into numerical data's pattern first. Numerical data distributions1. Generate comprehensive view for the numericals in Data Set1234list(set(df.dtypes.tolist()))df_num = df.select_dtypes(include = ['float64', 'int64']) # select only numerical datadisplay(df_num.head()) # output the first 5 entriesdf_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8) # Give the comprehensive views of all the distribution (in histogram) of the numerical values; Key steps:(i) From the graphs, find which features have similar distributions;(ii) Document the discovery for further investigation; 2. Correlation (correlation is affected by outliers)Find the strongly correlated values with the output. Call this list of values golden_features_list. 123df_num_corr = df_num.corr()['SalePrice'][:-1] # -1 because the latest row is SalePricegolden_features_list = df_num_corr[abs(df_num_corr) &gt; 0.5].sort_values(ascending=False)print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list)) 3. Correlation (outliers removal) Plot the numerical features and see which ones have very few or explainable outliers Remove the outliers from these features and see which one can have a good correlation without their outliers 1234for i in range(0, len(df_num.columns), 5):sns.pairplot(data=df_num, x_vars=df_num.columns[i:i+5], y_vars=['SalePrice']) Key steps:(i) Spot any clear outliers, document. Think of outlier's plausibility. Think of whether to remove it &amp; document;(ii) Spot any clearly linear/non-linear relationships, document;(iii) Spot any distribution with a lot of 0's: do Correlation (0 Removal); 4. Correlation (0 Removal)Removing all 0's in some columns and generate golden_features_list again, see if any new features are added. 123456789101112131415import operatorindividual_features_df = []for i in range(0, len(df_num.columns) - 1): # -1 because the last column is SalePrice tmpDf = df_num[[df_num.columns[i], 'SalePrice']] tmpDf = tmpDf[tmpDf[df_num.columns[i]] != 0] individual_features_df.append(tmpDf)all_correlations = {feature.columns[0]: feature.corr()['SalePrice'][0] for feature in individual_features_df}all_correlations = sorted(all_correlations.items(), key=operator.itemgetter(1))for (key, value) in all_correlations: print(\"{:&gt;15}: {:&gt;15}\".format(key, value)) golden_features_list = [key for key, value in all_correlations if abs(value) &gt;= 0.5]print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list)) Finally, we can give conclusion with respect to the Numerical data distribution analysis. Feature to feature (Categorical to Categorical) Correlation Analysis: Heat map for features: Steps of Analysis for the Heatmap: First of all, remove all simple correlations (easy to explain &amp; not that relevant) Next, identify the relationships that are pertinent to the questoin/task Lastly, conclude the features that are similar to be easily combined/ need further investigation/ clearly helpful to the task Document the analysis; 1234567corr = df_num.drop('SalePrice', axis=1).corr() # We already examined SalePrice correlationsplt.figure(figsize=(12, 10))sns.heatmap(corr[(corr &gt;= 0.5) | (corr &lt;= -0.4)], cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1, annot=True, annot_kws={\"size\": 8}, square=True)# this generates the heatmap that display highly related features;# Note that this map only displays bidirectional relationships;# We cannot conclude much if there are relationships among feature set of size &gt;= 3; Q –&gt; QQ to Q stands for \"Quantitative to Quantitative relationship\", which is often found in a pure numeric dataset. For qualitative relationships, tricks like counting and sorting can also be used to transform the qualitative data into numeric ones for Q to Q analysis. Extract strongly correlated quantitative features123features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]features_to_analyse.append('SalePrice')display(features_to_analyse) plot the distribution:1234fig, ax = plt.subplots(round(len(features_to_analyse) / 3), 3, figsize = (18, 12))for i, ax in enumerate(fig.axes): if i &lt; len(features_to_analyse) - 1: sns.regplot(x=features_to_analyse[i],y='SalePrice', data=df[features_to_analyse], ax=ax) Analysis of the distribution: Since Linear Regression is give, we focus on analyzing the spread of the data in each graph C –&gt; Q (Categorical to Quantitative relationship)C to Q stands for \"Categorical to Quantitative relationship\". This is different from qualitative or quantitive relationships, as we cannot compare the degree of desired attributes based on category number themselves. Thus we should see how these attributes are manipulated to make the relationship interpretable. Extract Categorical features123categorical_features = [a for a in quantitative_features_list[:-1] + df.columns.tolist() if (a not in quantitative_features_list[:-1]) or (a not in df.columns.tolist())]df_categ = df[categorical_features]df_not_num = df_categ.select_dtypes(include = ['O']) # include the non-numerical features Apply Boxplot123plt.figure(figsize = (12, 6))ax = sns.boxplot(x='SaleCondition', y='SalePrice', data=df_categ) # can replace \"SaleCondition\" with other features Apply Distribution plot123456fig, axes = plt.subplots(round(len(df_not_num.columns) / 3), 3, figsize=(12, 30))for i, ax in enumerate(fig.axes): if i &lt; len(df_not_num.columns): ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45) sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)fig.tight_layout() Through these plots, we can see that some categories are predominant for some features such as Utilities, Heating, GarageCond, Functional… These features may not be relevant for our predictive model. ConclusionThe methods above cover a wide range of tools being applied in data analytics. There are definitely many more directions in EDA, and I'll update my discovery every time I find some interesting things.","link":"/post/blogs/data/exploratory-data-analysis/"},{"title":"Prompt Engineering Whitebook","text":"Why promptingWhen working with LLMs, the rule number one is: Don't touch the model. Very often, people (especially students with more experience in model tuning and less industrial-level prompt engineering experiences) will opt for finetuning when they have a new problem at hand. However, the harsh reality is that most real-world problems are either simple enough to handle with a good prompt, or complex enough that fine-tuning on available large datasets become less effective. In my opinion, prompting should ideally be your first approach. Complex tasks can often be decomposed into smaller, easier tasks and solved with pretrained models. Yuo should only go changing model architecture once your prompts are as good as they can be. No company would want to burn money at start, only to realize that easy solution with prompt engineering is there lying on the table. Major benefits of prompt engineering include: Reduce costs by moving to a smaller model Eliminate finetuning costs Enable lower-latency communication by changing the general format AssumptionsThis blog assumes basic understanding of prompting, such as what forms a prompt, what are different components of a prompt, and how prompts are transformed into tokens for model inferences. You may check online resources for it if you don't now about them. TechniquesUse TemplatesMost open source model have their specific prompt tempaltes. You can refer to their website, or find it on Hugging Face. Some basic ones include Vicuna-13B 123### Human: your prompt here### Assistant: Llama-2-chat 12345[INST] &lt;&lt;SYS&gt;&gt;You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.&lt;&lt;/SYS&gt;&gt;{prompt}[/INST] EverythingLM-13B-16K 12345You are a helpful AI assistant.USER: {prompt}ASSISTANT: Claude 12Human: Human thingsAssistant: {{Response}} Things to take note of: Some models don't have system prompts Some models will prefer alternating between user prompt and assistant prompts When choosing models, you should often look out for instruction-finetuned models, as they are the most prevalent ones for chat completion/streaming chat tasks. Few-shot learningTo put it in a user-friendly text, few-shot learning in the context of llm prompting is simply providing example into the prompt. You may raise a question or instruction for the llm/chatbot to answer. But sometimes they don't know the answer format or they hallucinate without suffcient context. Giving some examples often helps models to understand the instructions better, thus providng a more cohesive and relevant answer. As an example, suppose one wants LLM to output a JSON, but on the first try, the JSON was malformed. To fix this issue, one can either pass the output to LLM and ask it to fix it by itself, or he can retry using a better prompt with few-shot learning: 12345### Human: Given a sentence \"This place is horrible\" from Wall Street Journel, determine if it has positive/negative/neutral sentiment. Output the result in JSON format. Here are a few examples:{\"sentence\": \"The food is enjoyable\", \"sentiment\": \"positive\"}{\"sentence\": \"Princess Kate was diagnosed with cancer\", \"sentiment\": \"netural\"}{\"sentence\": \"War criminials need to be punished heavily\", \"sentiment\": \"negative\"}### Assistant: If we want the model to learn to merge queries from past responses and better the answer, we can improve the prompt above by splitting in into conversations 123456789### System: Given a sentence from Wall Street Journel, determine if it has positive/negative/neutral sentiment. Output the result in JSON format.### Human: The sentence is \"The food is enjoyable\", the output JSON is:### Assistant: {\"sentence\": \"The food is enjoyable\", \"sentiment\": \"positive\"}### Human: The sentence is \"Princess Kate was diagnosed with cancer\", the output JSON is:### Assistant: {\"sentence\": \"Princess Kate was diagnosed with cancer\", \"sentiment\": \"netural\"}### Human: The sentence is \"War criminials need to be punished heavily\", the output JSON is:### Assistant: {\"sentence\": \"War criminials need to be punished heavily\", \"sentiment\": \"negative\"}### Human: The sentence is \"The place is horrible\", the output JSON is:### Assistant: You can also teach multi-turn behavior - like adding together queries, and cleaning them out when requested via this few-shot learning technique. With all these benefits, we must not ignore its potential problems: Model often struggles to move away from pre-training knowledge It significantly uses up the token budget of your prompts, which can be expensive Sometimes giving examples is counter-effective. For example, providing a single positive example can cause the model to always output positive label. Providing two positive and one negative can cause the model to think the next one must be negative. Sometimes this pattern happen because the label distribution is very skewed. Sometimes it could be domain knowledge issue as well. Be sure to check it out and eliminate potential hallucination issues when applying this technique. Manage prompt complexitySuppose you are talking to a human and providing instruction to them. If you provide a long, complex set of instructions in one shot and expects the human to follow it, how confident are you in him/her completing the instruciton as you wanted? Most cases it achieves nothing but anger in that person's mind. Now think about the case when you talk to a chatbot, the sheer complexity of prompt can also be countereffective from time to time. Hence, managing the complexity of your prompt is a really important part of prompt engineering. Here are a list of things I recommend checking to achieve a good balance when managing your prompts for your tasks. Most prompts have three primary types of complexity and we will handle them one by one. Task Complexity Definition: Difficulty of the major task Example: Who are the characters in this text is significantly simpler than Identify the key antagonists How to reduce it: Break it down to smaller, simpler tasks Insert a chain of thought before asking for an answer. Think step-by-step is an easy addition Pointing out which part of the problem to solve first. Models need to know where to start and start the right way. Sometimes you can debug model's thought process by asking it to print it out Inference Complexity Definition: The amount of inference the model needs to understand your task. Counterintuitively, this is something that affects small, seemingly simple prompts. Example: understanding what is an intent can be tough, as it can mean general objectives in research, or enquiry in customer service. How to reduce it: Provide explanation/definition to those keywords Switch to a simpler/general words if possible Often requires prompt size to grow Ask the model to define it himeselve to achieve implicit chain-of-thought Ancillary Functions Definition: smaller tasks you are explicitly (or implicitly) asking the model to perform Examples: transformations to the JSON; retrieving and merging things from previous messages. How to reduce it: Prompt Switching: essentially keeping the context and vary the instructions Note: Conversationally tuned models (like llama-2) will prefer this, but other instruction-following models might find it hard to retrieve intermittent context (hiding in between human instructions) when it comes to answering the final, big question. Self-consistency: You can test if the complexity is removed by turn the temperature up if your task permits it, and see if the results are aligned If your prompt works well across multiple models, it's a good sign that it's well-spelled out A checklist for reducing prompt comlexity Primary task The most valuable thing I need the model to do Key terms in the task: are they very, very well defined, or so simple that there's no ambiguity? Any explicit/implicit additional tasks aside from primary task: are they integral to the performance of my primary task? Can I split them into other prompts or find ways to reduce their complexity? Any domain knowledge or things that require domain expertise: can model infer or learn these eccentricities about this domain? Any instruction requirements: is my task a question? does it need instructions (like this list you're reading) on how to start towards a solution? Spoon-FeedingIntuition: LLMs are next-token probability predictors, and the sooner you can get them going in the right direction, the more likely that they'll follow it. Example: 12345Human: Please help this user with his questions, by providing a list of ingredients for his recipe.Human: I'm making a mud pie!Assistant: Cool! The ingredients you'll need are Notice in Assistant , the tokens all the way up to are are fixed, and the next token is our required word. Note that OpenAI GPTs don't support this strategy (but you can still leave uncompleted text at the end for a workaround), but almost every other model and provider does. Proper usage of System promptsAttention to system prompts have always been a potential weakness of GPT models (but may be fixed in later versions). However, Llama-2 class of models actually handle system prompts well, as they use special mechanisms in training (like Ghost Attention) to increase the effectiveness of a system prompt to influence a conversation, even after many messages. Some useful things you can use your system prompts for: Hold Facts, Rules (see below) or other general purpose information that don't change as the conversation proceeds. Set the personality of the assistant. A strong personality (e.g. You are a chess grandmaster) may lead to better quality of the task completed in some cases. Set (or reinforce) an output format (.e.g You can only output SQL.) Move repeated bits of user messages out so you can do better few-shot learning. Make changing the task for this prompt easier without editing the conversation history. Meaningfully distinct keywordsFor some keywords that you want the model to put close attention to, convert the normal natural language to a special format. It is recommended to use CAPITAL_UNDERSCORED_HEADINGS. As an example: 12345The travel document I want you to read:Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Use the travel document provided to extract the key destinations the user is travelling to. Can be transformed into: 123456USER_TRAVEL_DOCUMENT:\"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\"\"Extract the key destinations from USER_TRAVEL_DOCUMENT. Proper escapingIn most cases, the information provided (documents, emails, etc) will be in the same language and follow similar formats to your instructions. Use escaped (and Meaningfully distinct keywords) to help the model separate which is which. Use backticks (`) or triple quotes (”””) to escape your data sections. Use a few recommended formatting options for input/output Multi-line strings: pretty easy, use this for unstructured data. Bulleted lists: easy way to mark something as a list. Save your tokens unless your experience differs. Markdown tables: pretty token heavy. Use these if your data comes in markdown tables, or you need it for easy output formatting. Typescript: The significantly better choice for expressing a typespec, especially with comments mixed in. JSON: Uses more token than many of the above. But may become the new standard in the long term (OpenAI funciton has JSON formatted output support). YAML: Close to natural language. Also pretty conservative on tokens. Not having random curly braces helps the BPE better break up your characters into larger token chunks. A rule of thumb: If you want support, use JSON. If you want brevity, use YAML. If you have a typespec, use Typescript. If it's simple, just separate with newlines. Content structuring with Facts and RulesSometimes structuring your prompt may make your prompts easier to read for both you and the model. Aside from proper escaping, we often use facts and rules to guide models to complete the task: Facts list what the model should presume before working on the task. Organizing your prompts this way helps you better understand and modify them later on (and prevent prompt-rot) Rules are specific instruction to follow when executing on a taskAn example can be: 1234567FACTS:1. Today's date is 6 September 2023.2. Pax, pp, per person all mean the same thing.RULES:1. You need to outline your logical premise in Prolog before each sentence.2. Write the text as the user, not on behalf of them. Chain-of-ThoughtThis is a well-known method, I'll just pass two examples with different tasks for inspiration Cliff-summarising a story Let's say you want to take a story and summarise the key story beats. You keep trying but the LLM keeps missing things. Here’s one approach. 12345678910STORY:\"\"\"Just wakin' up in the mornin', gotta thank GodI don't know but today seems kinda oddNo barkin' from the dog, no smogAnd momma cooked a breakfast with no hog\"\"\"Summarise this story into the key plot points. One way to improve effectiveness is to work out how you would do it. 1234567891011121314151617Summarise this STORY into key plot points.STORY:\"\"\"Just wakin' up in the mornin', gotta thank GodI don't know but today seems kinda oddNo barkin' from the dog, no smogAnd momma cooked a breakfast with no hog\"\"\"Go step by step to get the plot points:1. Outline the key players in the story. Who are the characters?2. List the major plot points and who was involved.3. For each plot point, list the consequences of this happening.4. For each consequence, see if there are any story beats missing from the first list, and list them.5. Resummarise the story in terms of beats, labelling each point as positive or negative and it's contribution to the story. This kind of prompting also produces responses that are far easier to debug. Continuing a story Now say we wanted to write the next chapter for the same story - a far more creative endeavor. Here's a naive prompt: 12345678910STORY:\"\"\"Just wakin' up in the mornin', gotta thank GodI don't know but today seems kinda oddNo barkin' from the dog, no smogAnd momma cooked a breakfast with no hog\"\"\"Write the next chapter of the STORY. Here's a better one. 12345678910111213141516STORY:\"\"\"Just wakin' up in the mornin', gotta thank GodI don't know but today seems kinda oddNo barkin' from the dog, no smogAnd momma cooked a breakfast with no hog\"\"\"We need to write the next chapter of STORY, but let's go through the steps:1. List the main characters in the STORY, and what their personalities are.2. What are their arcs so far? Label each one on a scale of 1-10 for how interesting it is, and how important it is to the main story.3. List which arcs are unfinished.4. List 5 new characters that could be introduced in the next chapter.5. List 5 potential, fantastical things that could happen - major story beats - in the next chapter.6. Grade the new characters and the new occurrences 1-10 on how fun they would be, and how much they fit within the theme of the existing story.7. Write the next chapter. Chain-of-Thought but multi-path automation + validationWhen designing chain-of-thought prompt, or any set of facts + rules to better structure your prompt content, consider consulting GPT-4 or other expensive models to get suggestions. The pseudocode is 12345For each COT path (rules/facts): Build prompt with these context Run inference to get results Perform debugging step and generate a scoreSelect the candidate with the highest score Some other tricks (To be expanded) Pretended that some of our provided context came from the AI and not us. Language models will critique their own outputs much more readily than your inputs For each model, use delimiters and keywords that look and feel similar to the original template/dataset used for the model, even if they're not directly part of the dataset In some cases, asking the model to annotate its own responses with a probability of acceptance, and thresholding this value to remove the worst candidates can improve results. Using structured text like pseudocode may improve results Replace negation statements with assertions (e.g., instead of “don't be stereotyped,” say, “please ensure your answer does not rely on stereotypes”) If budget allows, find a way to express the output in structured format where it can be auto-verified (in polynomial time ideally). Then turn the temperature up and take a few passes through the same prompt. Pick the majority winner. How to debug your prompt Never pass user input (more specificly, raw customer input) directly to model for output Never invent custom formats. Use and modify what's already in the lexicon of the model. Remove syntax and semantic errors. Sometimes this cause models to output wrong things. Example: saying output characters in an instruction may direct model to prefer outputing multiple characters when there should be only one valid character. When dealing with specific output format, don't put trailing fullstop/coma/semicolon as they may break the output structure. Vary the order of your instructions and data to make a prompt work Vary where the information is placed (user prompt vs system prompt vs assistant prompt) Change the wording, sometimes the keywords/phrases that are domain-specific or abstract are understood by different models differently. check if changing some keywords to its variants or make them clearer can be helpful- When performances of output among different models using the same prompt are similar (sometimes can be done using an LLM evaluator), and you are happy with the results, your prompt is probably ready to use. When to modify the model itself You've tried extensive prompt optimization, and you're nowhere near your required success rate. You need to move to a smaller model, for privacy or cost reasons. You have a large enough dataset, and the time and money to finetune a model. Your problem space sits far outside the pretraining dataset - maybe you work in Swift, or you need to train a DSL. You have a particular style of interaction that you need to “bake in”, even at the cost of potentially overfitting. You need to reverse some prior finetuned behavior. References Everything I know about Prompting","link":"/post/blogs/llm/prompt-engineering/"},{"title":"A brief Intro to A&#x2F;B Testing","text":"IntroductionA/B testing is a type of hypothesis testing commonly used in business. It is mainly useful on for products that are mature and is suitable for fast iterative product development. The main idea is to assume a new modification is useful and take trials/experiments upon the modified product. Next, using the old version as a reference, see how significant is the improvement brought by the new modification. Note: When we say new modification, it must be warned that the change should contain only one factor, otherwise the influence can be compounded. 11 Steps in A/B Testing First time trying something new: run an A/A testing simultaneously to check for systematic biases Systematic Bias: is sampling error that stems from the way in which the research is conducted. There are three types: a. Selection bias: Biased way to select the factors/samples. b. Non-response bias: the participants/customers involved in your tests are different in their behavioral patterns from the people that are not involved in your study (other general public) c. Response bias: The result/inference that are given does not follow the truth/real observations. Define the goal and form hypothesis Null hypothesis : a general statement or default position that there is no relationship between two measured phenomena. Often claimed to be the case when there is no association between the performance of product and the a feature change of the product. If we believe otherwise, then we are arguing for a Alternative hypothesis . Identify control and treatment groups Determine factor to change (make sure no confounding factors are included) Determine sample size: general rule is where is the sample standard deviation and is the difference between the control and treatment. Determine duration: usually about 1-2 weeks, but highly dependent on budget and nature of the business Identify KPI/metrics to measure e.g: click through rate, conversion rate, renewal rate, bounce rate, average retention, etc… Identify what data needs to be collected Sometime people use the Customer Funnel analysis: credit: https://clevertap.com/blog/funnel-analysis. Example: [Netflix - Funnel Description] Customer will enter the home page, which invites customer to further enter via the button free trial for one month, then customer will try and then register and pay Example: [Netflix - Funnel Analysis]Funnel analysis: here the funnel will be how the product will affect each steps like improving converted customer size or improving returning customer size, during the procedure to reach click through/subscription, what side-effects/main effects are triggered Make sure that appropriate logging is in place to collect all necessary data Determine how small of a difference can be (define significance level and thus power of the experiment) Significance Level: the decision threshold and basically the probability that we reject the null hypothesis while it is true (type I error). A lower significance level indicates an underlying difference between the baseline and the control. In the majority of cases, the threshold value is about 0.05. P-Value: the probability that the difference between two values is related to random chance. The lower the p-value, the more likely H0 is to be discarded. As a result, what you saw did not happen randomly. Power: is the probability of rejecting the null hypothesis while it is false (type II error). Example: + p-value is less than -&gt; reject and Conclude . Determine what fraction of visitors should be in the treatment group (control/treatment split ratio) Run a power analysis to decie how much data is needed to collet and how long to run the test compute running time given customer flow Run the test for AT LEAST this running time long (*) For comparing ML algorithms, consider McNamer’s Test or 5x2 CV or Nonparametric Paired Test. You can find the details in this post Sanity Checks and post-experiment validation: Ensure to take a review of all the executions, subjects and objectives are as expected. Before you run A/B testing…1. Why should you run a A/B test? A/B testing is the key to understand what drives your business and make data-informed business decisions To understand the causal relationship and not simply the correlations 2. When to run experiments Deciding whether or not to launch a new product or feature To quantify the impact of a feature or product Compare data with intuition (Understand how users respond to certain parts of a product) 3. When not to run experiments No clear comparison between the control and experimental group Emotional changes need time: Logo/Brand name Response data hard to obtain Too time consuming/costly A closer look at Type I and Type II errors1. Terminology significance level: The significance level for a given hypothesis test is a value for which a p-value less than or equal to is considered statistically significant. Often denoted by α region of acceptance: The range of values that leads the researcher to accept the null hypothesis. [significance level↓ region of acceptance↑ : harsher condition to produce a winner] effective size: The difference between the true value and the value specified in the null hypothesis. [Effect size = True value - Hypothesized value] 2. Type 1 Error When the Null Hypothesis is true but rejected 'false positive' - happen when the tester validates a statistically significant difference even though there isn’t one (no winner situation). positive here means a valid winner concluded Type 1 errors have a probability of \"α\" correlated to the level of confidence that you set. A test with a 95% confidence level (α = 0.05) means that there is a 5% chance of getting a type 1 error. [In business sense] This means that you will wrongfully assume that your hypothesis testing has worked even though it hasn’t. [The business consequence] Potential loss of money after adjustment is made, because your variation didn’t actually beat your control version in the long run. 3. Type 2 Error When the Null Hypothesis is false but accepted 'false negative' - Happens when you inaccurately assume that no winner has been declared between a control version and a variation although there actually is a winner (can be either) The probability of a type 2 error is \"β\". β depends on the power of the test (i.e the probability of not committing a type 2 error, which is equal to 1-β). There are 3 parameters that can affect the power of a test: Sample size (n): Other things being equal, the greater the sample size, the greater the power of the test (but also the more expensive of the test). Significance level of test (α): The lower the significance level, the lower the power of the test. If you reduce the significance level (e.g., from 0.05 to 0.01), the region of acceptance gets bigger. As a result, you are less likely to reject the null hypothesis. This means you are less likely to reject the null hypothesis when it is false, so you are more likely to make a Type II error. In short, the power of the test is reduced when you reduce the significance level; and vice versa. The \"true\" value of your tested parameter or effective size: The greater the difference between the \"true\" value of a parameter and the value specified in the null hypothesis, the greater the power of the test. [The business consequence] Potential loss of money after adjustment is made, because your variation didn't actually beat your control version in the long run Toolkits Usually the test plan/create variation step can be executed with company's own techpack, or using some popular tools Google optimize optimizely For hypo testing and result analysis: online resources of excel's A/B testing macro are widely available","link":"/post/blogs/mlops/AB-testing/"},{"title":"Deep Learning System Design - A Checklist (Part I)","text":"IntroductionAfter reviewing many blog posts and working on several DL-based projects myself, I've compiled a list of must-do's for a robust, complete Deep Learning System. In general, when we consider a DL system to be “complete”, it needs to have the following components: Data Modeling Training &amp; Optimization Experiments Packaging and Deployment Serving Monitoring I'll walk through each step and provide checklists for each of them, detailing rationales and provide examples wherever possible. Step 1: DataData Source What is the availability of data? What is the size/scale of data? Do we have user feedback data? Do we use system/operation data (logs? API req/resp?) Are there privacy issues? A note about logs: Store logs for as long as they are useful, and can discard them when they are no longer relevant for you to debug your current system Data ETL What is the data size before/after transformation, this often involves granularity What is the data format? JSON CSV (Row-format) Parquet (Column format, Hadoop, AWS Redshift) Row-major vs Column-major Overall, row-major formats are better when you have to do a lot of writes, whereas column-major ones are better when you have to do a lot of column-based reads. Note: Pandas is column-major, NumPy is row-major by default (if not specified). Access Pandas DataFrame rows are faster after we do df.to_numpy() Model related: Metadata Training data Monitoring data (sometimes for iterative deployment with model updates) Where is the data stored (Cloud? Local? Edge?) Most of the time, it is cloud. Afterall, it costs little for school-level project to store data in AWS S3. Consider spliting app-related dat from model-related data (e.g. WandB vs MongoDB) Processing Recall ACID and BASE Tranactional: OLTP low latency (often for streaming service) high availability transaction won't go through if system cannot process it Often row-major Eventual consistency Analytical: OLAP Tolerant to higher query latency (often require trasnformation) less available: can afford some downtime delayed operation, but will go through during system overload Often uses a columnar storage format for better query performance. Strong consistency Data Routine ETL daily routine Example: using Airflow Data Quality &amp; Data Validation Are the feature information complete? Any missing data? Is the training/testing data fully labeled? (can we use self-supervised to do ML-based annotation?) Are there data drifts? Are there bias in the data? Packages to detect them? Routine to validate data? Example: use pandera package 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import pandera as pafrom azureml.core import Runrun = Run.get_context(allow_offline=True)if run.id.startswith(\"OfflineRun\"): import os from azureml.core.dataset import Dataset from azureml.core.workspace import Workspace from dotenv import load_dotenv load_dotenv() ws = Workspace.from_config(path=os.getenv(\"AML_CONFIG_PATH\")) liko_data = Dataset.get_by_name(\"liko_data\")else: liko_data = run.input_datasets[\"liko_data\"]df = liko_data.to_pandas_dataframe()# ---------------------------------# Include code to prepare data here# ---------------------------------liko_data_schema = pa.DataFrameSchema({ \"Id\": pa.Column(pa.Int, nullable=False), \"AccountNo\": pa.Column(pa.Bool, nullable=False), \"BVN\": pa.Column(pa.Bool, nullable=True, required=False), \"IdentificationType\": pa.Column(pa.String checks=pa.Check.isin([ \"NIN\", \"Passport\", \"Driver's license\" ]), \"Nationality\": pa.Column(pa.String, pa.Check.isin([ \"NG\", \"GH\", \"UG\", \"SA\" ]), \"DateOfBirth\": pa.Column( pa.DateTime, nullable=True, checks=pa.Check.less_than_or_equal_to('2000-01-01') ), \"*_Risk\": pa.Column( pa.Float, coerce=True, regex=True )}, ordered=True, strict=True)run.log_table(\"liko_data_schema\", liko_data_schema)run.parent.log_table(\"liko_data_schema\", liko_data_schema)# -----------------------------------------------# Include code to save dataframe to output folder# -----------------------------------------------##### Downstream taskliko_data_schema.validate(data_sample) Step 2: ModelingModel selection Start with model suitable for the task -&gt; task categorization with/without label/partial label numeric/categorical output generation/prediction (for generation you need to learn the latent space) Baseline selection Random Baseline Human Heuristic Simplest ML model Metric Selection What is the task type Classification Metrics: Binary Classification Accuracy Precision Recall F1 Score Area Under the Receiver Operating Characteristic curve (AUC-ROC) Area Under the Precision-Recall curve (AUC-PR) True Positive Rate (Sensitivity or Recall) True Negative Rate (Specificity) False Positive Rate False Negative Rate Classification Metrics: Multi-Class Classification Micro/Macro/Average Precision Micro/Macro/Average Recall Micro/Macro/Average F1 Score Confusion Matrix Multi-class Log Loss Cohen\\'s Kappa Jaccard Similarity Score Regression Metrics Mean Squared Error (MSE) Root Mean Squared Error (RMSE) Mean Absolute Error (MAE) R-squared (Coefficient of Determination) Mean Squared Logarithmic Error (MSLE) Mean Absolute Percentage Error (MAPE) Huber Loss Clustering Metrics Silhouette Score Davies-Bouldin Index Calinski-Harabasz Index Inertia (within-cluster sum of squares) Adjusted Rand Index Normalized Mutual Information (NMI) Homogeneity, Completeness, and V-Measure Anomaly Detection Metrics Precision at a given recall Area Under the Precision-Recall curve (AUC-PR) F1 Score Receiver Operating Characteristic curve (ROC) Area Under the Receiver Operating Characteristic curve (AUC-ROC) Natural Language Processing (NLP) Metrics BLEU Score ROUGE Score METEOR Score CIDEr Score Perplexity Accuracy, Precision, Recall for NER tasks Ranking Metrics Mean Reciprocal Rank (MRR) Normalized Discounted Cumulative Gain (NDCG) Mean Average Precision Precision at K Recall at K Recommender System Metrics Precision at K Recall at K Mean Average Precision (MAP) Bayesian Personalized Ranking (BPR) Root Mean Squared Error (RMSE) for collaborative filtering Image Segmentation Metrics Intersection over Union (IoU) Dice Coefficient Pixel Accuracy Mean Intersection over Union (mIoU) F1 Score Time Series Forecasting Metrics Mean Absolute Error (MAE) Mean Squared Error (MSE) Root Mean Squared Error (RMSE) Mean Absolute Percentage Error (MAPE) Symmetric Mean Absolute Percentage Error (SMAPE) Mean Directional Accuracy (MDA) Reinforcement Learning Metrics Average Reward Discounted Sum of Rewards Entropy of Policy Exploration-Exploitation Tradeoff Metrics What is the business objective Imbalance and Cost Sensitivtiy Threshold Selection Data Type Interpretability Robustness (IMPT!) Evaluation methods for Model comparison &amp; Model quality control When drawing conclusion about model performance, consider Students t-test Perturbation test (corruption, adversarial attack) Invariance test (Bias removal) Directional Expectation test (Common sense directions. E.g.: rainy season shouldn't have much higher temperature than dry season) Model calibration (when standalone probability in the output matters) see page 10 Confidence Evaluation (usefulness threshold for each individual prediction) Slice-based Evaluation (model performance on subgroups) Step 3: Training &amp; Optimization On what platform is the model trained? Do we use distribtued training? What are the potential issues Hardware (GPU memory, inter-GPU communication speed) Overfitting/underfitting Concept Drift training stability (less fluctuations) dead neuron Local minima vanishing/exploding gradients How to do debugging Start simple and gradually add more components (*)Overfit a single batch: If model can't overfit a small amount of data, there's something wrong with your implementation. Set seed properly Is hyperparameter tuning needed? Setup routine for tuning? How to optimize the training to make it feasible/efficient/fault tolerant? Mixed Precision Quantization FSDP/DDP/Tensor/Model/Pipeline Checkpointing Accumulation Knowledge Distillation PEFT? (LoRA, Prefix Tuning) What optimizer do we use? its scheduler? What loss do we use? Most of the time it is just same as metrics other scenarios include: Reconstruction loss: mean squared error (MSE) for continuous data or binary cross-entropy for binary data KL Divergence Contrastive Loss: Encourages similarity between augmented versions of the same sample and dissimilarity between different samples. (Siamese Networks / Triplet Loss / SimCLR / Contrastive Divergence Loss (restricted boltzmann machine)) Step 4: ExperimentsExperiment tracking are important, especially when the scale of training is large, and teamwork is involved. There are a lot of tracking tools available out there. kubeflow, mlflow, wandb, neptune.ai… you name it. When using these tools, what's critical is to consider things to keep track of. Must-have's Code: Preprocessing + training + evaluation scripts, Notebooks for feature engineering Other util codes Environment: Save the environment configuration files like Dockerfile (Docker), requirements.txt (pip), pyproject.toml (e.g., hatch and poetry), or conda.yml (conda). (IMPT) Save Docker images on Docker Hub or your own container repository is always a good practice before running experiment Data: Saving data versions (as a hash or locations of immutable data resources) You can also use modern data versioning tools like DVC (and save the .dvc files to your experiment tracking tool). Parameters: Experiment run’s configuration Save parameters used via the command line (e.g., through argparse, click, or hydra) Metrics: Logging evaluation metrics on train, validation, and test sets for every run. Task-specific Traditional ML Model weights Evaluation charts (ROC curves, Confusion matrix) Prediction distributions Deep Learning Model checkpoints (both during and after training, but beware of the cost) Gradient norms (to control for vanishing or exploding gradient problems) Best/worst predictions on the validation and test set after training Hardware resources: CPU/GPU Utility, Memory Utility, Disk I/O, Network Utility, throughput Computer Vision Model predictions after every epoch (labels, overlayed masks or bounding boxes) NLP, LLM Inference time Prompts (in the case of generative LLMs) Specific evaluation metrics (e.g., ROUGE for text summarization or BLEU for translation between languages) Embedding size and dimensions, type of tokenizer, and number of attention heads (when training transformer models from scratch) Feature importance, attention-based, or example-based explanations (see this overview for specific algorithms and more ideas) Structure Data Input data snapshot ( .head() on DataFrames if you are using pandas) Feature importance (e.g., permutation importance) Prediction explanations like SHAP or partial dependence plots (they are all available in DALEX) RL Episode info: return, length, intermediate states Total environment steps, wall time, steps per second Value and police function losses Aggregate statistics over multiple environments and/or runs Hyper Optim Run score: the metric you are optimizing after every iteration Run parameters: parameter configuration tried at each iteration Best parameters: best parameters so far and overall best parameters after all runs have concluded Parameter comparison charts: there are various visualizations that you may want to log during or after training, like parallel coordinates plot or slice plot (they are all available in Optuna, by the way) Final thoughts on ML training (aka model exploration)when developing ML models with exploratory experiments, I’ve always enjoyed the pseudocode from this blog post. This is really what it means to do ML in a real industrial setup. 1234567891011121314time, budget, business_goal = business_specification()creative_idea = initial_research(business_goal)while time and budget and not business_goal: solution = develop(creative_idea) metrics = evaluate(solution, validation_data) if metrics &gt; best_metrics: best_metrics = metrics best_solution = solution creative_idea = explore_results(best_solution) time.update() budget.update() To be continued…Usually this is where the school-based projects will end. However, to fully develop a system based on the model and the idea you derive, you still need some engineering skills in building the pipeline for deployment, serving and performance monitoring. Luckily we have a lot of tools for these that do the dirty works for us. Nonetheless, not paying attention to some details in these steps could lead to serious bugs or even cost you thousands of dollars! Hence to both remind myself and give some suggestions for the readers, I've created a part II for the checklist for these steps. Please check it out if you are interested!","link":"/post/blogs/mlops/deep-learning-system-design-1/"},{"title":"Deep Learning System Design - A Checklist (Part II)","text":"A quick recapIn the previous post, checklist part I, we've talked about the early stage of designing a deep learning system. These steps are often of paramount importance when we build some ml projects in our courseworks. At the end of these steps, we often have a ready-to-use model that solves the problem at hand. However, if we really want to make it a product, to benefit thousand of users or the community, a lot of engineering work on the backend still need to be done. This includes: Saving the model artifact, wrapping the solution up and deploy it Creating endpoints for user to interact with, aka model serving Iteratively update the model by monitoring the model performance and system performance, and fix any issue related to the product. Let's go through each of the step one by one. Step 5: Packaging and DeploymentTechnically speaking, packaging a model isn't really the right word to describe the process of saving a trained model for usage. When people talk about packaging a model, they usually mean storing the trained model somewhere to deploy it for future usage. Thus it is closely related to deployments. Hence, when it comes to saving the model, here's the things to look out for: What platform do you use to store the model: local? cloud? edge? What metadata do you need? model hyperparams? dependencies (this can be tricky a lot of times) model json files? (example: hugging face models) how do you do the what's the size requirement? can we containerize it? (i.e. building an environment easy for deployment and serving) Is model-versioning done effectively? Does the saved model work perfectly in the infrastructure? (GPU? Memory? Network?) Knowing when to update the model when deploying the model, several strategies can be considered as well. For example: Directly use the existing endpoints from experiment tracking tools (e.g. wandb, kubeflow) Setup external APIs (SageMaker, AWS Lambda, AWS ECS) Shadow Deployment A/B Testing (with bandit method sometimes) Canal Deployment Step 6: ServingThis is where the endpoint becomes crucial, you need to consider several components what is the backend api tool you use do you containerize your api server? do you make it a distributed system? are concurrency and parallelism available options? whether the inference task will be cpu/gpu bound or io bound? do you consider batch inference? streaming inference? (latency requirement) will message queue become important for communication between api server and model server? (e.g. failure recovery) are there ways to easily integrate the metrics from serving to the monitoring tool? (callback functions for example) how do you handle the input data? (database management) how to save the request/response information for future usage? (example: user feedback collection) Is there a way to conduct quick test for serving before user acceptance test? Security issues? How do you direct traffic to different models and collect results from them? (e.g. paired t-test during shadow deployment) Step 7: MonitoringDon't forget to do logging as it is super important. Make it structured with time stamps and severity levels. Some of the objects for the data and model components you should log include: Data pipeline events, Production data (if possible, include the metadata alongside), Model metadata; this includes the model version and configuration details, Prediction results from the model, Prediction results from shadow tests (challenger models); if applicable to your system, Ground truth label (if available), General operational performance (that is typical to standard monitoring systems). Some best practices include: For your pipeline, you should be logging runs from scheduled time to start time, end time, job failure errors, the number of runs; all to make an unhealthy pipeline easier to troubleshoot. For your models, you should be logging the predictions alongside the ground truth (if available), a unique identifier for predictions (prediction_id), details on a prediction call, the model metadata (version, name, hyperparameters, signature), the time the model was deployed to production. For your application, you should be logging the number of requests served by the champion model in production, average latency for every serving. For your data, log the version of every preprocessed data for each pipeline run that was successful so that they can meet audited and their lineage can be traced. For storing the structure of your logs, consider using a JSON format with an actual structure so they can be easily parsed and searched. Consider rotating log files for better management; delete old and unnecessary logs that you're sure you won't need again for auditing or other reasons. System-related Throughput Latency Endpoint Availability System Error Rate (e.g. system overload time, number of failed requests) Total number of API calls CPU/GPU Utility Disk I/O Memory Utility Dependency Health Cloud Infra Health Resource Cost Model-related Error Rate, Model Drifts data drift between training data and request data Data Quality Issues Outliers Detection &amp; Handling Retraining Frequency Model Versioniong Prediction Metrics Model Poisoning Attack Detection Explainability Audit Trails + Privacy User Feedback ConclusionWhile a deep learning system can “almost” be always built following the checklist I made here, we must stay close to our business objective for the system to be truly useful. In that sense, a close connection to our user would be very important, and things like defensive programming, friendly UI and user feedbacks play super important roles. In future posts, I'll talk about some of them. Stay tuned ~ References A Comprehensive Guide on How to Monitor Your Models in Production - Neptune.ai","link":"/post/blogs/mlops/deep-learning-system-design-2/"},{"title":"Dimension Reduction: Life savers","text":"OverviewHigh dimensional data modeling has always been a popular topic of discussion. Many research and work are done in this field simply because we have limited computational power. Even if quantum technology can greatly boost this power in near future, we will still face the curse of unlimited flow of data and features. Thus, it's actually extremely important that we reduce the amount of data input in a model. In this blog, we explore several well-known tools for dimensionality reduction, namly Linear Discriminant Analysis (LDA), Principle Component Analysis (PCA) and Nonnegative Matrix Factorization (NMF). LDA1. Definition Can be either a predictive modeling algorithm for multi-class classification or a dimensionality reduction technique A Generative Learning Algorithm based on labeled data Assumes Gaussian Distribution for ; Each attribute has the same variance (Mean removal/Feature Engineering with Log/Root functions/Box-Cox transformation needed) The class calculated from the discrimination function as having the largest value will be the output classification () LDA creates a new axis based on Maximize the distance between means Minimize the variations within each categories Procedure For Dimensionality Reduction (reduced-rank LDA) Compute the d-dimensional mean vectors for the different classes from the dataset. Compute the scatter matrices (in-between-class and within-class scatter matrix). Compute the eigenvectors (e1,e2,…,ed) and corresponding eigenvalues () for the scatter matrices. Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix W (where every column represents an eigenvector). Use this d×k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: Y=XW (where X is a n×d-dimensional matrix representing the n samples, and y are the transformed n×k-dimensional samples in the new subspace). For classification (Essentially, LDA classifies the sphered data to the closest class mean.) Perform eigen-decomposition on the pooled covariance matrix Spheres the data: to produce an identity covariance matrix in the transformed space Obtain group means in the transformed space: Classify according to : where is the group's prior probability Extensions to LDA Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables). Flexible Discriminant Analysis (FDA): Where non-linear combinations of inputs is used such as splines. Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA. 2. Pros &amp; ConsPros Need Less Data Simple prototype classifier: Distance to the class mean is used, it's simple to interpret. The decision boundary is linear: It's simple to implement and the classification is robust. Cons Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired. In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired. Support for more complex prototype classification is desired. 3. Application Bankruptcy prediction Facial recognition 4. Code implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from sklearn.datasets import load_wineimport pandas as pdimport numpy as npnp.set_printoptions(precision=4)from matplotlib import pyplot as pltimport seaborn as snssns.set()from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_split# Loading Datawine = load_wine()X = pd.DataFrame(wine.data, columns=wine.feature_names)y = pd.Categorical.from_codes(wine.target, wine.target_names)# Merge X and y (Training set)df = X.join(pd.Series(y, name='class'))## The Simply Way: using sklearnfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminantAnalysis()X_lda = lda.fit_transform(X, y)## The hard way: Understand the logic here# Compute means for each classclass_feature_means = pd.DataFrame(columns=wine.target_names)for c, rows in df.groupby('class'): class_feature_means[c] = rows.mean()class_feature_means# Compute the Within Class Scatter Matrix using the Mean Vectorwithin_class_scatter_matrix = np.zeros((13,13))for c, rows in df.groupby('class'): rows = rows.drop(['class'], axis=1)s = np.zeros((13,13))for index, row in rows.iterrows(): x, mc = row.values.reshape(13,1), class_feature_means[c].values.reshape(13,1) s += (x - mc).dot((x - mc).T) within_class_scatter_matrix += s# Compute the Between Class Scatter Matrix:feature_means = df.mean()between_class_scatter_matrix = np.zeros((13,13))for c in class_feature_means: n = len(df.loc[df['class'] == c].index) mc, m = class_feature_means[c].values.reshape(13,1), feature_means.values.reshape(13,1) between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)# Compute the Eigenvalues &amp; Eigenvectors, then sort accordinglyeigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]pairs = sorted(pairs, key=lambda x: x[0], reverse=True)# Print the Explained Varianceeigen_value_sums = sum(eigen_values)print('Explained Variance')for i, pair in enumerate(pairs): print('Eigenvector {}: {}'.format(i, (pair[0]/eigen_value_sums).real))# Identify the Principle Eigenvalues (here k = 2); Compute the new feature space X_ldaw_matrix = np.hstack((pairs[0][1].reshape(13,1), pairs[1][1].reshape(13,1))).realX_lda = np.array(X.dot(w_matrix))le = LabelEncoder()y = le.fit_transform(df['class']) # Here the y is just the encoded label set# Visualizeplt.xlabel('LD1')plt.ylabel('LD2')plt.scatter( X_lda[:,0], X_lda[:,1], c=y, cmap='rainbow', alpha=0.7, edgecolors='b') PCA1. Definition A linear model (base version) aimed at unsupervised dimensionality reduction. Basic intuition: projecting data onto its orthogonal feature subspace It is a technique for feature extraction — it combines our input variables in a specific way, then we can drop the \"least important\" variables while still retaining the most valuable parts of all of the variables (high variance, independent, few number) Each of the \"new\" variables after PCA are all independent of one another (due to the linear model assumption) PCA effectively minimizes error orthogonal to the model itself It can only be applied to datasets which are linearly separable Complete procedure Standardize the data. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix (by performing eigendecomposition), or perform Singular Value Decomposition (SVD) [SVD preferred due to efficiency and automatic eigenvalue sorting] Find the hyperplane that accounts for most of the variation using SVD (that hyperplane represents 1st componet); This basically means it tries to minimize the points’ distance to the hyperplane/maximize the distance from the projected point to the origin Find the orthogonal subspace, get from the subspace a best hyperplane with largest variation using SVD again (2nd component which is clearly independent of 1st component) Repeat to find all the components Use the eigenvalues to determine the proportion of variation that each component account for; construct the new system using the principle (top k) components; plot the samples using the projections on components as coordinates We should keep the component if it contributes substantially to the variation It eventually cluster samples that are highly correlated; It is possible to restore all the samples if each component correspond to one distinct variable (Note that # of PC &lt; # of Samples) Warning: scaling and centering data is very Important!!! Kernel PCA an extension of PCA into non-linear dataset by project dataset into a higher dimensional feature space using the kernel trick (recall SVM) Some popular kernels are Gaussian/RBF Polynomial Hyperbolic tangent: Note that the kernel matrix still need to be normalized for PCA to use kernel PCA so kernel PCA will have difficulties if we have lots of data points. Robust PCA an extension of PCA to deal with sparsity in the matrix It factorizes a matrix into the sum of 2 matrices, , where is the original matrix, is the low-rank (with lots of redundant information) matrix and is a sparse matrix (In the case of corrupted data, often captures the corrupted entries) Application: Latent semantic indexing =&gt; captures all common words while captures all key words that best identify each document. The minimization is over subject to . Minimizing L1-norm results in sparse values, while minimizing nuclear norm (sometimes also use Frobenious norm ) leads to sparse singular values (hence low rank) 2. Pros &amp; ConsPros Removes Correlated Features Improves Algorithm Performance Reduces Overfitting Cons Independent variables become less interpretable Data standardization is must before PCA Information Loss (if PCs chosen are not sufficient) 3. Application When interpretability is not an issue, use pca When the dimension is too large or you want to identify features that are independent, use pca 4. ComparisonPCA vs LDA Not as good as LDA in clustering/classification effect, yet idea for Factor analysis PCA projects the entire dataset onto a different feature (sub)space, and LDA tries to determine a suitable feature (sub)space in order to distinguish between patterns that belong to different classes PCA vs ICA In PCA the basis you want to find is the one that best explains the variability of your data; In ICA the basis you want to find is the one in which each vector is an independent component of your data (which usually has mixed signals/mixed features) 5. Steps Standardize the data to ensure that all variables have a mean of 0 and a standard deviation of 1. Compute covariance matrix: This matrix shows how each variable is related to every other variable in the dataset Eigen-decomp + feature selection Projection / transformation 6. Code implementation sklearn version 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns; sns.set(style='white')%matplotlib inline%config InlineBackend.figure_format = 'retina'from sklearn import decompositionfrom sklearn import datasetsfrom mpl_toolkits.mplot3d import Axes3Ddigits = datasets.load_digits()X = digits.datay = digits.target# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))plt.figure(figsize=(16, 6))for i in range(10): plt.subplot(2, 5, i + 1) plt.imshow(X[i,:].reshape([8,8]), cmap='gray')pca = decomposition.PCA(n_components=2)X_reduced = pca.fit_transform(X)print('Projecting %d-dimensional data to 2D' % X.shape[1])plt.figure(figsize=(12,10))plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, edgecolor='none', alpha=0.7, s=40, cmap=plt.cm.get_cmap('nipy_spectral', 10))plt.colorbar()plt.title('MNIST. PCA projection')pca = decomposition.PCA().fit(X)plt.figure(figsize=(10,7))plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)plt.xlabel('Number of components')plt.ylabel('Total explained variance')plt.xlim(0, 63)plt.yticks(np.arange(0, 1.1, 0.1))plt.axvline(21, c='b')plt.axhline(0.9, c='r')plt.show() numpy version 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltimport numpy.random as rnd# Create random 2d datamu = np.array([10,13])sigma = np.array([[3.5, -1.8], [-1.8,3.5]])print(\"Mu \", mu.shape)print(\"Sigma \", sigma.shape)# Create 1000 samples using mean and sigmax = rnd.multivariate_normal(mu, sigma, size=(1000))print(\"Data shape \", x.shape)# normalize / standardizex = (x - x.mean(axis=0, keepdims=True)) / x.std(axis=0, keepdims=True)# compute covariancecov = np.cov(x.T) # IMPT: don't forget the transpose hereeig_val, eig_vec = np.linalg.eig(cov)indices = np.argsort(eig_val * -1)eig_val = eig_val[indices]eig_vec = eig_vec[:,indices]print(\"Sorted Eigen vectors \", eig_vec)print(\"Sorted Eigen values \", eig_val, \"\\n\")# Get explained variancesum_eig_val = np.sum(eig_val)explained_variance = eig_val / sum_eig_valprint(explained_variance)cumulative_variance = np.cumsum(explained_variance)print(cumulative_variance)pca_data = np.dot(x, eig_vec)print(\"Transformed data \", pca_data.shape)# Reverse PCA transformationrecon_data = pca_data.dot(eig_vec.T) * var + mean NMF1. Definition It automatically extracts sparse and meaningful (easily interpretable) features from a set of nonnegative data vectors. We basically factorize into 2 smaller matrices non-negative and such that and ( low-rank approximate factorizations) Interpretation of and : Basically, we can interpret to be a weighted sum of some components, where each row in is a component, and each row in contains the weights of each component Idea of the algo: Formalize an objective function and iteratively optimize it A local minima is sufficient for the solution Objective function to minimize : Frobenius norm: w.r.t. s.t. Generalized Kullback-Leibler divergence: Choices of Optimization technique used: Coordinate descent (alternative: gradient descent which fix and optimize , then fix and optimize until tolerance is met) Multiplicative Update , Method to choose the optimal factorisation rank, : General guideline: Trial and error, Estimation using SVD based of the decay of the singular values Insights from experts Tricks Initialization: uses SVD to compute a rough estimate of the matrices and . If the non-negativity condition did not exist, taking the top k singular values and their corresponding vectors would construct the best rank k estimate, measured by the frobenius norm. Since and must be non-negative, we must slightly modify the vectors we use. Regularization: Since the represents weights of a component, it may produces weights that are too high/low. The classical way is to use or regularization losses 2. Application NMF is suited for tasks where the underlying factors can be interpreted as non-negative Image processing Topic Modeling Text mining Hyperspectral unmixing 3. Code Implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135from operator import itemgetterfrom concurrent.futures import ProcessPoolExecutorfrom time import timeimport osimport gensimimport pandas as pdimport itertoolsimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefrom nltk.stem import WordNetLemmatizerfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.decomposition import NMFfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoderlemmatizer = WordNetLemmatizer()data_path = 'matrix_factorization_arxiv_query_result.json'articles_df = pd.read_json(data_path)articles_df.head()def stem(text): return lemmatizer.lemmatize(text)def map_parallel(f, iterable, **kwargs): with ProcessPoolExecutor() as pool: result = pool.map(f, iterable, **kwargs) return resultdef retrieve_articles(start, chunksize=1000): return arxiv.query( search_query=search_query, start=start, max_results=chunksize )def vectorize_text(examples_df, vectorized_column='summary', vectorizer=CountVectorizer): vectorizer = vectorizer(min_df=2) features = vectorizer.fit_transform(examples_df[vectorized_column]) le = LabelEncoder() ohe = OneHotEncoder() labels = le.fit_transform(valid_example_categories).reshape(-1, 1) labels_ohe = ohe.fit_transform(labels).todense() vectorized_data = { 'features': features, 'labels': labels, 'labels_onehot' : labels_ohe } return vectorized_data, (vectorizer, ohe, le)def extract_keywords(text): \"\"\" Use gensim's textrank-based approach \"\"\" return gensim.summarization.keywords( text=stem(text), lemmatize=True )def filter_out_small_categories(df, categories, threshold=200): class_counts = categories.value_counts() too_small_classes = class_counts[class_counts &lt; threshold].index too_small_classes valid_example_indices = ~categories.isin(too_small_classes) valid_examples = df[valid_example_indices] valid_example_categories = categories[valid_example_indices] return valid_examples, valid_example_categoriesdef print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): message = \"Topic #%d: \" % (topic_idx + 1) message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]) print(message) print()categories = articles_df['arxiv_primary_category'].apply(itemgetter('term'))main_categories = categories.apply(lambda s: s.split('.')[0].split('-')[0])main_categories_counts = main_categories.value_counts(ascending=True)main_categories_counts.plot.barh()plt.show()main_categories_counts[main_categories_counts &gt; 200].plot.barh()plt.show()categories.value_counts(ascending=True)[-10:].plot.barh()plt.show()articles_df['summary_keywords'] = [extract_keywords(i) for i in articles_df['summary']]article_keyword_lengths = articles_df['summary_keywords'].apply(lambda kws: len(kws.split('\\n')))valid_examples, valid_example_categories = filter_out_small_categories(articles_df, main_categories)vectorized_data, (vectorizer, ohe, le) = vectorize_text( valid_examples, vectorized_column='summary_keywords', vectorizer=TfidfVectorizer)x_train, x_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split( vectorized_data['features'], vectorized_data['labels_onehot'], vectorized_data['labels'], stratify=vectorized_data['labels'], test_size=0.2, random_state=0)nmf = NMF(n_components=5, solver='mu', beta_loss='kullback-leibler')topics = nmf.fit_transform(x_train)n_top_words = 10tfidf_feature_names = vectorizer.get_feature_names()print_top_words(nmf, tfidf_feature_names, n_top_words)dominant_topics = topics.argmax(axis=1) + 1categories = le.inverse_transform(y_train_labels[:,0])pd.crosstab(dominant_topics, categories)","link":"/post/blogs/mlops/dimentionality-reduction/"},{"title":"Understanding Distributed Training in Deep Learning","text":"IntroductionSince last year, the quest for large X models have been nonstop, and people kept exploring the possibility to build more universal and robust models. While some still put a doubt if models with more parameters will be effective, most have faith in the scaling law proposed by DeepMind and OpenAI researchers. The progress in 1 year is promising, as it seems that we are steadily moving towards the era of AGI. However, the education barely follows. College and Unversity are still bound by the budget to enable students to get in touch to large model training, especially when it comes to multi-gpu / multi-node distributed training. In light of this, I would love to share what I understand about distributed training, and how can we get started in this domain to catch up with recent industrial progress. 1. Definition Leverages multiple compute resources—often across multiple nodes or GPUs—simultaneously, accelerating the model training process. Mainly a form of parallelism, requires some understanding of low-level operation system (memory, communication and GPU architecture) For those interested, I will recommend taking CMU 15-418 Parallel Computer Architecture and Programming to get an in-depth understanding. 2. Parallelism in Training Two primary forms of parallelism: model parallelism and data parallelism Model Parallelism: Used when a model doesn’t fit into the memory of a single device. Different parts of the model are placed on different devices, enabling the training process to occur across multiple GPUs or nodes. This approach is particularly useful for exceptionally large models. Data Parallelism: Split the dataset across various devices, with each processing a unique subset of the data. The model’s parameters are then updated based on the collective gradients computed from these subsets (with different strategies). 3. Strategies in detail[Note]: I'll mainly use PyTorch in this blog as it is the most popular and convenient choice. It is mainly based on torch.distributed package. In the meantime, some convenient scripts are created by Lightning AI with their own libraries. I'll show some code using their library for people who just want a shortcut and get rid of the details behind distributed training. Data Parallelism How DistributedDataParallel works: NCCL: multi-GPU, multi-node communication primitives. all-gather, all-reduce, broadcast, reduce-scatter, reduce routines, point-to-point send/receive. High bandwidth, low latency on PCIe and NVLink interconnects All GPUs share same initial weights. Aggregate all gradients in different GPUs and update the weight collectively. Need to update optimizer state and weights after AllReduce. DDP Implementation 12345678910111213141516171819202122232425262728### DDP - PyTorch Versionimport torchimport torch.distributed as distfrom torch.nn.parallel import DistributedDataParallel as DDPfrom torch.utils.data.distributed import DistributedSamplerdef main(): # Initialize distributed environment dist.init_process_group(backend='nccl') # Create model model = YourModel() model = DDP(model) # Load data and distribute it across processes train_loader = DistributedSampler(YourDataset()) # Training loop for epoch in range(epochs): for data in train_loader: inputs, labels = data outputs = model(inputs) loss = YourLoss(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() For more advanced details like RPC-Based Distributed Training (RPC) and Collective Communication (c10d), refer to torch.distributed original docs Fully Sharded DP (FSDP) What is in the GPU memory (x params, FP16) Params: 2x (fp16 with 2 bytes) Gradients: 2x Optimizer (AdamW) Param copy: 4x (float32) Momentum: 4x Variance: 4x How FSDP works FSDP unit (vertical splitting), can be: A layer splitted A stage splitted A group of layers splitted Sharding Storing the FSDP unit on FlatParameter Split FlatParameter on multiple nodes (after zero padding for divisible property) All-Gather performed by NCCL gather all parts and sync across all nodes Done before both forward and backwards discard peer parts after forward/backward Reduce-scatter performed via NCCL Each node gets part of the result of gradient (backward only) Note that All-Reduce is not used coz it broadcast same results to all nodes E.g. Each node i has all gradients G_i1, G_i2, ..., G_in, after reduce-scatter, each node will have gradient redistributed, with node i getting sum of G_ki, where k spans from 1 to n Reason to use/not to use FSDP When to use Model size is too large (not data size) More communication between GPUs Hence trade memory for speed: more GPU memory cost due to communication, however, communication overhead reduced via NCCL acceleration If want to trade speed for memory, see activation checkpointing When not to use For models &lt; 100 million params, consider activation-checkpointing and reversible layers Recommend to use BFloat16 instead of Float16 (Float16 requires ShardedGradScaler) Mixed Precision Training Concern (Package compatibility) FSDP Implementation 123456789101112131415161718192021222324252627282930### FSDP Versionfrom torch.distributed.fsdp import ( FullyShardedDataParallel, CPUOffload,)from torch.distributed.fsdp.wrap import ( default_auto_wrap_policy, enable_wrap, wrap)import torch.nn as nnclass model(nn.Module): def __init__(self): super().__init__() self.layer1 = nn.Linear(8, 4) self.layer2 = nn.Linear(4, 16) self.layer3 = nn.Linear(16, 4)model = DistributedDataParallel(model())fsdp_model = FullyShardedDataParallel( model(), fsdp_auto_wrap_policy=default_auto_wrap_policy, cpu_offload=CPUOffload(offload_params=True),)# Custom wrapwrapper_kwargs = Dict(cpu_offload=CPUOffload(offload_params=True))with enable_wrap(wrapper_cls=FullyShardedDataParallel, **wrapper_kwargs): fsdp_model = wrap(model()) Model Parallelism split horizontally Implementation1234567891011121314class model_parallel(nn.Module): def __init__(self): super().__init__() self.layer_1 = nn.Sequential(...) self.layer_2 = nn.Sequential(...) self.layer_1.cuda(0) self.layer_2.cude(1) def forward(self, x): x = x.cuda(0) x = self.layer_1(x) x = x.cuda(1) x = self.layer_2(x) x = ... return x Inefficient sometimes: in the code above, GPU may be idle if layer 2 is not run during training Does not work well if the model architecture does not naturally lend itself to being divided into parallelizable segments. Pipeline Parallelism Mixed data and model parallelism, involves scheduling of data flow Split into multiple stages, and each stage is assigned to a different device The output of one stage is fed as input to the next stage. Sometimes inefficient and suffers from idle time when machines are waiting for other machines to finish their stages: pipeline is waiting for a stage to finish in both the forward and backward pass, the period when some machine are idle aer referred to as a bubble. Tensor parallelism Split vertically + horizontally (in units of a tensor) Can be more effective as it leverages efficiencies within matrix multiplication by spliting a tensor up into smaller fractions and expedite the computation The detail can be expanded into another blog, however, I will refer you to this excellent blog instead of reinventing the wheel myself again. Might require models specifically designed to take advantage of this form of parallelism. It may not be as universally applicable as data or model parallelism. torchrun An elegant way to run distributed training using torch.distributed package. Please refer to details here Make use of rendezvous backend to achieve high availability and failure recovery A few major advantages include: Single-node multi-worker Multi-node Multi-GPU Fault tolerant Elastic Distributed Training on the Cloud Since most of the resources are available from the cloud, and they are on-demand, it is common practice to migrate local code to be run on remote servers. You can spin up GPU resources (usually more capable than your local version) yourself and manage the dependencies/monitoring independenly, or you can resort to integrated solutions like AWS SageMaker or Azure ML or Google AI Studio as they often provide convenient API endpoints to interact with those GPU instances. In many scenarios, their management include inter-gpu/inter-node communication as well, which is a big plus. As an example, you can setup AWS accordingly and run your distributed training using SageMaker via this tutorial A sample script is as follows: 1234567891011from sagemaker.pytorch import PyTorchestimator = PyTorch( ..., instance_count=2, instance_type=\"ml.p4d.24xlarge\", # Activate distributed training with SMDDP distribution={ \"pytorchddp\": { \"enabled\": True } } # mpirun, activates SMDDP AllReduce OR AllGather # distribution={ \"torch_distributed\": { \"enabled\": True } } # torchrun, activates SMDDP AllGather # distribution={ \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } } # mpirun, activates SMDDP AllReduce OR AllGather) Other packages PyTorch Lightning - a lightweight PyTorch wrapper that provides a high-level interface for researchers and practitioners to streamline the training of deep learning models. It abstracts away many of the boilerplate code components traditionally required for training models, making the code cleaner, more modular, and more readable. It requires little setup of code and just need to insert a few parameters to the trainer Example1234567891011trainer = L.Trainer( max_epochs=3, callbacks=callbacks, accelerator=\"gpu\", devices=4, # &lt;-- NEW strategy=\"ddp\", # &lt;-- NEW precision=\"16\", logger=logger, log_every_n_steps=10, deterministic=True,) Hugging Face Accelerate: a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code. It is still built on torch_xla and torch.distributed, but have get users rid of writing custom code to adapt to these platforms. Benefits include easy utilization of ZeRO Optimizer from DeepSpeed, achieve FSDP and mixed-precision training as well. Example 1234567891011121314151617from accelerate import Acceleratoraccelerator = Accelerator()model, optimizer, training_dataloader, scheduler = accelerator.prepare( model, optimizer, training_dataloader, scheduler)for batch in training_dataloader: optimizer.zero_grad() inputs, targets = batch inputs = inputs.to(device) targets = targets.to(device) outputs = model(inputs) loss = loss_function(outputs, targets) accelerator.backward(loss) optimizer.step() scheduler.step() In terminal, run accelerate launch {my_script.py} 4. Challenges and Solutions Communication Overhead: In distributed training, the exchange of information between devices becomes a potential bottleneck. As the number of devices increases, coordinating updates and sharing gradients become more complex. Solutions: Optimized Communication Protocols: Leveraging optimized communication protocols, such as NVIDIA NCCL for GPU communication, helps minimize the latency associated with inter-device communication. Gradient Accumulation: By accumulating gradients locally on each device before synchronization, communication frequency is reduced. This strategy can be beneficial in scenarios where frequent synchronization is not necessary. Fault Tolerance: In distributed environments, hardware failures or network issues are inevitable. Ensuring fault tolerance is essential to maintain the integrity of the training process. Solutions Checkpointing: Regularly saving model checkpoints allows training to resume from the most recent checkpoint in case of a failure. This practice minimizes data loss and ensures continuity. Redundancy: Introducing redundancy by running multiple instances of the training job across different nodes adds a layer of resilience. Load balancing techniques can be employed to distribute tasks effectively. Scaling Issues: Scaling distributed training to a large number of nodes presents challenges in terms of efficiency and resource management. Strategies Dynamic Resource Allocation: Implementing dynamic resource allocation ensures that resources are allocated efficiently based on the current load. Kubernetes and other orchestration tools can facilitate dynamic scaling. Parameter Servers: Utilizing parameter servers, which are dedicated servers responsible for storing and distributing model parameters, can enhance the scalability of distributed training. 5. Common Mistakes Not pipelining Pipeline Parallelism is always something to include. Notice the use of ZeRO-3 also uses pipeline parallelism Not balancing pipeline stages There will be some brief periods where either a machine is idle and waiting on the next minibatch from the previous machine or takes longer than other machines to execute its computation, thus slowing down the pipeline. You should ideally construct your pipeline such that each machine does as close to the same amount of computation as possible. This means timing how long it takes data to get through different layers in the model, timing how long forward and backward propagation takes for each model partition, and ensuring roughly equivalent data sizes across mini-batches. This is critical for optimizing pipeline efficiency. To achieve this, setting up profiler like PyTorch Profiler is critical for evaluation of computations done during model training Weight staleness When model training is pipelined across multiple machines, there is a delay that happens between when the forward computation on data occurs and when the gradients based on that computation are backpropagated to update the model weights. As a result, forward propagation are calculated using weights that aren't updated with the latest gradients. Solution: weight stashingA system “maintains multiple versions of a model’s weights, one for each minibatch.” After the completion of each forward pass, the system can store a model’s weights as part of the state associated with that minibatch. When the time comes for backpropagation, the weights associated with that minibatch are retrieved from the stash and used for the gradient computation. This ensures that the same version of weights are used for the forward and backward pass over a single minibatch of data within a pipelined stage, and statistical convergence is improved. Driver and library inconsistencies between machines Containerization / Virtualization using tools like Docker solves the problem Wrong type of Optimizer Update Example: Synchronous vs Asynchronous SGD Asynchronous SGD (HogWild as a popular choice) which showed that SGD could be run in parallel, without locks, and without too much effect on algorithm convergence. Asynchronous SGD allows weight updates to proceed without each machine waiting for the other to send their gradients. Network issues, firewalls, ports, and communication errors Solutions: Relying less on network for communication If necessary to communicate, a process must specify the IP address and port number across which to transmit this information Backup Frequently Better logging Slow data transmission Solutions: Avoid making RPC calls Try higher bandwidth interconnects like NVLink and Infini-band FP32 -&gt; FP16 / Mixed precision transmit a subset of gradients as soon as they are calculated (i.e. sending the gradients of a single layer) while at the same time, backpropagation is being performed on subsequent layers. 6. A complete Distributed DL pipeline Distributed Training Setup: Set up a distributed computing environment, typically using a cluster or cloud infrastructure like AWS, Google Cloud, or Azure. Ensure that all nodes in the cluster have the necessary libraries (TensorFlow, PyTorch, etc.) and dependencies installed. Split the training dataset across nodes to distribute the workload. Synchronization and Communication: Implement a synchronization mechanism to ensure that the model’s weights are updated consistently across all nodes. Choose a communication protocol (e.g., Parameter Server, AllReduce) for aggregating gradients and exchanging model updates. Model Initialization: Initialize the same model architecture with random weights on all nodes. Setup model to follow data parallelism Training Loop (The main discussion we had in the blog): Start the training loop on each node with its batch of data. Compute gradients for the batch, update local weights, and synchronize with other nodes. Repeat this process for a predefined number of epochs. Implement early stopping to prevent overfitting and save the best-performing model checkpoint during training. Periodically evaluate the model’s performance on the validation dataset to ensure it’s learning effectively. Save model checkpoints at regular intervals during training to resume from a specific point in case of interruptions. If necessary, scale up the distributed training environment by adding more nodes to further accelerate training or handle larger datasets. Monitoring and Logging: Implement monitoring and logging to track training progress, including loss, accuracy, and other relevant metrics. Use tools like TensorBoard or custom logging solutions to visualize training statistics. Hyperparameter Tuning: Perform hyperparameter tuning, which may include learning rate adjustments, batch sizes, and other parameters, to optimize the model’s performance. Note: you should set a budget alert before this, as running multiple experiments (on a large model) in a distributed setting can be very COSTLY!!! Post-training Analysis: This can go before/after/hand-in-hand with step 6, as part of model tuning Analyze the trained model’s performance on the test dataset to assess its generalization capabilities. Deployment: Deploy the trained model for inference in your production environment, whether it’s on the cloud or at the edge. Sometime this requires distributing model weights across servers as well Additional Fine-tuning (Optional): Fine-tune the model as needed based on deployment feedback or new data. Checkout Hugging Face’s TRL library &amp; its tutorials to understand more. Documentation: Document the entire distributed training process, including configuration settings, data preprocessing steps, and model architecture, for future reference. Maintenance and Updates: Regularly update and maintain the distributed training system, including libraries, dependencies, and data pipelines, to ensure its reliability and performance. For the basic scripts without distributed training and with basic DDP. You may refer to the tutorial here. If you want a one-off solution, please refer to the code below. 7. A more challenging code using native PyTorchIf you are interested in building it from scratch with PyTorch directly, checkout the following code (if you don’t understand the syntax, please DIY) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233\"\"\"A demo on how to setup custom trainer with efficient training\"\"\"import osimport argparseimport apex.amp as ampimport torchimport torch.nn as nnimport torch.distributed as distimport torch.multiprocessing as mpfrom torch.nn.parallel import DistributedDataParallel as DDPfrom torch.utils.data import Dataset, DataLoaderfrom torch.utils.data.distributed import DistributedSamplerclass ConvNet(nn.Module): def __init__(self, num_classes=10): super(ConvNet, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) self.layer2 = nn.Sequential( nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) self.fc = nn.Linear(7*7*32, num_classes) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.reshape(out.size(0), -1) out = self.fc(out) return outdef ddp_setup() #(rank: int, world_size: int): \"\"\" Args: rank: Unique ID of each world_size: Total number of processes \"\"\" # multi-gpu setup # os.environ['MASTER_ADDR'] = 'your master node ip' # os.environ['MASTER_PORT'] = '8888' # dist.init_process_group( # backend='nccl', # init_method='env://', # world_size=world_size, # rank=rank # ) dist.init_process_group(backend=\"nccl\")class Trainer: def __init__( self, model: torch.nn.Module, train_data: DataLoader, optimizer: torch.optim.Optimizer, criterion: torch.nn.Module, # gpu_id: int, save_every: int, snapshot_path: str, ) -&gt; None: # self.gpu_id = gpu_id self.local_rank = int(os.environ[\"LOCAL_RANK\"]) self.global_rank = int(os.environ[\"RANK\"]) self.model = model.to(self.local_rank) self.train_data = train_data self.optimizer = optimizer self.criterion = criterion self.epochs_run = 0 self.save_every = save_every self.model, self.optimizer = amp.initialize( self.model, self.optimizer, opt_level='O1') if os.path.exists(snapshot_path): print(\"Loading Snapshot\") self._load_snapshot(snapshot_path) self.model = DDP(self.model, device_ids=[self.local_rank]) def _load_snapshot(self, snapshot_path): \"\"\"Resume training from previous checkpoint\"\"\" snapshot = torch.load(snapshot_path) self.model.load_state_dict(snapshot['model']) self.optimizer.load_state_dict(snapshot['optimizer']) self.epochs_run = snapshot[\"epochs_resume\"] amp.load_state_dict(snapshot['amp']) print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\") def _run_batch(self, source, targets): self.optimizer.zero_grad() output = self.model(source) loss = self.criterion(output, targets) loss.backward() with amp.scale_loss(loss, self.optimizer) as scaled_loss: scaled_loss.backward() self.optimizer.step() return loss.item() def _run_epoch(self, epoch: int, total_epochs: int): self.model.train() for i, (source, targets) in enumerate(self.train_data): source = source.to(self.local_rank) targets = targets.to(self.local_rank) loss = self._run_batch(source, targets) if (i + 1) % 100 == 0 and self.local_rank == 0: print( f'[GPU{self.global_rank}] Epoch [{epoch + 1}/{total_epochs}], Step [{i + 1}/{len(self.train_data)}], Loss: {loss:.4f}') self.model.eval() with torch.no_grad(): for i, (source, targets) in enumerate(self.val_data): source = source. targets = targets. loss = ... def _save_snapshot(self, save_dir, epoch, model_seed): path = f\"{save_dir}/base_model_seed={model_seed}_epoch={epoch}.pt\" torch.save({ 'model': self.model, # if saving state_dict, use .module.state_dict() 'optimizer_state_dict': self.optimizer.state_dict(), 'scheduler_state_dict': self.scheduler.state_dict(), 'amp': amp.state_dict(), 'epochs_resume': self.epochs_run }, path) print(f\"Epoch {epoch} | Training snapshot saved at {path}\") def train(self, total_epochs: int, model_seed: int, save_dir: str): \"\"\"Training script\"\"\" for epoch in range(self.epochs_run, total_epochs): self._run_epoch(epoch, total_epochs) if self.local_rank == 0 and epoch % self.save_every == 0: self._save_snapshot(save_dir, epoch, model_seed)def load_train_params(): train_set = MyTrainDataset(2048) model = ConvNet() optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) criterion = torch.nn.CrossEntropyLoss() return train_set, model, optimizer, criteriondef prepare_dataloader(dataset: Dataset, batch_size: int, num_workers: int, sampler: DistributedSampler): return DataLoader( dataset, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=num_workers, sampler=sampler )def run(args): \"\"\"Run entire pipeline\"\"\" torch.manual_seed(args.seed) # rank = args.nr * args.gpus + gpu # ddp_setup(rank, args.world_size) ddp_setup() dataset, model, optimizer, criterion = load_train_params() # sampler = DistributedSampler( # dataset, num_replicas=args.world_size, rank=rank) sampler = DistributedSampler(dataset) train_data = prepare_dataloader( dataset, batch_size=32, num_workers=0, sampler=sampler) trainer = Trainer(model, train_data, optimizer, criterion, args.save_every, args.snapshot_path) trainer.train(args.total_epochs, args.seed, args.save_dir) dist.destroy_process_group()def main(): \"\"\"Entry point to call the script using torchrun (which manages the ) e.g: 4 GPU per machine, 50 epochs, saving every 10 epoch torchrun \\ --nproc_per_node=4 \\ --nnodes=2 \\ --node_rank=0 \\ --rdzv_id=456 \\ --rdzv_backend=c10d \\ --rdzv_endpoint=HOST_MACHINE:PORT \\ FILE_NAME.py --epochs=50 --save_every=10 nproc_per_node: usually num of GPUs per machine nnodes: num of machines node_rank: ID: 0/1/2/.... for each machine Notes on endpoint: choose endpoint whose machine has high network bandwidth Note: Multi-GPU on single machine is much faster than Multi-node each with single GPU due to bandwidth limit over TCP \"\"\" parser = argparse.ArgumentParser() # parser.add_argument('-n', '--nodes', default=1, # type=int, metavar='N') # parser.add_argument('-g', '--gpus', default=1, type=int, # help='number of gpus per node') # parser.add_argument('-nr', '--nr', default=0, type=int, # help='ranking within the nodes') parser.add_argument('--epochs', default=2, type=int, metavar='N', help='number of total epochs to run') parser.add_argument('-s', '--seed', default=42, type=int, metavar='N') parser.add_argument('--save_every', default=5, type=int, help='interval to save the snapshot') parser.add_argument('--save_dir', default='.', type=str, help='directory to save the snapshot') parser.add_argument('--snapshot_path', default='.', type=str, help='path of the snapshot to restore training from') args = parser.parse_args() ######################################################### args.world_size = args.gpus * \\ args.nodes if args.gpus &gt;= 0 else torch.cuda.device_count() args.total_epochs = args.epochs # mp.spawn(main, nprocs=args.world_size, args=(args,)) run(args) ################################################################### POST MORTEM ###################\"\"\"Common Troubleshooting1. Check nodes can communicate through **TCP**2. Check inbound rules on a security group (on AWS)3. export NCCL_DEBUG=INFO to set verbose logs4. export NCCL_SOCKET_IFNAME to ensure TCP connection is correctSLURM work scheduler setup TODO\"\"\" References https://neptune.ai/blog/distributed-training-errors","link":"/post/blogs/mlops/distributed-training/"},{"title":"Feature Selection &amp; Model Selections","text":"OverviewRunning machine learning models have become much easier in recent years. The prevalence of tutorials and model packages makes it much more convenient for people to apply various theoretically complex algorithms on their datasets and thrive. So to excel in the field of data science, one cannot simple KNOW how to use models, but also appreciate each model's significance and select proper models wisely. That's where feature selections and model selections come in. Both turn out to be challenging and extremely useful in the same time. In light of this, I want to take down the notes I learned through practice and tutorials some key aspects of these two things. Feature Selection Benefits It enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret. It improves the accuracy of a model if the right subset is chosen. It reduces Overfitting Methods Here we discuss about some widely used methods for feature selections. To facilitate the demo code, we require the following packages to be applied and data being tuned: 123456789101112131415161718from sklearn.datasets import load_bostonimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport statsmodels.api as smfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lassofrom sklearn.feature_selection import RFE%matplotlib inline#Loading the datasetx = load_boston()df = pd.DataFrame(x.data, columns = x.feature_names)df[\"MEDV\"] = x.targetX = df.drop(\"MEDV\",1) #Feature Matrixy = df[\"MEDV\"] #Target Variable Filter Methods No mining algorithm included Uses the exact assessment criterion which includes distance, information, dependency, and consistency. The filter method uses the principal criteria of ranking technique and uses the rank ordering method for variable selection. Generally used as a dasta preprocessing step Several main filter methods based on the variable attributes: filter methods Wrapper Methods workflow: filter methods - Use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset Computationally expensive 3 Types: Forward Selection: An iterative method Start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model. Backward Elimination: An iterative method Start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features. E.g. If the p-value is above 0.05 then we remove the feature, else we keep it. 12345678910111213141516171819202122# Adding constant column of ones, mandatory for sm.OLS modelX_1 = sm.add_constant(X)# Fitting sm.OLS modelmodel = sm.OLS(y,X_1).fit()display(model.pvalues)# Backward Eliminationcols = list(X.columns)pmax = 1while (len(cols)&gt;0): p= [] X_1 = X[cols] X_1 = sm.add_constant(X_1) model = sm.OLS(y,X_1).fit() p = pd.Series(model.pvalues.values[1:],index = cols) pmax = max(p) feature_with_p_max = p.idxmax() if(pmax&gt;0.05): cols.remove(feature_with_p_max) else: breakselected_features_BE = colsprint(selected_features_BE) Recursive Feature elimination: A greedy optimization algorithm It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination 1234567891011model = LinearRegression()#Initializing RFE modelrfe = RFE(model, 7)#Transforming data using RFEX_rfe = rfe.fit_transform(X,y) #Fitting the data to modelmodel.fit(X_rfe,y)print(rfe.support_)print(rfe.ranking_)&gt;&gt;&gt; [False False False True True True False True True False True False True]&gt;&gt;&gt; [2 4 3 1 1 1 7 1 1 5 1 6 1] Here we took LinearRegression model with 7 features and RFE gave feature ranking as above, but the selection of number '7' was random. Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to 13. We then take the one for which the accuracy is highest. 1234567891011121314151617181920#no of featuresnof_list=np.arange(1,13) high_score=0#Variable to store the optimum featuresnof=0 score_list =[]for n in range(len(nof_list)): X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0) model = LinearRegression() rfe = RFE(model,nof_list[n]) X_train_rfe = rfe.fit_transform(X_train,y_train) X_test_rfe = rfe.transform(X_test) model.fit(X_train_rfe,y_train) score = model.score(X_test_rfe,y_test) score_list.append(score) if(score&gt;high_score): high_score = score nof = nof_list[n]print(\"Optimum number of features: %d\" %nof)print(\"Score with %d features: %f\" % (nof, high_score)) As seen from above code, the optimum number of features is 10. We now feed 10 as number of features to RFE and get the final set of features given by RFE method 1234567891011cols = list(X.columns)model = LinearRegression()#Initializing RFE modelrfe = RFE(model, 10) #Transforming data using RFEX_rfe = rfe.fit_transform(X,y) #Fitting the data to modelmodel.fit(X_rfe,y) temp = pd.Series(rfe.support_,index = cols)selected_features_rfe = temp[temp==True].indexprint(selected_features_rfe) (*) Bidirectional Elimination: A combination of Forward Selection &amp; Backword Elimination Self-defined Methods There are many interesting methods that can be directly applied in experimentations. However, one method that caught my eyes is the Boruta method: Boruta Method (Using shadow features and random forest) The main reason I liked this is because its application on Random Forest and XGBoost models. It generally works well with well structured data and relatively smaller datasets. In the hindsight, it is still relatively slower as compared to some simpler selection criterion, and it does not handle multicollinearity immediately. checkout this python tutorial for more details Embedded Methods It combines the qualities of filter and wrapper methods. It's implemented by algorithms that have their own built-in feature selection methods Workflow Embedded Method Workflow Here in the demo code we will do feature selection using Lasso regularization. If the feature is irrelevant, lasso penalizes it's coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken. 1234567891011121314reg = LassoCV()reg.fit(X, y)print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)print(\"Best score using built-in LassoCV: %f\" % reg.score(X,y))&gt;&gt;&gt; Best alpha using built-in LassoCV: 0.724820&gt;&gt;&gt; Best score using built-in LassoCV: 0.702444coef = pd.Series(reg.coef_, index = X.columns)print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" + str(sum(coef == 0)) + \" variables\")&gt;&gt;&gt; Lasso picked 10 variables and eliminated the other 3 variablesimp_coef = coef.sort_values()import matplotlibmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)imp_coef.plot(kind = \"barh\")plt.title(\"Feature importance using Lasso Model\") Filter vs Wrapper Now let us make a comparison between filter methods and wrapper methods, the two most commonly used ways in feature selection. Characteristics Filter Method Wrapper Methods Measure of feature relevance correlation with dependent variable actually training a model on a subset of feature Speed Much faster Slower due to model training Performance Evaluation statistical methods for evaluation Model results cross validation Quality of feature set selected May be suboptimal Guaranteed to output optimal/near-optimal feature set Overfitting ? Less likely Much more prone to Model SelectionHere we must clarify one important conceptual misunderstanding: Note: Classical Model selection mainly focuses on performing metrics evaluations through different models, tuning the model parameter and variating the training datasets. The choice of model in the end is often manual. Hence, it differs from the automated model selection procedure where the final selection of model is also done automatically. The latter is often known as AutoML, and has gained quick wide popularity in recent years. We now think about what are the main strategies to improve model performance: Use a more complicated/more flexible model Use a less complicated/less flexible model Tuning hyperparameters Gather more training samples Gather more data to add features to each sampleClearly, the first 4 are model selection strategies, and the last one is feature selection. When we make these adjustments, we must keep in mind the The Bias-variance trade-off: bias: Usually the case where the model underfits, i.e. it does not have enough model flexibility to suitably account for all the features in the data variance: Usually the case where the model overfits, i.e. so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution For high-bias models, the performance of the model on the validation set is similar to the performance on the training set. For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set. We can easily visualize this via the learning curve Plot 1: The curve to find the best amount of train set size (too low –&gt; high variance; too high –&gt; high bias) In the meantime, we observe from the validation curve below that model complexity/hyperparameter choices affect the model performances as well Plot 2: The curve to find the best hyperparameters For more details on metrics evaluation and hyperparameter tuning with feedback from validation sets, interested readers can read my blogs on these topics as well.","link":"/post/blogs/mlops/feature-and-model-selections/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it’s the task of minimizing the cost/loss function parameterized by the model’s parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don’t have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it’s negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step. Now let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent’s variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/blogs/mlops/gradient-descent/"},{"title":"Hyperparameter Tuning","text":"OverviewHyperparameter tuning is a large field of study, just like any subjects under the topic of machine learning. In fact, I really need to thank this topic for bringing me into the field of Bayesian Optimization and Bandit, as well as the future sequential decision-making models I researched on. In this blog, I'll present some classical and popular methods for hyperparameter tuning. Before that, let us make crystal clear what is hyperparameter and why is it so important. Hyperparameter vs parameter Parameters: The variables inside the model that are gradually updated and estimated via data training. For example, the coefficients in a regression model, or the weights of a deep neural network. hyperparameters: The input variables to a model that stay the same during the training. They are often used in the algorithm to actually help estimate the model parameters. For example, the error rate specified in a statistical learning model like Support Vector Machine or the learning rate in deep neural networks. From the description above, we see that we should choose our hyperparameters wisely so that the parameters which helps our model stand out from the rest models using the same algorithm. Unfortunately, these inputs to the models are often in a continuous space, and we will rarely be able to explore every possible value. That's why numerous tuning methods were devised to help us obtain good hyperparameters that improve our models' performances. Automated hyperparameter tuningAlthough the job can be via manual selection. This is a very tedious process. Instead, many algorithms surfaced to help us overcome this difficulty. 1. Random SearchIn each iteration, we choose from the input space (for hyperparameters) random combination of hyperparameters to run the model. After our budget for iterations runs out, we then compare the performance of model with each combination and select the best one. 2. Grid SearchAs compared to random search, we choose from the input space a set of hyperparameters combinations evenly (sometimes with additional greedy exploration). We then choose from the observed models the best one. This avoid unintential negligence of certain regions in the input space. Unfortunately, the two methods above require the “boundedness” assumption for the input domain. The following methods allow for a general open set for the input spaces. 3. Bayesian OptimizationIn general, a sequential design strategy for global extrema computation of black-box functions that does not assume any functional form. It can be applied in hyperparameter optimization as well. Strategy Description: Prior: a function that is applied on existing observations. The prior captures the bellief about the behaviour of the function. Posterior: the distribution function over the objective function after the priors are evaluated on the observations. Acquisition function: the functions to determine the next query point based on the optimization objective Methods to define the prior/posterior distributions: Gaussian Process Finding the values of p(y|x) where y is the function to be minimized (e.g., validation loss) and x is the value of hyperparameter More expensive Usually executed with few observations Assume Gaussian distribution initially Generate new points (expected value and variance) with in the support Tree of Parzen Estimator Less expensive construct 2 distributions for 'high' and 'low' points, and then finds the location tht maximizes the expected improvement of 'low' points models P(x|y) and P(y) The drawback is the lack of interaction between hyperparameters. Acquisition Functions: Mainly trade-off exploitation and exploration so as to minimize the number of function queries Exploitation means sampling where the surrogate model predicts a high objective Exploration means sampling at locations where the prediction uncertainty is high. Several types of functions: Probability of improvement Expected improvement Upper Confidience Bound Thompson Sampling Entropy Search Methods Optimization of functions: Mainly through discretization using Newton's Method such as lbfgs In general, BO is widely applied in hyperparameter optimization, and is often ideal when the function evaluation is very costly, as its convergence rate is often much better as compared to other methods. 4. HyperbandHyperband is a variation of random search, but with the decision-making models from bandit algorithms to help find the best time allocation for each of the configurations. The method is theoretically sound, and has great variants ASHA (Asynchronous Hyperband) and BOHB (Bayesian Optimization with Hyperband) This also aroused my interests in bandit problems. You may read the research paper here. Genetic Algorithm1. Definition A search heuristic that is inspired by Charles Darwin’s theory of natural evolution. This algorithm reflects the process of natural selection where the fittest individuals are selected for reproduction in order to produce offspring of the next generation. 5 phases: Genetic Algorithm Procedure Initial population stochastic process: the individuals' genes are usually initialized at random. Fitness function Determines the ability (Fitness score) of an individual to compete with other individuals) Selection Error: A significant selection error means low fitness. Those individuals with greater fitness have a higher probability of being selected for recombination. Rank-based fitness assignment Most used method for fitness assignment Formula: Here is a constant called selective pressure, and its value is fixed between 1 and 2. Higher selective pressure values make the fittest individuals have more probability of recombination The is the rank of individual Selection Selection operator selects the individuals according to their fitness level The number of selected individuals is 2, being the population size. Roulette wheel Most used selection method A stochastic sampling with replacement The roulette is turned, and the individuals are selected at random. The corresponding individual is selected for recombination Crossover Every time, picks two individuals at random and combines their features to get four offsprings for the new population until the new population has the same size as the old one. The crossover operator recombines the selected individuals to generate a new population (dropping the selected individuals, the output population size remains constant.) illustration: Crossover Illustration Mutation The crossover operator can generate offsprings that are very similar to the parents. This might cause a new generation with low diversity. The mutation operator solves this problem by changing the value of some features in the offsprings at random. To decide if a feature is mutated, we generate a random number between 0 and 1. If this number is lower than a value called the mutation rate, that variable is flipped. The mutation rate is usually chosen to be 1/m, where m is the number of features. With that value, we mutate one feature of each individual (statistically). 2. Pros &amp; ConsPros Genetic algorithms can manage data sets with many features. They don't need specific knowledge about the problem under study. These algorithms can be easily parallelized in computer clusters. Cons Genetic Algorithms might be costly in computational terms since the evaluation of each individual requires the training of a model. These algorithms can take a long time to converge since they have a stochastic nature. 3. Application If the space to be searched is not so well understood and relatively unstructured (e.g. non-convex, undifferentiable, etc), and if an effective GA representation of that space can be developed, GA is good for usage They’re best for problems where there is a clear way to evaluate fitness. If the base algorithm's computation is expensive, it is not advisable to use this method It is rather rare to use, if you want, checkout this notebook using deap as the package for GA Some tools to use1. Scikit learn Random Search Grid Search 2. HyperOpt Random Search Tree of Parzen Estimators (TPE) Adaptive TPE 3. Optuna Most BO algorithms contained Pruning feature which automatically stops the unpromising trails in the early stages of training 4. Ray Tune ASHA, BOHB Distributed asynchronous automatically Very Scalable Supports Tensorboard and MLflow. Supports a variety of frameworks such sklearn, xgboost, Tensorflow, pytorch, etc. ConclusionFor engineers, it is really matter of choices based on the nature of your code/project. However, for researchers, what optimization strategy you choose could directly affect the theoretical performance of the algorithm. Hence it is worth reading more into the topic of Bayesian Optimization and Sequential decision-making problems. I will also update my posts on BO/Bandit later.","link":"/post/blogs/mlops/hyperparam-tuning/"},{"title":"MLOps Post Training Considerations","text":"IntroductionSeveral years before, since I started working on ML projects, I've had traumatizing experiences conducting post-training operations. So many years later, I still found the mistakes and lessons I learnt from those projects valuable. Therefore, I've decided to share my thoughts here, for my own references (remember the do's and don'ts!!!) and for anyone interested to take their piece of gem home. So what should be done after model training? Many university-level courses touched the surface of the topic, but in essence, it contains the handling of your model artifacts and training results, applying appropriate logging/monitoring and deployment to a server for actual usage. A great workflow chart I learned from Jeremy Jordan has demonstrated it well. Credit: Jeremy Jordan As you can see from the workflow: Experiment tracking is performed immediately after model training to persist the experiment results. Finetuned models are saved to a model registry (a form of database for model) to allow easy retrieval of this model Once we want to perform inference using this model, we set up a server which interacts with the registry and payload processed from backend to produce inference results A event stream (online or offline) service is need to enable drift detection and manage data flows. It should interact with inference server and your feature store actively, on a event-driven or scheduled manner Additional metric monitoring and backend logging are needed for issue recovery, model performance evaluation and reliability testing. In this blog, I'll focus on experiment tracking, model registry, serving and monitoring. Note that I would not go through every detail in one blog, as you and I will both get tired reading a lengthy blog. I'll put some relevant links at places for people intereted to explore further. In the future, I'll also come up with blogs discussing each component in full detail as well. So stay tuned for that if you like my style XD. Experiment TrackingEvery mature data scientist and ml engineer/researcher should appreciate the important of experiment tracking: Reproducibility: ML experiments often involve multiple dependencies and configurations. Tracking every experiment ensures reproducibility, allowing researchers to revisit and replicate results. Collaboration: In team environments, multiple researchers may contribute to a project. Clear experiment tracking facilitates collaboration by providing a shared understanding of the progress and results. Model Iteration: Continuous improvement is a core aspect of deep learning. Experiment tracking helps monitor model iterations, enabling researchers to identify what works and what doesn't. There are a few components in experiment tracking: Logging: Maintain a log for each experiment and save it in a unified place. Log your model hyperparameters, experiment metrics, and any other relevant information. This aids in easy retrieval and comparison. Version Control: this include tagging and versioning of your training data, your model, metadata and experiments. This significantly reduces the chance of redoing experiments which may result in significant waste of resources (Yes I did this once…) Metadata Annotation: Annotate experiments with metadata such as project name, researcher name, and experiment purpose. This contextual information proves invaluable in understanding the context of each experiment. Visualization: Use visualization tools to track and compare metrics across experiments. This aids in identifying trends, outliers, and areas for improvement. In many cases, inputs and outputs stored are visualized as well. Platforms to useNowadays, tracking experiments are much easier than 5, 10 years ago with the help of well-developed tracking tools. A few powerful ones I've used and would recommend are MLflow Weights &amp; Biases (WandB) Neptune.ai These tools have api's directly embedded in your training scripts. For example, for mlflow, all you need to do is run mlflow ui 12345678910111213141516import mlflowexperiment_name = \"YOUR_EXPERIMENT_NAME\"run_name = datetime.now().strftime(\"%Y%m%d-%H%M\")mlflow.set_tracking_url(\"http://localhost:5000\") # replace it if you have a dedicated tracking server urlmlflow.set_experiment(experiment_name=experiment_name)with mlflow.start_run(experiment_id=experiment_id, run_name=run_name): # Start your model training ... # logging metrics metrics = YOUR_METRICS_GETTER mlflow.log_metric(...) # logging artifact mlflow.log_artifact(...) and you will get a beautiful ui that looks like this: Tracking individual componentsTracking ErrorsTo enable logging errors, one can use the default python logger, and save the log history as text/artifacts into these platforms. Otherwise, tools like WandB also has its integrated logger that directs the std outputs into its db and saves the log conveniently. Tracking Hyperparameters Method 1: config yaml filein a yaml file 12345678910111213141516171819202122project: ORGANIZATION/home-creditname: home-credit-default-riskparameters: # Data preparation n_cv_splits: 5 validation_size: 0.2 stratified_cv: True shuffle: 1 # Random forest rf__n_estimators: 2000 rf__criterion: gini rf__max_features: 0.2 rf__max_depth: 40 rf__min_samples_split: 50 rf__min_samples_leaf: 20 rf__max_leaf_nodes: 60 rf__class_weight: balanced # Post Processing aggregation_method: rank_mean then in your python file you can do: 1234567import yamlwith open(config_path) as f: # note that you should choose loader wisely as some values (e.g. int) may be parsed wrongly if you use the wrong loader config = yaml.load(f, Loader=yaml.SafeLoader) # config is dictprint(config['parameters']['n_cv_splits']) # 5 Method 2: command line + argparseThe most common choice in ML community (and the most convoluted one) Method 3: Hydrausing the same yaml file, do the folloing 123456789import hydrafrom omegaconf import DictConfig@hydra.main(config_path='hydra-config.yaml')def train(cfg): print(cfg.pretty()) # this prints config in a reader friendly way print(cfg.parameters.rf__n_estimators) # this is how to access single value from the configif __name__ == \"__main__\": train() Tracking DataEnsure your data source is coherent and versioned consistently requires the use of data registry sometimes. One example is using DVC. You may refer to this AWS Tutorial on DVC usage to setup your data registry. Due to the versatile nature of data sources, we cannot always rely on a SQL DB (e.g. MySQL) or NoSQL DB (e.g MongoDB, InfluxDB) to manage our experiment training data, hence the usage of DVC. Tracking MetricsNormally metrics are easily to track directly with the tools I mentioned above. Very often they come with the set of visualizations as well. In case you need to produce your own visualizations, especially when evaluting the impact of model configurations on the metric values, libraries like optuna, hyperopt and scikit-optimize will have support for visualizations. Tracking Model Environment Solution 1: Docker images (preferred) Create a Dockerfile 1234567891011121314151617181920# Use a miniconda3 as base imageFROM continuumio/miniconda3# Installation of jupyterlabRUN pip install jupyterlab==0.35.6 &amp;&amp;pip install jupyterlab-server==0.2.0 &amp;&amp;conda install -c conda-forge nodejs# Installation of Neptune and enabling neptune extensionRUN pip install neptune &amp;&amp;pip install neptune-notebooks &amp;&amp;jupyter labextension install neptune-notebooks# Setting up Neptune API token as env variableARG NEPTUNE_API_TOKENENV NEPTUNE_API_TOKEN=$NEPTUNE_API_TOKEN# Adding current directory to containerADD . /mnt/workdirWORKDIR /mnt/workdir run docker build -t YOUR_TAG --build-arg NEPTUNE_API_TOKEN=$NEPTUNE_API_TOKEN . finally start the image by (example below) 1234567docker run -p 8888:8888 IMAGE_TAG:latest /opt/conda/bin/jupyter lab --allow-root --ip=0.0.0.0 --port=8888 Solution 2: conda Usually .yaml configuration file create conda environment by running: conda env create -f environment.yaml update yaml fle by conda env export &gt; environment.yaml Tracking ModelsModel needs to be versioned and the artifacts stored in model registry, which I'll talk about in the next section. Model RegistrySaving and versioning a model can be done at experiment tracking stage, or isolated out as a single stage in the workflow. This is because we usually have it as an output of the experiment runs, and it involves several more intricate details. A few aspects include: Different environment the registry (dev/stage/prod) Different forms of registering models (save/package/store) Dev vs Prod Registry Purpose: Dev Environment: where data scientists and machine learning engineers work on creating, experimenting, and refining rapid iterations multiple experiments exploratory models Prod Environment: where the finalized, stable, and optimized models are deployed to serve predictions in a real-world setting. Production models are expected to be reliable, scalable, and performant mainly consist of champion models and challenger models both should be well-tested and verified before saved in this registry Access and Permissions: Dev Environment: access might be more open to facilitate collaboration and experimentation. Data scientists/MLEs often have broader access to try out different ideas and approaches. Prod Environment: access to the model registry in the production environment is typically more restricted Authorized personnel, such as DevOps or IT administrators, should have the ability to deploy or update models in the production registry to maintain stability and security Less of a concern from data scientist perspective Model Versions: Dev Environment: usually a large number of model versions as different experiments and iterations are conducted. Prod Environment: have fewer, well-tested, and validated model versions. The focus is on deploying stable models that meet performance and reliability requirements. Logging and Monitoring: Dev Environment: more focused on tracking experiment results, understanding model behavior, and debugging Prod Environment: critical for tracking the performance of deployed models, identifying issues in real-time, and ensuring that the system meets service-level objectives. Security Considerations: Dev Environment: Security measures in the development environment may be more relaxed to enable faster experimentation Model is within internal control, less exposed to external uses. Major concern is actually training/testing data leakage Prod Environment: Strict security measures are implemented in the production environment to safeguard against unauthorized access, data breaches, or other security threats Prevent leakage of model information (metadata, weights, architectures, etc) as they are important company properties Save vs package vs store ML models Save Model Save params to disk Mainly for local operations on a single model(save/load model state dict, optim state dict, etc) Tools: Pickle, HDFS, JSON, etc Package Model Bundle model with additional resources (model file, dependencies, configuration files, etc) Enable easy distribute and deploy the ML model in a production environment Tools: Docker Image, Python Packages Store Model Mainly for centralized model storage (model artifact) To facilitate model sharing across team Tools: MLFlow Save1234567891011121314151617import torch# Load the model and optimizermodel = ...optimizer = ...# Save the model and optimizer states in a single filetorch.save( {'model': model.state_dict(), 'optimizer': optimizer.state_dict()}, 'saved_states.pt')# Load the saved states from the filesaved_states = torch.load('saved_states.pt')# Restore the model and optimizer states from the dictionarymodel.load_state_dict(saved_states['model'])optimizer.load_state_dict(saved_states['optimizer']) Package123456789101112131415161718192021222324# load dependenciesimport onnximport onnxruntime# Load your trained model (e.g., PyTorch or TensorFlow)model = ...# Convert the model to ONNX formatonnx_model = onnx.convert(model, 'my_model')# Save the ONNX model to a filewith open('my_model.onnx', 'wb') as f: f.write(onnx_model.SerializeToString())# Load the ONNX model from the fileonnx_model = onnx.load('my_model.onnx')# Create a new session for the modelsession = onnxruntime.InferenceSession(onnx_model)# Use the session to perform inference or deploymentinputs = ... # Get input data (e.g., images, audio samples, etc.)outputs = session.run(None, inputs) Store Either use Database to store model versions, or consider using a model registry Example of MLFlow 123456789101112131415import mlflow# Load your trained model (e.g., PyTorch or TensorFlow)model = ...# Create a new experiment for the model in the Model Registryexperiment_id =mlflow.get_experiment(experiment_name=\"MyModelExperiment\").experiment_id# Set up the artifact for the modelartifact_path = \"models/my_model\"artifact_uri = f\"runs:/{experiment_id}/{artifact_path}\"# Log the artifact to the Model Registrymlflow.log_artifact(model, artifact_uri) Model ServingModel serving is actually a fairly complicated process, as it involves deep understanding of backend engineering and data engineering, as well as more infrastructure management. There are several methods to serve/deploy a model: Online Offline Streaming Batch Serverless For the simplest part, all you'll need is to parse the data from the request, feed it as input (after feature engineering) into the model you loaded, and get the result. However, in order to make model serving efficient and powerful, many parts of the process need to be optimized: API architectureA few strong candidates to build the request/response formatting for data communication when deploying model are Remote Procedure Call (RPC), WebSocket, and RESTful APIs. Let's discuss the trade-offs associated with each approach: Remote Procedure Call (RPC): Pros: Efficiency: RPC protocols (e.g., gRPC) are known for their efficiency, making them suitable for scenarios where low-latency communication is crucial. Streaming: Some RPC frameworks support bidirectional streaming, allowing continuous communication between clients and servers, which can be beneficial for real-time updates. Cons: Complexity: Implementing and managing RPC services can be more complex than other approaches, especially when dealing with advanced features like bidirectional streaming. WebSocket: Pros: Low Latency: WebSockets provide low-latency, full-duplex communication, making them suitable for applications requiring real-time updates. Bidirectional Communication: WebSockets enable bidirectional communication, allowing the server to push updates to clients efficiently. Cons: Connection Management: Managing WebSocket connections can be more challenging than REST APIs, especially when dealing with issues like connection drops and reconnections. Standardization: WebSockets lack a standardized way to describe APIs compared to RESTful APIs. RESTful API: Pros: Simplicity: RESTful APIs are simple and easy to understand, making them accessible to a wide range of developers. Widespread Adoption: RESTful APIs are widely adopted and supported by a vast ecosystem of tools and libraries.Statelessness: RESTful APIs are inherently stateless, simplifying scalability and fault tolerance. Cons: Latency: For real-time applications, RESTful APIs might introduce higher latency due to the request-response nature of communication. Limited Push Mechanism: Traditional REST APIs lack a built-in mechanism for the server to push updates to clients in real-time. Online vs OfflineNotice that while some part of the inference may require online models, other parts of the system may be done offline. Take a recommendation system for example: The latest recommended feeds should be generated in real-time/online. However, some of the embedding/coarse search may be done offline in fixed duration. The results of these offline inferences are then used as embeddings/context for the refined search/ranking during the real-time inference stage. ETL-based DeploymentETL (Extract, Transform, Load) jobs copy and process data from a source to a destination, commonly used in data warehousing. In machine learning model deployment, ETL involves extracting features, predicting, and saving results. Unlike real-time systems, ETL jobs don't provide quick predictions but process many records at once, contrasting with web apps. You may use ETL structure for serving the model when your app is monolith and the pipeline is less convoluted. Tools like Apache Beam, MapReduce or Airflow are perfect tools for performing ETL-driven model serving. Model service as part of MicroservicesIf you are serving model on a larger distributed system with many microservices, you would likely need to consider inter-service communication, and push your inference task into an event queue if necessary. Tools like Kafka, RabbitMQ and Celery are designed to achieve this by ways of pub-sub or message brokers. Further discussion will cause it to divert into the field of backend system design, and I shall stop here for interested people to learn more on themselves. Serverless ArchitectureIf you don't want to setup a server yourself (managing the infra can be a huge pain) and would like an endpoint that is fault-tolerant, scalable and ready-to-use, many cloud service companies have it available. Examples include Azure ML, AWS Lambda and GCP AI Studio. You can find tutorials on respective tools to setup your inference endpoint with the model you've developed. Model MonitoringMetrics to monitorModel metrics Prediction distributions Feature distributions Evaluation metrics (when ground truth is available) System metrics Request throughput Error rate Request latencies Request body size Response body size Resource metrics CPU utilization Memory utilization Network data transfer Disk I/O Sample procedure for ML app monitoring Create a containerized REST service to expose the model via a prediction endpoint. Setup Instrumentor to collect metrics which are exposed via a separate metrics endpoint In the repo from JJ, it is instrumentator.instrument(app).expose(app, include_in_schema=False, should_gzip=True) After deploying our model service on the Kubernetes cluster, we can port forward to a pod running the server and check out the metrics endpoint running at 127.0.0.1:3000/metrics: kubectl port-forward service/wine-quality-model-service 3000:80 Deploy Prometheus (PULL-based mechanism) to collect and store metrics. Prometheus refers to endpoints containing metric data as targets which can be discovered either through service discovery or static configuration. Usually service discovery would be a good choice as it enable Prometheus to discover which targets should be scraped This can be simply done by configuring the endpoints to monitor in a separate service 12345---endpoints: - path: metrics port: app interval: 15s Deploy Grafana to visualize the collected metrics. Finally, we'll simulate production traffic using Locust so that we have some data to see in our dashboards. Some of the standard tests to include are: Make a request to our health check endpoint choose a random example from the dataset and make a request to prediction service choose a random example, corrupt the data, and make a bad request to our prediction service Other things to consider Drift Detection Service Strategies deploy a drift-detection service log a statistical profile log the full feature payload Best PracticesPrometheus Avoid storing high-cardinality data in labels. Every unique set of labels for is treated as a distinct time series, high-cardinality data in labels can drastically increase the amount of data being stored. As a general rule, try to keep the cardinality for a given metric (number of unique label-sets) under 10. Metric names should have a suffix describing the unit (e.g. http_request_duration_****seconds****) Use base units when recording values (e.g. seconds instead of milliseconds). Use standard Prometheus exporters when available. Grafana Ensure your dashboards are easily discoverable and consistent by design. Use template variables instead of hardcoding values or duplicating charts. Provide appropriate context next to important charts. Keep your dashboards in source control. Avoid duplicating dashboards. https://towardsdatascience.com/testing-features-with-pytest-82765a13a0e7 Last wordsWhile this blog covers a lot of topics and provide several examples in the processing of post-training tasks, they are far to complete. In order to truly master each component, I would recommend play around with each part for a few times and make mistakes. This was the path I took, but without any preliminary guide like this. Nonetheless, it was fruitful and really helped me understand the nitty gritty of post-training tasks. Hence, review this blog when you feel like setting up a new project, and make it part of the process!","link":"/post/blogs/mlops/ml-post-training/"},{"title":"DL Training: An in-depth discussion","text":"IntroductionTraining in the field of Deep Learning has been a well-studied component which contributed significantly to the development of large-scale models. As a pivotal part of model research, optimizing training of a model can lead of numerous benefits including cost saving, energy consumption reduction and expedition of effective model discovery. Personally I feel that it can be taught as an actual coursework due to the load of content it has and the fact that every DL researcher and engineer should master training thoroughly to be truly productive. Unfortunately, no school or even online coursework has considered this, partly because that the overhead cost of leveraging multi-gpu or gpu cluster can be high. Nonetheless, I'm optimistic that as we further scale the gpu resources and advent our cloud computing technology, my proposal will soon become feasible. But before that happens, let's have a simplified “course” on it in this blog post. AssumptionsThis blog assumes the mastery of basic training pipeline and will only focus on advanced training techniques. We would also assume that data is clean and preprocessed. So anything related to data quality, feature transformation, tokenization or data loader definitions will be out of the scope of this blog. Instead, we focus on a training of a single (usually large scale) model with a given set of hyperparameters. In the meantime, we also have a fixed loss/evaluation metric for a given task during training. To learn more about these components. Checkout my post on designing a large-scale deep learning system OptimizationOptimizersThe basic form of an optimizer is often basic gradient descent (GD) and its variants stochastic gradient descent and mini-batch gradient descent. They are often considered to be rooted from convex optimization, but adapt to nonconvex cases greatly. Both from a theoretical and practical aspects, I’d recommend spending several hours studying the first few lectures of Convex Optimization course from CMU to have a deep understanding of this canonical optimization technique. On the other hand, there are several key optimizers need to be further mentioned with their motivations, pros and cons and update formulas. I’ve listed them below for your convenient references, if you are interested in more details, please refer to a dedicated post about optimizer selections. AdaGrad (Adaptive Gradient Algorithm): Adaptive Learning Rates: AdaGrad adapts the learning rates for each parameter based on historical gradients. Parameters that receive large gradients get smaller learning rates, and vice versa. Advantages: Effective for sparse data. Automatic adjustment of learning rates. Limitations: The accumulation of squared gradients in the denominator can lead to a diminishing learning rate, making it less effective for non-convex problems. Update Rule: : Updated parameter at time (t+1). : Current parameter at time (t). $\\eta): Learning rate. : Sum of squared gradients up to time (t). : Gradient of the objective function (J) with respect to parameters at time (t). : Smoothing term to avoid division by zero. RMSProp (Root Mean Square Propagation): RMSProp addresses the diminishing learning rate problem in AdaGrad by introducing a decay term. It uses a moving average of squared gradients to adjust learning rates. Advantages: Improved adaptability to different types of data. Mitigates the diminishing learning rate problem. Limitations: Does not have built-in momentum, which can be a limitation in certain cases. Update Rule: : Exponential moving average of squared gradients up to time (t). Other symbols are the same as in AdaGrad. Adam (Adaptive Moment Estimation): Combination of Momentum and RMSProp: Adam combines the concepts of momentum and RMSProp. It maintains both a moving average of gradients and a moving average of squared gradients. Advantages: Effective in practice for a wide range of applications. Combines benefits of momentum and adaptive learning rates. Limitations: Sensitive to hyperparameter choices. Can exhibit noisy behavior on certain types of data. Update Rule: : Bias-corrected moving average of gradients. : Bias-corrected moving average of squared gradients. Other symbols are the same as in AdaGrad and RMSProp.Computation of and : : Exponential moving average of gradients. : Exponential moving average of squared gradients. and : Decay rates for the moving averages.Final Update Rule: : Small constant (to avoid division by zero). AdamW The inclusion of weight decay in AdamW allows for more effective regularization, and it is often preferred when regularization is a concern. Update Rule: : Weight decay coefficient. SchedulersVery often, one would also need to consider the scheduling of learning rate. The purpose is to prevent the model from converging too quickly and getting stuck in local minima. In the case of large-scale models, this is less of a problem. As studies have shown, the proximity of local minima to the global answer is generally good when model size is big enough. However, it has also been shown that warming up the gradients and rate scheduling will help stabilize training, leading to easier experiment monitoring as a consequence. Here are the overview of a few common schedulers. Inverse Square Root (ISR): this is one of the simplest and most common learning rate schedules, where the learning rate is set to 0 at the beginning of training and then increased by a fixed factor each epoch until reaching a maximum value, after which it remains constant for the rest of training. This helps prevent overfitting by gradually increasing the learning rate as the model becomes more efficient during early stages of training, while maintaining a steady pace throughout the remaining period. Exponential Decay (ED) - this is another simple and popular learning rate schedule that involves using an exponential function to adjust the learning rate over time. The learning rate is set to 0 at the beginning of training and then increased exponentially until reaching a maximum value, after which it remains constant for the rest of training. The allows for faster descent at early stage and more refined exploration at a later stage Stepwise Schedule - this is another simple and popular learning rate schedule that involves using discrete steps to adjust the learning rate over time. The learning rate is set to 0 at the beginning of training and then increased by a fixed factor each step until reaching a maximum value, after which it remains constant for the rest of training. Memory Reduction in TrainingAs DL model sizes grow bigger and bigger in recent years, our hardware (GPU, TPU) can hardly handle the amount of parameters in the model. Caching everything direclty in-memory is no longer a viable option, especially for individual researchers/engineers with limited support of GPU. Therefore, reducing memory consumption in training becomes extremely important. Using the following techniques, we can reduce the size of weights, biases and activations while maintaining accuracy, and hence enable smooth training without the heinous “Out-Of-Memory” error. Automatic Mixed Precision (AMP)When working with large datasets or deep models where computing gradients on all data points can become computationally expensive, using a higher precision (e.g., floating-point) during training may result in slower convergence times due to the increased computational cost. On the other hand, using fixed-point arithmetic may not be able to capture fine details of the model's behavior and lead to lower accuracy. To address this trade-off, automatic mixed precision algorithms select a precision level based on various factors such as memory usage, computation time, and accuracy requirements. In general, floating-point is used during training when more precision is needed (e.g., large weights or small gradients) while fixed-point arithmetic is used for inference when lower precision can result in faster performance (e.g., smaller models). True Low-Precision TrainingIn contrary to AMP, true low-precision training is needed when size of parameters are still too big. This is one of the last resorts to sacrifice accuracy for training completeness. In this process, all parameters and gradients are in lower precision and are converted back to higher precision after training. Reduce Batch SizeOne of the easiest strategies to adopt when OOM appears is to use a smaller batch size. By doing so, we have much less data saved in cache, as the matrix used to compute gradient is much smaller (a scale of O(N) where N is number of parameters). However, the risk of slower convergence, unstable training and lower accuracy is closely related to a small batch size as well. When mini-batch SGD is “too close” to SGD, the benefits brought multicore processing will also diminsh. So be careful when using it. Gradient AccumulationWhen reducing batch size leads to severely poorer performance, and increasing it causes infeasible training, we have a work-around, gradient accumulation, which virtually increase the batch size during training. This is easily achievable (see following code): 123456789101112131415161718for batch_idx, batch in enumerate(train_loader): model.train() # FORWARD AND BACK PROP outputs = model( batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"] ) outputs[\"loss\"] = outputs[\"loss\"] / accumulation_steps # accumulate loss fabric.backward(outputs[\"loss\"]) # optimizer accumulate the gradient derived from this loss ### UPDATE MODEL PARAMETERS if not batch_idx % accumulation_steps or (batch_idx + 1 == len(train_loader)): # update every accumulation_steps iterations optimizer.step() optimizer.zero_grad() Note that this method is at a cost of slower training due to the sequential update within the “larger” batch. Optimizer Sometimes, choose a stateless optimizer like SGD instead of Adam/AdamW will also help reduce memory consumption. Optimizer states can take up a significant amount of memory. For example, Adam optimizer requires first+second-order derivatives to be stored in the state for future updates. This is a additional storage term, as you can see from this image. On the other hand, SGD is simply stateless, and requires only an O(N) temporary storage of first-order gradients. Hence, you can consider switching from the commonly used Adam to SGD if it doens't result in significant performance drops. Distributed Training and Tensor ShardingPerhaps the most widely used strategy in industry when it comes to memory optimization is the use of sharding and distributed training. Companies have the resources and compute to perform model training on multiple GPUs and distribute the tensors across multiple servers. This is closely tied to the concept of parallelism. There are many forms of parallelisms in distributed training: Data Parallelism Model Parallelism Tensor Parallelism Pipeline Parallelism Each of them aims to fix some inefficiencies in the training pipeline. Many packages have been developed to implement these strategies for the benefits of DL researchers and engineers. I provide a more in-depth discussion in a different post, together with discussion about the common errors people face during distributing training. Gradient Checkpointing Gradient Checkpointing is an ingenious method to reduce memory usage by repeated discard . It comes from the observation that the most memory intensive part of training deep neural networks is computing the gradient of the loss by backpropagation. By checkpointing nodes in the computation graph defined by your model, and recomputing the parts of the graph in between those nodes during backpropagation, it is possible to calculate this gradient at reduced memory cost. THe selected checkpoint nodes in the computational graph are kept in memory after the forward pass, while the remaining nodes are recomputed at most once. After being recomputed, the non-checkpoint nodes are kept in memory until they are no longer required. It does slow down training, but the benefit is a reduction to square-root scale of memory. You may resort to PyTorch’s autograd library to easily craft a simple Checkpoint feature (note how the ctx has enable saving of function and args from forward method to be applied in backward later): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import torchimport torch.nn as nnimport torch.autograd as autogradclass CheckpointFunction(autograd.Function): @staticmethod def forward(ctx, func, *args): ctx.func = func ctx.args = args with torch.no_grad(): ctx.save_for_backward(*args) result = func(*args) return result @staticmethod def backward(ctx, grad_output): args = ctx.saved_tensors func = ctx.func f_args = tuple(args) f_args += (grad_output,) with torch.enable_grad(): grad_input = torch.autograd.grad(func(*f_args), f_args, allow_unused=True) return (None,) + grad_input[1:]class CheckpointModule(nn.Module): def __init__(self, module): super(CheckpointModule, self).__init__() self.module = module def forward(self, *args): return CheckpointFunction.apply(self.module, *args)# Define a simple feed-forward networkclass SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(1000, 500) self.fc2 = nn.Linear(500, 200) self.fc3 = nn.Linear(200, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x# Wrap the network with checkpointingcheckpointed_net = CheckpointModule(SimpleNet())# Example usageinput_data = torch.randn(1, 1000, requires_grad=True)output = checkpointed_net(input_data)loss = output.sum()loss.backward()print(\"Gradients w.r.t input_data:\", input_data.grad) A simple example using Hugging Face's transformer library looks like this: 12345678trainer = Trainer( ... args=TrainingArguments( ... gradient_checkpointing=True, ), ...) (For those interested) A more rigorous version with analysis Usually a hook-based non-reentrant is used Instead of saving the “large” tensor, we only store its (lightweight) index in the graph, and use that index to reconstruct it later When the non-reentrant activation checkpoint is called, the function’s forward pass is run in a CheckpointHook context manager. Under this context manager, any tensor packed and saved for the backward pass is discarded and replaced with a placeholder (here we arbitrarily use its index i). During the backward pass, the first backward function that tries to access and unpack its saved tensors, triggers the forward function to be recomputed under a RecomputationHook, which intercepts any tensors saved to store them in the recomputed list (detached from the computational graph to avoid reference cycles). It is important to note that the whole mechanism relies on the recomputed tensors being accessed in the same order in both forward and backward. To make sure that is the case, the real implementation also contains the code to save/restore the global state (e.g. preserving RNG states, which is important to ensure that modules such as Dropout produce the same output in both calls to run_function). A high level construct of the non-reentrant version is as follows 12345678910111213141516171819202122232425262728293031323334353637class Frame: # a struct for shared variables def __init__(self): self.recomputed = [] self.count = 0class RecomputationHook(torch.autograd.graph.saved_tensors_hooks): def __init__(self, frame): def pack(x): frame.recomputed.append(x.detach()) return x.detach() def unpack(X): # is only relevant for more complex scenarios return x super().__init__(pack, unpack)class CheckpointHook(torch.autograd.graph.saved_tensors_hooks): def __init__(self, frame, run_function, args): def pack(unused_x): i = frame.count frame.count += 1 return i def unpack(i): if not frame.recomputed: # only once, while unpacking the first tensor during backward with RecomputationHook(frame), torch.autograd.enable_grad(): run_function(*args) res = frame.recomputed[i] frame.recomputed[i] = None return res super().__init__(pack, unpack) def checkpoint_without_reentrant(run_function, *args): with CheckpointHook(Frame(), run_function, args): res = run_function(*args) return res Parameter offloadingWhile the model is training via gradient updates, it is usually a subsection of the parameters in the model that gets updated every time. This leads to idleness of GPU memory, which simply stores the result of the unused parameters before their turn of update. To further utilize this segment of the memory, people found a way to offload some parameters from GPU to CPU, and only reload it for update/computation whenever necessary. While this definitely increases communication cost and slows training down, it reduces memory usage by a large scale. In some other scenarios (e.g. ZeRO-3), optimizers states are also offloaded to save memory, which further reduces memory consumption. Flash AttentionTo improve the performance of Transformer-based models, flash attention was introduced by researchers at Google and Stanford University as an alternative to traditional self-attention mechanisms, which can be computationally expensive and prone to vanishing gradients. In essence, Flash Attention works by using a small window size (e.g., 512) to capture local context within the input sequence. The model then applies two separate attention mechanisms: one that uses full-length self-attention to capture long-range dependencies, and another that uses Flash Attention to focus on shorter-range patterns. The idea is that by using a smaller window size for the latter mechanism, the model can achieve better performance while still capturing important local features of the input sequence. This allows for more efficient computation and improved performance in downstream tasks like text classification or machine translation. Model DistillationSometimes, it is just easier to reduce the model size when memory constraint is hit. This had long been a success aside from techiniques such as model pruning from the invention of Jeffery Hinton. Model Distillation, or knowledge distillation, transfers knowledge from a larger and more complex model to a smaller and simpler one. Intuitively, the larger model is trained on a given dataset. Then it will act as a “teacher” to generate output representations that can be interpreted by the smaller model. These output representations are often in the form of probability distributions over the original input data, which allows for easier interpretation and use in downstream tasks. On the other hand, the smaller “student” model will try to predict outputs based on these probability distributions. Distillation is an enormous field of study in DL, and people often use existing libraries and distilled models made available by thousands of generous researchers who willingly release their trained model weights to the public. QuantizationAnother way to directly reduce model size is via quantization. Quantization typically involves mapping each value in the input to a range of possible output values, using techniques like thresholding or rounding. This is done by defining a set of bins that cover the entire range of possible outputs for a given input, and then assigning each output to its corresponding bin based on some threshold value. During training, the weights and activations are typically quantized before being stored on disk or transmitted over networks in order to reduce their size without sacrificing too much accuracy. Additionally, the gradients of the loss function can be computed using an approximate gradient method called stochastic gradient descent with quantization(SGD-Q), which allows for faster convergence and improved performance compared to standard SGD methods. Do note that this method is slightly different from the low-precision or mixed precision strategy, as the former often recovers the precision after training, but this method allows the model trained to stay in the same precision during inference, which effecitvely reduced persistent model storage requirement as well. To learn more about quantization, checkout this blog A sample code implementation looks like this: 1234567891011121314151617from transformers import AutoModelForCausalLM, BitsAndBytesConfigfrom peft import prepare_model_for_kbit_training, get_peft_modelmodel_name = 'gpt2-large'quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16,)# load modelmodel = AutoModelForCausalLM.from_pretrained( model_name, device_map=\"auto\", quantization_config=quantization_config, trust_remote_code=True,) Note that if you are using peft (e.g. for LoRA), you should also consider a quantized version for it using prepare_model_for_kbit_training: 1234...from peft import prepare_model_for_kbit_training# preprocess the quantized model for traininngmodel = prepare_model_for_kbit_training(model) Using PyTorch Lightning for simplified trainingHere we use lightning as an example to show how to quickly scale up training using the points mentioned above. Lightning Module ModelCheckpoint, CSVLogger Automatic Mixed Precision Training 123456789101112131415161718192021import Lightning as Lclass LightningModel(L.LightningModule): ...model = SomeModelLoader.from_pretrained(...)lightning_model = LightningModel(model)callbacks = [ ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\") # save top 1 model]logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")trainer = L.Trainer( max_epochs=3, callbacks=callbacks, accelerator=\"gpu\", precision=\"16\", # &lt;-- NEW devices=[1], logger=logger, log_every_n_steps=10, deterministic=True,) Static Graphs with Torch.Compile New feature in Torch 2.0 speed up PyTorch code execution by generating optimized static graphs instead of running PyTorch code with dynamic graphs Under the hood, this is a 3-step process including graph acquisition, graph lowering, and graph compilation. see official explanation May not speed up necessarily (use it when dominated by model runtime rather than graph computation) -&gt; initial optimization compilation step takes a few minutes but eventually accelerates the model training Code may break down in distributed settings 12345...model = SomeModelLoader.from_pretrained(...)model = torch.compile(model)lightning_model = LightningModel(model)... DDP (model + data parallel -&gt; pipeline parallel) 1234567891011trainer = L.Trainer( max_epochs=3, callbacks=callbacks, accelerator=\"gpu\", devices=4, # &lt;-- NEW strategy=\"ddp\", # &lt;-- NEW precision=\"16\", logger=logger, log_every_n_steps=10, deterministic=True,) Sharding strategy=\"deepspeed_stage_2\" to replace strategy=\"ddp\" note that deepspeed_stage_2 refers to the stage two of Zero Redundancy Optimizer (ZeRO), which effectively achieves the tensor sharding for optimizer states and gradients. note that here stages mean different sharding strategies. Consider optimizer states/gradients/weights to be sharded and optimizer state to be offloaded to CPU (refer to Paged Optimizers if can't remember) if necessary. This corresponds to different stages of ZeRO method as well. Use Lightning Fabric module (faster than Trainer). You may look it up online for further references. Manage your training processAfter all these training optimizations, it would be wasteful not to save your experiment data. Therefore, post-training processing becomes extremely important. Here is a brief list of actions to take note of: Metrics Monitoring: keeping the experiment data in persistent storage like MongoDB or SQL DB via mature mlops platforms like MLflow, Weights and Biases or Kubeflow is a convenient and important step to further analyse progress in model training and debugging potential bottlenecks/errors. Performance Monitoring: finding inefficiencies in hardward utilization (idle processes, slow hardware communication, low memory usage) can help further speed up your training and saving you tons of money. Your choice of server communication tools become important in this sense as well, and monitoring that part can be very tricky to carry out if you don't have sufficient low-level knowledge. Logging: error logging is not an easy thing when it comes to model training. Problems often can only be found from a parameter/matrix level, and the fact that training is distributed accross servers make it even harder to achieve. Model Checkpointing: saving you model half way during the training is critical to achieve fault-tolerant training. PyTorch maintainers has developed a thorough set of tools like dcp, elastic, rendezvous and torchrun in collaboration with deepspeed to help achieve that. If you are really into completing the full training cycle and squeezing every last sip of the resources you've paid for, I would urge you to check out my other post to learn more about it. To conclude…Training a model involves significant amount of design decisions that vary based on model, domain and data. It requires many years of experiences and mistake-making to form the right intuitions for model training setups. That said, constantly practicing it using some personal projects is what I did, and what I would recommend every passionate ML researcher/engineer should do to keep themselves updated with the latest progress in DL training. It might not “make perfect”, but it certainly saves you and your company enormous time and money in the day-to-day interactions with every-bloating monsterous models. References and Further Readings https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention https://sebastianraschka.com/blog/2023/pytorch-faster.html How Activation Checkpointing enables scaling up training deep learning models","link":"/post/blogs/mlops/ml-training/"},{"title":"Neural Network Applied: Optimizer Selection","text":"BackgroundAs one starts to use Neural Networks in their data models, he will inevitably encounter code of form like this: One might be quickly puzzled by the 3 terms optimizer, adam and sparse_categorical_crossentropy here. The first 2 are part of this blog's focus, which is about the optimization strategy applied in a Neural Network execution, and the sparse_categorical_crossentropy is a loss function used to help with the optimization. To understand the relevance of optimizer, one must first understand how an NN is trained. During the training of an NN, the weights of each neuron keeps getting updated so that the loss can be minimized. However, randomly updating the weights is not really feasible as there are hundreds of thousands of weights. Hence our smart scientists came up with a backward propagation (BP) algorithm for updating the weights. One may learn more about BP here. Behind BP we now require the optimizer to facilitate the updating of weights in each iteration. Right below we discuss a few most commonly used optimizers: Gradient DescentGradient Descent is the most basic optimization strategy which is based on the first order derivative of a loss function. The first order derivative serves as a guide on the direction to modify the weight so as to minimize the loss function. We've discussed its variant in details in an earlier post. To refresh our memory and make this blog more coherent, let's quickly recap here. Analytic form: Characteristics of Gradient Descent include: It's used heavily in linear regression and classification algorithms. Easy computation and implementatoin (Pros) May trap at local minima (Cons) Weights are changed only after calculating gradient on the whole dataset. So, if the dataset is too large then the convergence may take very long time (Cons) Requires large memory to calculate gradient on the whole dataset (Cons) Stochastic Gradient DescentGradient Descent has the problem of calculate gradient on the whole dataset in each itearation for weight update. Here Stochastic Gradient Descent aims to resolve this issue by processing data in random batches. As the model parameters are frequently updated parameters have high variance and fluctuations in loss functions at different intensities. Analytic form: Characteristics of SGD include: The learning rate needs to be updated in each iteartion to aviod over-fitting Faster convergence rate and less memory used (Pros) High variance in model parameters. (Cons) May continue to run even when global minima is achieved. (Cons) To reduce the variance we further have the mini-batch Gradient Descent which divides the data into mutiple batches and updates the model parameters after every batch (vs 1 data entry per update in SGD). In general, Gradient Descent method has the challenge of Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge. Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate. May get trapped at local minima. MomentumMomentum was invented for reducing high variance in SGD and softens the convergence. It takes advantage of information from previous directions via a formula Analytic form: Characteristics of Momentum include: The momentum term is usually set to 0.9 or a similar value. Faster Convergence and smaller variance (pros) Less Oscilliation &amp; more smooth shifting of direction (pros) AdagradOften, the learning rate of the optimizer is a constant. However, one may expect the optimizer to explore faster at the start and slower at the end to quickly converge to an optimum. Hence the learning rate may subject to change as iteration goes. Adagrad aims to achieve such effect. If we use low learning rates for parameters associated with most frequently occurring features, and high learning rates for parameters associated with infrequent features. We can get a good model. Analytic form: where and Here is a smoothing term that avoids division by zero (usually on the order of 1e−8) Characteristics of Adagrad include: Learning rate changes for each training parameter. Don't need to manually tune the learning rate. (pros) Able to train and performs well on sparse data. (pros) Computationally expensive as a need to calculate the second order derivative. (cons) Learning rate is monotone decreasing as iteration increases. (cons) AdaDeltaIt is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size . In this optimizer, exponentially moving average is used rather than the sum of all the gradients. By the idea above, we reducing the window size of from to : Analytic form: Characteristics of AdaDelta include: Learning rate does not decay necessarily (pros) More computationally expensive as expectation is involved (cons) RMSPropThe RMSProp algorithm full form is called Root Mean Square Prop, which is an adaptive learning rate optimization algorithm proposed by Geoffery Hinton. RMSProp is another strategy that tries to resolve Adagrad's radically diminishing learning rates problem by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient. While Adagrad accumulates all previous gradient squares, RMSprop just calculates the corresponding average value, so it can eliminate the problem of quickly dropping of learning rate of the Adagrad. By the idea above, we replace the with an expectation formula: Analytic form: Conclusion for the dynamic learning rate optimizer: Good for sparse data Be careful of the diminishing speed of learning rate More expensive computationally in general AdamAdam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don't want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaGrad, Adam also keeps an exponentially decaying average of past gradients M(t). In summary, Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. Note that although the name momentum looks fancy, the terms we need to consider are just first and second order momentum, which are essentially mean and variance of the gradients. Afterwards, we can consider 2 terms and as follows Hence the formula is as follows: These 2 terms are used to approximate the first and second moments, that is: Although we have above, theorem suggests that we can use the observed and to approximate and directly. After bias correction, we derive the terms Here and have really good default values of 0.9 and 0.999 respectively. Finally the update formula is just","link":"/post/blogs/mlops/nn-optimizers/"},{"title":"Starting your AI&#x2F;ML Project: from research to engineering","text":"Welcome to the world of AI and ML engineering! While the realms of research and academia provide a solid foundation, transitioning your knowledge into practical applications demands a comprehensive understanding of the various components and considerations specific to the industry. In this guide, we'll navigate through crucial aspects you need to address before embarking on a real-world AI/ML project. From data handling to project management and system scalability, we'll delve into the high-level overview and major concerns that accompany the shift from research to production. High level overviewProject detailsWhen moving from research-based project to a real-world engineering project, the first thing is alwasy to make sure there's impact. The outcome should be beneficial to some stakeholders, or there's no real difference between your work at school and the carefully crafted results from the community's point of view, and your work will become a simple self-absorbed show. Hence consider the following steps before you even start the project: Goals: Clearly Outlining ObjectivesEstablishing clear goals is foundational to project success. Define the primary objectives that the project aims to achieve. Actions to take: Conduct stakeholder meetings to gather input. Clearly articulate short-term and long-term goals. Prioritize goals based on business impact. User Experience: Focusing on User-Centric DesignUser experience directly influences the success of a project. Ensure that the system is designed with the end user in mind. Actions to take: Conduct user research to understand needs. Develop user personas and journey maps. Prioritize features that enhance usability. Performance constraints: Defining LimitationsIdentify and understand any performance constraints that could impact the system’s functionality or responsiveness. Actions to take: Define performance benchmarks. Identify potential bottlenecks and limitations. Explore monitoring options for performance tracking. Evaluation: Establishing Metrics for SuccessEvaluation metrics provide a quantitative measure of project success. Clearly define the criteria for assessing performance. This can be deterministic, but very often requires human evaluation &amp; inputs to truly provide accurate judgement. Actions to take: Define key performance indicators (KPIs). Establish a framework for continuous evaluation. Incorporate feedback loops for improvement. Personalization: Tailoring ExperiencesPersonalization enhances user engagement. Determine whether personalization features are relevant to your project. Actions to take: Assess the feasibility of personalization. Implement personalization algorithms. Balance personalization with privacy considerations. Project Constraints: Identifying Limiting FactorsEvery project operates within constraints. Identify and understand the constraints that may impact project execution. This include cost, manpower, time, regulation, infrastructure and even locations. Carefully review potential constraints are of paramount importance as you grow the project in the long run, something you won't even plan ahead based on research, where the inherent constraints are already given and fixed. Key ComponentsIn general, it is beneficial to consider the following aspects when starting your project. Data Collection: Harvesting Information GoldThe process of collecting data sets the stage for the entire project. Defining clear objectives for data collection, identifying relevant sources, and implementing effective mechanisms for gathering diverse and representative data are essential. The quality and relevance of the collected data significantly impact the performance of machine learning models. Transformation: Refining Raw PotentialRaw data often requires preprocessing and transformation to be suitable for machine learning. This includes tasks such as normalization, encoding, and handling missing values. A robust data transformation pipeline ensures that the data is in a format that the algorithms can effectively learn from. Validation: Ensuring Reliability and GeneralizationData validation in Python is the process of ensuring that data is accurate, consistent, and reliable. It involves verifying that data meets predefined criteria and rules to maintain its quality. This critical step prevents erroneous or unreliable information from entering the system. Versioning: Tracking Data EvolutionMaintaining version control for datasets is as critical as it is for code. Data versioning enables reproducibility and traceability, ensuring that changes in the dataset can be tracked over time. This is particularly important when dealing with evolving data sources. Algorithm Complexity: Balancing Power and InterpretabilityChoosing the right level of algorithmic complexity is a delicate balance. While complex models can capture intricate patterns, they might be harder to interpret and may require more resources. Simpler models, on the other hand, might be more interpretable but may struggle with capturing complex relationships. Dev-to-Prod Setting: Bridging Development and DeploymentAlgorithms should seamlessly transition from development to production. The development-to-production setting involves ensuring that the model operates effectively in a real-world environment. This includes considerations for stricter quality control, deployment platforms, scalability, and integration with other components of the system. Quality Control: Upholding Model IntegrityImplementing quality control measures for algorithms involves assessing their performance against predefined metrics. Regular monitoring, validation, and updating of models are crucial for ensuring that they continue to make accurate predictions in dynamic environments. Iterations: The Path to Continuous ImprovementMachine learning models benefit from an iterative approach. Continuous refinement based on feedback from real-world usage helps enhance model performance. This involves revisiting data, retraining models, and deploying updated versions to reflect evolving patterns. Resource Requirements: Balancing Power and EfficiencyUnderstanding and managing the resource requirements of algorithms are vital for cost-effective and efficient operations. This includes considerations for computational power, memory usage, and scalability, especially when deploying models at scale. Cost: Optimizing Efficiency and ExpensesOptimizing costs related to algorithmic choices involves finding the right trade-off between performance and budget constraints. This is important when your algorithm depends on some external APIs (e.g. OpenAI GPT) This can include exploring cost-efficient algorithms, leveraging cloud services judiciously, and optimizing computational resources. Infrastructure Computational Resources: Powering Intelligent AlgorithmsChoosing the right computational resources is essential for executing complex algorithms efficiently. This includes selecting the appropriate hardware, such as Central Processing Units (CPUs) or Graphics Processing Units (GPUs), based on the nature of the computations involved. Specialized accelerators, like Tensor Processing Units (TPUs), can further enhance performance for specific tasks. Scalability: Adapting to Growing DemandsScalability is a critical aspect of infrastructure planning. An effective AI/ML system should be able to handle increasing workloads gracefully. Whether it's accommodating a growing user base, handling larger datasets, or managing more complex models, a scalable infrastructure ensures that your system remains responsive and performs optimally as demands evolve. Data Storage: Managing the Lifeblood of IntelligenceEfficient data storage is fundamental to an AI/ML project. The infrastructure must provide reliable and scalable storage solutions for the vast amounts of data that machine learning models rely on. This includes considerations for data retrieval speed, redundancy for fault tolerance, and mechanisms for secure and compliant data handling. Networking: Facilitating Seamless CommunicationIn a distributed computing environment, effective networking is crucial. The infrastructure should facilitate seamless communication between different components of the system, ensuring timely data transfer and model updates. Network architecture choices impact latency, bandwidth, and overall system responsiveness. Containerization and Orchestration: Ensuring Portability and ManagementContainerization technologies like Docker provide a standardized way to package applications and their dependencies. Container orchestration tools, such as Kubernetes, enable efficient deployment, scaling, and management of containerized applications. These technologies enhance portability, ease of deployment, and resource utilization within the infrastructure. Security: Safeguarding Intelligent AssetsSecurity is a paramount concern in any infrastructure design. Protecting sensitive data, ensuring secure communication, and implementing access controls are essential. Additionally, regular updates and patches should be applied to safeguard against potential vulnerabilities in both software and hardware components. Monitoring and Logging: Insights for OptimizationImplementing robust monitoring and logging mechanisms provides insights into the performance and health of the infrastructure. Tracking metrics, such as resource utilization, response times, and error rates, enables proactive identification of issues and optimization opportunities. Cost Optimization: Balancing Performance and BudgetOptimizing costs involves striking a balance between performance requirements and budget constraints. Leveraging cloud services, optimizing resource allocation, and employing cost-effective storage solutions contribute to an infrastructure that meets operational needs without unnecessary expenses. Interface Design: Intuitive and AccessibleThe design of the user interface should prioritize intuitiveness and accessibility. A well-designed interface enhances user experience, making it easy for users to interact with and benefit from the underlying machine learning capabilities. Clear visuals, logical workflows, and thoughtful information presentation contribute to a positive user experience. Interactivity: Enabling User-System CollaborationInteractivity in the interface allows users to actively engage with the machine learning system. This can include features such as real-time feedback, interactive visualizations, and responsive elements that facilitate a dynamic and collaborative interaction between users and the AI system. Integration: Cohesiveness in FunctionalitySeamless integration with existing systems and workflows is essential. The interface should complement the user's existing tools and processes, ensuring a cohesive experience. Compatibility with various devices and screen sizes enhances the versatility of the interface. Feedback Mechanisms: Informing and Guiding UsersIncorporating effective feedback mechanisms is crucial for user understanding and trust. The interface should provide clear feedback on user actions, system responses, and any relevant information. This transparency fosters user confidence in interacting with the machine learning system. Adaptability: Tailoring to User PreferencesAn adaptive interface considers user preferences and dynamically adjusts its presentation based on user behavior and feedback. Customizable features, personalized recommendations, and adaptive layouts contribute to an interface that aligns with diverse user needs. Hardware Processing Units: Balancing Speed and EfficiencyThe choice of processing units, such as CPUs, GPUs, or TPUs, has a significant impact on the speed and efficiency of machine learning operations. Selecting the appropriate processing units for specific tasks and optimizing their utilization contributes to overall system performance. Memory: Managing Data AccessEffective memory management is crucial for handling large datasets and complex models. Optimizing memory usage, considering data transfer speeds, and minimizing latency contribute to the efficient functioning of machine learning algorithms. Scalability: Meeting Growing DemandsScalability involves the ability of the hardware infrastructure to accommodate increased computational demands. Scalable hardware configurations ensure that the system can handle growing workloads, whether due to an expanding user base or increasing complexity in machine learning tasks. Reliability: Ensuring Continuous OperationReliable hardware is fundamental for the uninterrupted operation of machine learning systems. Redundancy, fault tolerance, and effective monitoring mechanisms contribute to the reliability of the hardware infrastructure, minimizing the risk of system failures. Energy Efficiency: Sustainable ComputingConsidering the environmental impact of machine learning operations is becoming increasingly important. Implementing energy-efficient hardware solutions contributes to sustainability, reducing both operational costs and the carbon footprint of AI/ML systems. Major concerns when shifting from research to productionTransitioning from research-oriented machine learning projects to production-ready systems involves navigating several critical concerns. These considerations play a pivotal role in ensuring the successful deployment and operation of machine learning models in real-world scenarios Objectives: Aligning with Business GoalsThe objectives of a research project may differ from the goals of a production system. Ensuring alignment with business objectives is crucial for delivering tangible value. Hence, you should clearly define and prioritize the business objectives that the machine learning model aims to achieve. Regularly reassess alignment to adapt to evolving business needs. Computational Priority: Efficiency in ProductionResearch models may prioritize accuracy over computational efficiency, leading to challenges in deployment where low-latency and resource efficiency are critical. You need to optimize models for efficient inference, considering factors such as model size, inference speed, and resource utilization. Strike a balance between accuracy and computational demands. Data: Ensuring Quality and AccessibilityResearch datasets may not fully represent the complexities of real-world production data, and ensuring data accessibility is essential for ongoing model performance. It is critical to curate high-quality, diverse datasets that closely reflect production scenarios. Implement robust data pipelines and monitoring to ensure data quality and availability. Fairness: Mitigating Bias and Ethical ConcernsBiases present in research data or models may lead to unfair outcomes in production, raising ethical concerns and potential negative impacts. To ensure fairness, prioritize fairness and ethical considerations in model development. Implement measures to detect and mitigate biases, and regularly evaluate model fairness. Interpretability: Enhancing Model ExplainabilityComplex research models may lack interpretability, making it challenging to explain predictions to stakeholders and ensure transparency. Integrate interpretability techniques into model development to enhance understanding will become super useful in production applications. Use methods such as feature importance analysis and model-agnostic interpretability tools like . Requirements for MLsysFinally let's talk about when should be considered to gauge the quality of a ML system, the backbone of your real-world project's outcome. Here are five critical requirements for ML systems: Scalability:The system's ability to handle an increasing amount of data, workload, or user requests while maintaining performance. Considerations: Evaluate the system's scalability under varying workloads and data volumes. Assess the capability to efficiently scale both training and inference processes. Consider distributed computing for parallel processing and efficient resource utilization. Maintainability:The ease with which the ML system can be managed, updated, and modified over time. Considerations: Implement modular and well-documented code to facilitate easy maintenance. Incorporate version control for both code and models. Establish a monitoring and logging system for tracking system health and performance. Regularly update dependencies and address technical debt. Adaptability:The ability of the ML system to adapt to changes in data distributions, user requirements, or environmental factors. Considerations: Design models that can be retrained or fine-tuned with new data. Implement continuous learning techniques for adapting to evolving patterns. Consider automated retraining pipelines based on changing data characteristics. Ensure flexibility in feature engineering and model configurations. Reliability:The consistency and accuracy of the ML system's predictions or outcomes over time. Considerations: Implement rigorous testing procedures to validate model performance. Establish robust error handling mechanisms to handle unexpected situations. Monitor and address issues related to data quality, outliers, and changing distributions. Consider implementing fallback strategies for critical applications. Traceability:The ability to trace and understand the decision-making process of the ML system, including the origin of data, model training, and inference. Considerations: Maintain comprehensive documentation of data sources, preprocessing steps, and model architectures. Implement model versioning to trace changes over time. Record and monitor model predictions, including explanations for interpretability. Establish an audit trail for regulatory compliance and accountability. These requirements collectively contribute to the overall quality and success of an ML system. Striking a balance between scalability, maintainability, adaptability, reliability, and traceability is essential for building robust, effective, and sustainable machine learning solutions in real-world projects. Regularly reassess and update these considerations to keep pace with evolving project needs and industry best practices. Closing…Each of these aspects can be further explored, and I will keep updating blog posts on these parts. In the meantime, checkout these blogs that provide more details about the Key Components listed above. How to design a deep learning system","link":"/post/blogs/mlops/project-considerations/"},{"title":"Quantization in Deep Learning","text":"QuantizationQuantization in deep learning refers to the process of reducing the precision of the weights and activations of a neural network, typically from 32-bit floating-point numbers to lower bit-width integers. This can significantly reduce the memory requirements and computational complexity of neural networks, making them more efficient for deployment on resource-constrained devices. In this blog, I'll outline several major strategies in quantization for deep learning, including the types of quantization, special data structures for quantization and techniques used. Strategies Learned Quantization Schemes Instead of using fixed quantization schemes, learned quantization allows the network to adaptively determine the quantization parameters during training. Benefits: Improved flexibility: The network can learn the most suitable quantization parameters. Better adaptation to data distribution: Learned quantization can adjust to the characteristics of the data. Challenges: Increased training complexity: Learning quantization parameters adds complexity to the training process. A popular use case: Dynamic (Range) Quantization Focuses on adjusting quantization parameters based on the observed dynamic range Available during training and inference int8: quantized_val = real_value* scaling_factor + zero-point, zero-point is the int8 value corresponding to the value 0 in the float32 realm, scaling_factor = 127 / max_val_in_tensor float32 values outside of the [a, b] range are clipped to the closest representable value activations are read and written to memory in floating point format Post Training Static Quantization key: Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers) and model output cannot be calibrated unless we run a few inference cycles. allows both arithmetic and int8 memory access feed batches of data through the network and compute distributions of different activations -&gt; observer task the distributions determine how the activations are quantized (e.g. equal distributed 256 values) at inference time Operator fusion: fuse multiple operations to a single operation, saving memory access and numerical acuracy per-channel quantization: independely quantize weights in each output channel (or other specific dimensions) in a conv/linear layer -&gt; higher accuracy but more memory usage Benefits: Simplicity: Can be applied to pre-existing models without the need for retraining. Quick deployment: Pre-trained models can be quantized for efficient deployment. Reduce latency a lot (don't have to convert to quantized values to float and convert back between operations) Challenges: Loss of accuracy: Post-training quantization may lead to a significant drop in model accuracy. Quantization Aware Training This strategy involves training the neural network with quantization in mind. During training, the model is exposed to the quantization effects, helping it adapt to lower precision. highest accuracy typically, as it considers during training methods that rectify inference quantization errors weights and activations are “fake quantized” during training (both forward and backward) quantization-aware loss encourages the model to minimize the difference between quantized values (e.g., 8-bit integers) and the original (unquantized) values. Fake quantization layer: layers that mimic quantization and dequantization operations. During training, gradients flow through the fake quantization layers. This means that the model learns how to deal with the discrete values introduced by quantization. GuidelineThe developer team of Tensorflow has created a suggested workflow for quantization as follows, which makes a lot of sense and should be considered as a viable chain of thoughts when applying quantization: 12345671. try default Dynamic2. 2.1 if (fast enough) go to 3 2.2 else: use static quantization3. 3.1. if (no evaluation or equivalently, is training): use quantization-aware training 3.2. else: done Mixed Precision TrainingMixed-precision quantization involves using different precision levels for different layers of the neural network. For example, weights may be quantized to 8 bits, while certain layers or activations may use higher precision. While some consider it as part of the quantization, I'd personally treat it as a separate concept as it takes up an important component in many of the large model trainings, which preserving the accuracy to a large extent using a master copy in higher precision. Major steps Intuition: FP32 -&gt; FP16 during forward pass (model now in FP16) and FP16 -&gt; FP32 during back-prop (to ensure accuracy) A master copy of the weights is stored in FP32. This is converted into FP16 during part of each training iteration (one forward pass, back-propagation and weight update). At the end of the iteration, the weight gradients are used to update the master weights during the optimizer step. For FP16, any number with magnitude smaller than will be equated to zero as it cannot be represented (this is the denormalized limit for FP16). Therefore, by completing the updates in FP32, these update values can be preserved. Loss Scaling: small gradient values can appear even before update -&gt; in range of fix: after model forward pass, loss is calculated, we then do a scaling to ensure gradient values are greater than weights in FP16 now -&gt; [[loss in FP32 -&gt; Scaled loss in FP32 -&gt; Scaled Gradients in FP16]] Afterwards, scaled gradient in FP32 -&gt; gradient in FP32 Loss function is FP32 operation to preserve error accuracy, hence the loss always in FP32 Note: large scaling factor is okay except when it causes overflow (remove scaler in this case): generally 500,000 to 10,000,000 automatic scaling: start with very large , if Inf or NaN present, decrease scale, skip update. If Inf or NaN missing for while, increase the scale A great illustration of the steps outlined above is here (source: Nvidia): In some cases, reduction from FP32 into FP16 can result in too significant accuracy losses. Hence researchers have explored alternative data structures like Bfloat16 aned NF4 to help resolve this issue. So in the final section of this blog, let me give a short overview of these data types to make the topic of quantization complete and sound. Sample CodeI'll provide a simple illustration of python code for each strategy using torch.quantization library 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import torchimport torchvision.transforms as transformsimport torchvision.models as modelsimport torch.quantization as quantization# Load a pre-trained modelmodel = models.resnet18(pretrained=True)model.eval()# Define example input dataexample_input = torch.rand(1, 3, 224, 224)# ------------------------------# Dynamic (Range) Quantization# ------------------------------quantized_model_dynamic = quantization.quantize_dynamic( model, # Original model {torch.nn.Linear}, # Specify layers to quantize dtype=torch.qint8 # Quantization data type)quantized_model_dynamic.eval()# ------------------------------# Post Training Static Quantization# ------------------------------quantized_model_static = quantization.quantize_qat( model, # Original model {torch.nn.Linear}, # Specify layers to quantize input_calibrations=[(example_input, 0)], # Calibration data dtype=torch.qint8 # Quantization data type)quantized_model_static.eval()# ------------------------------# Quantization Aware Training# ------------------------------# Define a dummy dataset loader for quantization-aware trainingdummy_data_loader = torch.utils.data.DataLoader(torch.randn(100, 3, 224, 224))# Enable quantization-aware trainingquantized_model_qat = quantization.QuantWrapper(model)quantized_model_qat.train()# Fine-tune the quantized modelfor epoch in range(5): for data, target in dummy_data_loader: quantized_model_qat.zero_grad() output = quantized_model_qat(data) loss = torch.nn.functional.cross_entropy(output, target) loss.backward() # Perform optimization steps ...# ------------------------------# Mixed Precision Training# ------------------------------# Convert the model to mixed precisionmixed_precision_model = quantization.quantize_jit( model, # Original model example_input, # Example input for calibration dtype=torch.float16 # Quantization data type)# Use mixed precision model for trainingmixed_precision_model.train() DL-specific Data structureBfloat16Bfloat16 (Brain Floating Point 16-bit) was developed by Google as part of its efforts to optimize deep learning performance.It plays a role in quantization by offering reduced precision suitable for deep learning applications. Its use in mixed-precision training and compatibility with specific hardware accelerators contribute to improved efficiency in terms of both computation and memory usage. Motivation: Ensure identical behavior for underflows, overflows, and NaNs -&gt; bfloat16 has the same exponent size as FP32. However, bfloat16 handles denormals differently from FP32: it flushes them to zero. Unlike FP16, which typically requires special handling via techniques such as loss scaling, BF16 comes close to being a drop-in replacement for FP32 when training and running deep neural networks. 4-bit NormalFloat (NF4)4-bit quantization is discussed in the popular paper QLoRA: Efficient Finetuning of Quantized LLMs. The aim is to reduce the memory usage of the model parameters by using lower precision types than full (float32) or half (bfloat16) precision. Normalization: standard normalize the weights Method: Find nearest value in evenly spaced [-1, 1] ranged values. e.g. [-1, -.5, 0, .5, 1] -&gt; 0.32 is quantized to .5 Double Quantization: - A method that quantizes the quantization constraints saving additional memory (approx. 3 GB for a 65B model). DQ symmetrical quantizes the 32-bit Floating Point (FP32) quantization constants of the first quantization into 8-bit Floating Point (FP8) quantized quantization constants.","link":"/post/blogs/mlops/quantization/"},{"title":"Model Validations and Performance Evaluators","text":"OverviewVery often, people came up with various models with completely different underlying logics. In order to do reasonable comparisons among them, we must perform proper evaluations of the model performances and validate these models' efficacy via some datasets. This is where cross-validation comes in and performance metrics become extremely important. In this blog, I will first discuss about various techniques applied in cross-validation, followed by some analysis of commonly used metrics/scores Cross validationWhen we try to train a model, we require a training set and a test set. Very often, we need to split the training set itself so that we can use part of it as validation set: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set. Unfortunately, this would be too wasteful of the dataset we have. To fully utilize our datasets, we need cross-validation, which can split our datasets in to folds, and we pick some folds as the choice for validation set. After several iterations ran with different choices, we take an average of the evaluation scores as the final performance score for the model. The above strategy is often known as k-fold CV. It is computationally expensive, but it makes good use of the entire training set. Cross Validation MethodsBased on the choice of folds we pick, we may end up with different CV methods. Below is a list of methods we can use and their intuitions: Fixed index partition K-Folds: partition the into k equal-sized subsets. choose 1 set from all subsets as the validation set. Run the model obtained only on this validation once. Repeated K-Folds: run K-Folds n times, producing different splits and validation sets in each repetition. Fixed validation set size Leave One Out: Choose only one entry as the validation set Leave P Out: Choose only entries as the validation set Fixed label ratio Stratified K-Folds: Each set contains approximately the same percentage of samples of each target class as the complete set. Random index partition Suffle &amp; Split K-Folds: If we randomly pick entries in to get the resultant k partitioned subsets, we are equivalently doing shuffling before the partition. All these data partition methods are very intuitive and can be found in sci-kit learn. Here I'll just give a demo of how code should be run for CV. 12345from sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import ShuffleSplitclf = svm.SVC(kernel='linear', C=1, random_state=42) # Some model herecv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0) # You can specify the partition method here scores = cross_val_score(clf, X, y, cv=cv) # clf - model; X - training set; y - test set Performance MetricsNow let's move on to the next section on the metrics applied to evaluate the machine learning algorithms. There are metrics used for different tasks: Supervised Classification Supervised Regression Unsupervised Clusetering In this blog post, we will consider mainly on classification models, as the metrics for the remaining two task will be directly applied in these tasks to improve their model performances. We will leave it to interested readers to read my posts on models for these two tasks to explore the related metrics. ClassificationNote: Sometimes we have multiclass and multilabel problems instead of binary classifications. The answer from this post gives clear definitions for both problems: Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. Multilabel classification assigns to each sample a set of target labels. This can be thought of as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these. In the following discussions, I'll use [m-c] and [m-l] as labels to denote if the metric is applicable to Multiclass classification and Multilabel classification respectively. 1. Accuracy Score [m-l]This is the ratio of number of correct predictions to the total number of input samples It works well only if there are equal number of samples belonging to each class. Otherwise, it may lead to over/under-estimation of the model's true performance due to imbalanced data. 2. Balanced accuracy score [m-c]To address the problem above, we can introduce this score, which averages over recall scores per class where each sample is weighted according to the inverse prevalence of its true class. This formulation ensures that the score won't be too high just because one class's accuracy is high 3. Top-k accuracy score [m-l] [m-c]A generalization of accuracy score: prediction is considered correct as long as the true label is associated with one of the k highest predicted scores. 4. Confusion Matrix [m-c]The matrix which describes the complete performance of the model. This gives a good intuition over how well the classification is done. We can look at a sample pic here.1 A sample confusion matrix Based on the counts of corrected labeled and wrongly labeled samples, we can compute the following terms (say with regard to label 'sentosa', YES = is 'sentosa', NO = not 'sentosa') True Positives (TP) : The cases in which we predicted YES and the actual output was also YES. True Negatives (TN) : The cases in which we predicted NO and the actual output was NO. False Positives (FP) : The cases in which we predicted YES and the actual output was NO. Equivalently Type-I error. False Negatives (FN) : The cases in which we predicted NO and the actual output was YES. Equivalently Type-II error. These four terms are crucial as they help to compute the following terms 1: Precision: , intuitively the ability of the classifier not to label as positive a sample that is negative. It focuses on Type-I error. Recall: , intuitively the ability of the classifier to find all the positive samples. It focuses on Type-II error. F1-score: , a balanecd value (harmonic mean) of the precision and recall. We can generalize it to F-beta score, which allows more variated balanced between precision and recall: 5. Jaccard Similarity [m-l] [m-c]The jaccard_score function computes the average of Jaccard similarity coefficients, also called the Jaccard index, between pairs of label sets. 6. Hinge Loss [m-c]It only considers prediction errors. It is widely used in maximal margin classifiers such as support vector machines.Suppose the label is in , output label is , true label is , then we have 7. Receiver operating characteristic (ROC) and Area Under Curve(AUC)ROC is a plot of True Positive Rate (TPR)/Recall against False Positive Rates (FPR): A sample image looks like this 1: The area under the ROC curve is know as AUC, and it equals the probability that a randomly chosen positive example ranks above (is deemed to have a higher probability of being positive than negative) a randomly chosen negative example. The greater the value, the better is the performance.Unfortunately, ROC curves aren't a good choice when your problem has a huge class imbalance.","link":"/post/blogs/mlops/validation/"},{"title":"Stats in ML: Dirichlet Distribution","text":"DirichletBefore university, I've never heard of Dirichlet distribution. It seemed to me like a loner in the family of statistics when I first saw it. But soon I discovered that this is a huge misunderstanding. Dirichlet is way too important, too usful in the field of machine learning and statistical learning theories to be ignored by any interested scholars. I know deep down in my heart that I must dedicate one whole blog for it to stress its significance. As we walk along the path, we shall see why it is so great (in the past, present and future). Let's begin with a general definition of it. Formal definitionThe Dirichlet distribution Dir() is a family of continuous multivariate probability distributions parameterized by a vector of positive reals. It is a multivariate generalisation of the Beta distribution. Dirichlet distributions are commonly used as prior distributions (e.g. for categorical and multinomial distributions) in Bayesian statistics. Conjugate Prior and its usage.In Bayesian probability theory, we have Posterior , Prior and Likelihood related via the Bayes' theorem: If the posterior distribution and the prior distribution are from the same probability distribution family, then the prior and posterior are called conjugate distributions, and the prior is the conjugate prior for the likelihood function . Note that in many algorithm, we want to find the value of that maximizes the posterior (Maximum a posteriori). If the prior is some weird distribution, we may not get an analytical form for the posterior. Consequently, more complicated optimization strategies like interior point method may need to be applied, which can be computationally expensive. If both the prior and posterior have the same algebraic form, applying bayes rule to find is much easier. ExpressionAnalogous to multinomial distribution to binomial distribution, Dirichlet is the multinomial version for the beta distribution. Dirichlet distribution is a family of continuous probability distribution for a discrete probability distribution for categories , where for and , denoted by parameters . Formally, we denote . The expression is then where is some constant normalizer. Think of as the probability density associated with θ which is used for a multinomial distribution, given that our Dirichlet distribution has parameter . Moments explicit expressions To see that Dirichlet distribution is the conjugate prior for multinomial distribution, consider prior , and likelihood , where is the sample/result representing success out of trials for each object . Therefore, . We can intrepret this as \"given prior Dirichlet distribution (with param ) of probability vector for a total of objects and an observation vector , the posterior belief of the is a new Dirichlet Distribution with param ()\". Note the key points here are: Distributino has 2 parameters: the scale (or concentration) , and the base measure . A Dirichlet with small concentration favors extreme distributions, but this prior belief is very weak and is easily overwritten by data. It shall be seen as a generalization of Beta: Beta is a distribution over binomials (in an interval ); Dirichlet is a distribution over Multinomials (in the so-called simplex ). If we want to marginalize the parameters out (often used in ML models for parameter optimization) we can use the following formula: If we want to make prediction via conditional pdf of new data given previous data, we can use the following formula instead: Side noteThe above section gives a comprehensive view of dirichlet distribution. However, a more widely applied technique is Dirichlet Process. It is similar to Gaussian Process, but uses Dirichlet as conjugate prior instead on problems with multinomial likelihood (e.g. Latent Dirichlet Allocation). We've discussed this idea in the topic modeling blog. Interested readers can go that that blog for details. References Dirichlet distribution Lei Mao's Blog","link":"/post/blogs/prob_and_stats/dirichlet/"},{"title":"An overview of Hidden markov model and its algorithms","text":"OverviewStochastic Process is a critical piece of knowledge in statistical learning. It's also an important piece in the increasing popular reinforcement learning field. I feel like I might apply its algorithms or do research work on it in the future. Hence, I create this blog to introduce an important concept, hidden markov model (HMM), and some useful algorithms and their intuitions for HMM. Markov ChainThe HMM is based on augmenting the Markov Chain(MC), which is a type of a random process about a set of states. The transition from one state to another depends on certain probabilities , which we define as transition probability. The nice property of an MC is that the transition probability of only depends on the previous state , i.e. . We call it the Markov Assumption. This property allows us to produce an each transition graph like this[1] [1]:https://web.stanford.edu/~jurafsky/slp3/A.pdf HMM DefinitionA Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don't normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. A hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. In HMM, we use observations to describe observed events with values denoted by , and states to describe hidden events with values denoted by . Note that Markov Assumption still holds. In addition, we have output independence . In HMM, we try to solve the following problems: Problem 1 (Likelihood): Given an HMM with transition probabilities and observation probabilities and an observation sequence , determine the likelihood . Problem 2 (Decoding): Given an observation sequence and an HMM , discover the best hidden state sequence . Problem 3 (Learning): Given an observation sequence and the set of states in the HMM, learn the HMM parameters and . Algorithms1. The Forward Algorithm - Likelihood solverThe forward algorithm is a dynamic programming method that computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single forward trellis, which computes the probability of being in state after seeing the first observations, given the parameteres , i.e. From above, we can quickly derive the result by: first initialize . Recrusively apply the above expression (1) for . Compute 2. The Viterbi Algorithm - Decoding solverLike forwarding algorithm, Viterbi is also DP that makes uses of a dynamic programming Viterbi trellis. The idea is to process the observation sequence left to right, filling out the trellis. Each cell of the trellis, , represents the probability that the HMM is in state after seeing the first observations and passing through the most probable state sequence , given the parameters . The value of each cell is computed by recursively taking the most probable path that could lead us to this cell. Here, a major difference from (1) is that we now take the most probable of the extensions of the paths that lead to the current cell. In addition to the max value , we shall also keep track of the solution This is called backpointers. We need this value because while the forward algorithm needs to produce an observation likelihood, the Viterbi algorithm must produce a probability and also the most likely state sequence. We compute this best state sequence by keeping track of the path of hidden states that led to each state, and Viterbi backtrace then at the end backtracing the best path to the beginning (the Viterbi backtrace). Finally, we can compute the optimal score and path We use a demo code to illustrate the process. The code for forwarding algorithm and Forward-Backward Algorithm can be implmented in a similar fashion. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import numpy as npimport pandas as pd# Define the problem setupobs_map = {'Cold':0, 'Hot':1}obs = np.array([1,1,0,1,0,0,1,0,1,1,0,0,0,1])inv_obs_map = dict((v,k) for k, v in obs_map.items())obs_seq = [inv_obs_map[v] for v in list(obs)]print(\"Simulated Observations:\\n\",pd.DataFrame(np.column_stack([obs, obs_seq]),columns=['Obs_code', 'Obs_seq']) )pi = [0.6,0.4] # initial probabilities vectorstates = ['Cold', 'Hot']hidden_states = ['Snow', 'Rain', 'Sunshine']pi = [0, 0.2, 0.8]state_space = pd.Series(pi, index=hidden_states, name='states')a_df = pd.DataFrame(columns=hidden_states, index=hidden_states)a_df.loc[hidden_states[0]] = [0.3, 0.3, 0.4]a_df.loc[hidden_states[1]] = [0.1, 0.45, 0.45]a_df.loc[hidden_states[2]] = [0.2, 0.3, 0.5]print(\"\\n HMM matrix:\\n\", a_df)a = a_df.valuesobservable_states = statesb_df = pd.DataFrame(columns=observable_states, index=hidden_states)b_df.loc[hidden_states[0]] = [1,0]b_df.loc[hidden_states[1]] = [0.8,0.2]b_df.loc[hidden_states[2]] = [0.3,0.7]print(\"\\n Observable layer matrix:\\n\",b_df)b = b_df.values# Apply the Viterbi Algorithm based on http://www.blackarbs.com/blog/introduction-hidden-markov-models-python-networkx-sklearn/2/9/2017def viterbi(pi, a, b, obs): nStates = np.shape(b)[0] T = np.shape(obs)[0] # init blank path path = path = np.zeros(T,dtype=int) # delta --&gt; highest probability of any path that reaches state i delta = np.zeros((nStates, T)) # phi --&gt; argmax by time step for each state phi = np.zeros((nStates, T)) # init delta and phi delta[:, 0] = pi * b[:, obs[0]] phi[:, 0] = 0 print('\\nStart Walk Forward\\n') # the forward algorithm extension for t in range(1, T): for s in range(nStates): delta[s, t] = np.max(delta[:, t-1] * a[:, s]) * b[s, obs[t]] phi[s, t] = np.argmax(delta[:, t-1] * a[:, s]) print('s={s} and t={t}: phi[{s}, {t}] = {phi}'.format(s=s, t=t, phi=phi[s, t])) # find optimal path print('-'*50) print('Start Backtrace\\n') path[T-1] = np.argmax(delta[:, T-1]) for t in range(T-2, -1, -1): path[t] = phi[path[t+1], [t+1]] print('path[{}] = {}'.format(t, path[t])) return path, delta, phi# Run the algopath, delta, phi = viterbi(pi, a, b, obs)state_map = {0:'Snow', 1:'Rain', 2:'Sunshine'}state_path = [state_map[v] for v in path]pd.DataFrame().assign(Observation=obs_seq).assign(Best_Path=state_path) 3. Forward-backward algorithm - Learning solverThe standard algorithm for HMM training is the forward-backward, or Baum-Welch Welch algorithm, a special case of the Expectation-Maximization (EM) algorithm. The algorithm will let us train both the transition probabilities and the emission probabilities of the HMM. EM is an iterative algorithm, computing an initial estimate for the probabilities, then using those estimates to computing a better estimate, and so on, iteratively improving the probabilities that it learns. For a real HMM, we only get observations, and cannot compute which observation is from which state directly from an observation sequence since we don't know which path of states was taken through the machine for a given input. What's more, we don't even know when is a hidden state present. The Baum-Welch algorithm solves this by iteratively estimating the number of times a state occurs. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities to derive better and better probabilities. And we're going to do this by computing the forward probability for an observation and then dividing that probability mass among all the different paths that contributed to this forward probability. First, we define backward probability , the probability of seeing the observations from time to the end, given that we are in state at time (and given the automaton ): We can actually think of it as a reverse of the forwarding probability , and the computation is just the \"reverse\" of that for , so now the computation of : First initialize . Recrusively apply . Compute Now, we start the actual work. To estimate the , we may adopt a simple maximum likelihood estimation How do we compute the expected number of transitions from state to state ? Here's the intuition. Assume we had some estimate of the probability that a given transition was taken at a particular point in time in the observation sequence. If we knew this probability for each particular time , we could sum over all times to estimate the total count for the transition . We can then compute probability of being in state at time and state at time , given the observation sequence and of course the model via bayes' rule: since we can easily compute we get Similarly, by considering expected number of times in state and observing symbol . Define the probability of being in state at time as , compute We are ready to compute . For the numerator, we sum for all time steps in which the observation is the symbol that we are interested in. For the denominator, we sum over all time steps . The result is the percentage of the times that we were in state and saw symbol (the notation means “sum over all for which the observation at time was Using (3), (4), we can apply EM algorithm easily, as demonstrated below: Initialize iterate until Convergence: E-step: $$\\xi_t(i, j) = \\frac{\\alpha_t(i) \\widehat{P}_{ij} \\widehat{P^H}j\\left(o{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\; \\forall t, i \\; \\text{and} j $$ M-step: This concludes the blog, thank you!","link":"/post/blogs/prob_and_stats/hidden-markov-models/"},{"title":"Variational Autoencoder (VAE)","text":"A short Intro to VAE1. BackgroundThere mainly 2 types of deep generative models: Generative Adversarial Network (GAN) Variational Autoencoder (VAE) We will discuss about VAE in this blog. In future blogs, we will venture into the details of GAN. 2. A basic intuitionA VAE is an autoencoder whose encodings distribution is regularised (via variational inferenece) during the training in order to ensure that its latent space has good properties allowing us to generate some new data. 3. Encoder &amp; Decoderencoder is an agent that transform older featurer representation to a new set of feature representation (usually a lower dimension) using selection or extraction and decoder is an agent producing the reverse process. The encoded representation can span a feature space of certain dimensionality. We call it latent space. Furthermore, we know that certain properties/information of original features may be lost if we encode them. So we categorize transformations as lossy or lossless transformation. we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. 4. AutoencoderAutoencoder is done by setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks. We usually use a Mean Square Error as the loss function for backpropagation. This is often compared with PCA. When the structure of encoder/decoder gets deeper and more non-linear, we observe that autoencoder can still proceed to a high dimensionality reduction while keeping reconstruction loss low. There are mainly 2 drawbacks of Autoencoder: the lack of interpretable and exploitable structures in the latent space (lack of regularity) Difficulty in reducing a large number of dimensions while keeping the major part of the data structure information in the reduced representations As a result of the above two drawbacks, we may generate meaningless data if we simply encode into a latent space and sample random points from it to decode (a generative model often requires this). This issue leads to the need of regularization for the latent's space distribution. This is the motivation for Variational Autoencoder. VAE in detailThe key idea for VAE is that, instead of encoding an input as a single point, we encode it as a distribution over the latent space. In essence, we don't have point-wise estimation, but an estimation of original inputs' distribution (hence Bayesian inference is here of significant help). Next the evalution of an error is based on a new sample drawn from the distribution estimator and compared with original sample. (In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. See why this is the case in mathematical details below) Because of regularization, we now have an additional term inside loss, which is the KL divergence. We see that the KL divergence between 2 Gaussians has a closed form and hence can be computed easily. 1. All the math down hereWe require two assumptions: a latent representation is sampled from the prior distribution ; the data is sampled from the conditional likelihood distribution Now we note here that the “probabilistic decoder” is naturally defined by , that describes the distribution of the decoded variable given the encoded one, whereas the “probabilistic encoder” is defined by , that describes the distribution of the encoded variable given the decoded one. These two expressions remind us easily of the Bayes Rule, which we have . We assume and . Since f is arbitrary, the evidence term is often an intractable integral. Hence we need Variational Inference (VI) to help us approximate directly via a easily tractable distribution (often a Gaussian distribution ). We define where are parametrized functions. With uncertainty in , we aim to obtain optimal as 2. Practical idea: Neural NetworkNow that we have an optimization problem which may be solved using NN, we still need to address a few issues. First, the entire space of is too large, so we need to constrain the optimisation domain and decide to express f, g and h as neural networks. In practice, g and h are not defined by two completely independent networks but share a part of their architecture and their weights (with dependent paramters) For simplicity of computation, we often require to be a multidimensional Gaussian distribution with diagonal covariance matrix. With this assumption, h(x) is simply the vector of the diagonal elements of the covariance matrix and has then the same size as g(x). On the other hand NN models , which represents the mean of assumed as a Gaussian with fixed covariance. Using these ideas, we can first sample and evaluate an followed by error evaluation and backpropagation. Note here that the sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called reparametrisation trick, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture. This is done by producing a concrete sample from . So we have a Monte-carlo estimation via sampling to replace the expectation term in the loss term (1). We then backpropagate error to obtain the final result. Code implementationTo Be Updated","link":"/post/blogs/prob_and_stats/variational-autoencoder/"},{"title":"Variational Inference","text":"Introduction1. Background of Bayesian methodsIn the field of machine learning, most would agree that frequentist approaches played a critical role in the development of early classical models. Nevertheless, we are witnessing the increasing significance of Bayesian methods in modern study of machine learning and data modelling. The simple-looking Bayes' rule has inspired a lot wonderful models in areas like topic modelling, representation learning and hyperparameter optimization. In these models, the latent variables are the focus of the study. By analysing several data on the observed variables , we hope to get some meaningful information (for example, a point estimate or an entire distribution) about these latent variables. 2. Problem with Bayesian methods: intractable integralWhile the rule looks easily understandable, the numerical computation is hard in reality. One major issue is the intractable integral we need to compute in order to get the , which is often called the \"model evidence\". This is often because the search space for is combinatorially too large, making the computation extremely expensive. A common approach to deal with this problem is to approximate the posterior probability directly. Some popular choices include Monte Carlo Sampling methods and variational inference. In this report, we will introduce the variational methods, which are perhaps the most widely used inference technique in machine learning. We will analyse a particularly famous technique in variational methods, mean-field variational inference. 3. Main idea of variational inferenceIn variational inference, we can avoid computing the intractable integral by magically modelling the posterior directly. The main trick here is to approximate the unknown distribution with some similar distribution q. Since we can choose the q to belong to a certain family of distribution (hence tractable), the problem is now transformed into an optimization problem about the parameters of . Understanding Variational Bayesian methodIn this section, we demonstrate the theory behind variational Bayesian methods. 1. Kullback-Leibler DivergenceAs mentioned above, variational inference needs a distribution to approximate the posterior distribution . Therefore we need to gauge how well a candidate approximates the posterior. A common measure is Kullback-Leibler Divergence (often called KL divergence). KL divergence is defined as Where means the expected value with respect to distribution . The formula can be interpreted as follows: if is low, the divergence is generally low. if is high and is high, the divergence is low. if is high and is low, the divergence is high, hence the approximation is not ideal. Take note of the following about use of KL divergence in Variational Bayes: KL divergence is not symmetric, it's easy to see from the formula that as the approximation distribution is usually different from the target distribution . In general, we focus on approximating some regions of as good as possible (Figure 1 (a)). It is not necessary for the to nicely approximate every part of As a result (usually called forward divergence) is not ideal. Because for some regions which we don't want to care, if , the KL divergence will be very large, forcing to take a different form even if it fits well with other regions of (refer to Figure 1(b)). On the other hand, (usually called reverse KL divergence) has the nice property that only regions where requires and to be similar. Consequently, reverse KL divergence is more commonly used in Variational Inference. 2. Evidence lower boundUsually we don't directly minimizing KL divergence to obtain a good approximated distribution. This is because computing divergence still depends on the posterior . The computation involves the \"evidence\" term which is expensive to compute, as shown in the formula below: The approximation using reverse KL divergence usually gives good empirical results, even though some regions of may be compromised Figure 1: Reverse KL vs Forward KL divergence: The left image has a better approximation on part of . (a) Reverse KL simulation (b) Forward KL simulation We can directly conclude by the fact that the term is than the log of evidence. We can also proof this result using Jensen's inequality as follows: By the definition of marginal probability, we have , take log on both side we have: The last 2 lines follow from Jensen's Inequality which states that for a convex function , we have This term is known as the Evidence Lower Bound, or Since log does not depend on , we can treat it as a constant from the perspective of optimizing . Hence, minimizing is now equivalent to maximizing General procedureIn general, a variational inference starts with a family of variational distribution (such as the mean-field family described below) as the candidate for . We can then use the manually chosen to compute the ELBO terms and . Afterwards, we optimize the parameters in to maximize the ELBO value using some optimization techniques (such as coordinate ascent and gradient methods). Mean Field Variational Family1. The \"Mean Field\" AssumptionsAs shown above, the particular variational distribution family we use to approximate the posterior is chosen by ourselves. A popular choice is called the mean-field variational family. This family of distribution assumes joint approximation distribution to be factorized over some partition of the latent variables. This implies mutual independence among the n fractions in the partition. In particular: we have where is factorized into . For simplicity, we assume that each fraction only contains 1 latent variable , it is often referred as \"naive mean field\". This family is nice to analyse because we can model each distribution with a tractable distribution based on the problem set-up. Do note that a limitation of this family is that we cannot easily capture the interdependence among the latent variables. 2. Derivation of optimal Now in order to derive the the optimal form of distribution for and thus the overall , we need to go back to the ELBO optimization with this mean-field family assumption. Recall the formula for ELBO (we use here as it is the convention): We express this formula in terms of as using functional integral (see appendix A): With this new expression, we can consider maximizing with respect to each of the . The optimal form of is the one which maximizes , that is: We take the derivative with respect to using Lagrange multipliers and set to 0 yields: where is a normalization constant that plays minimal role in the variable update. The funtional derivative of this expression actually requires some knowledge about calculus of variations, specifically Euler-Lagrange equation. 3. Variable update with Coordinate AscentFrom equation we found that . Therefore iterative optimization algorithms like Coordinate Ascent can be applied to update the latent variables to reach their optimal form. Note that all the 's are interdependent during the update, hence in each iteration, we need to update all the 's. As short description for the coordinate ascent in this setup will be: Compute values (if any) that can be directly obtained from data and constants Initialize a particular to an arbitrary value Update each variable with the step function Repeat step 3 until the convergence of ELBO A more detailed example of coordinate ascent will be shown in next section with the univariate gaussian distribution example. A point to take note that in general, we cannot guarantee the convexity of ELBO function. Hence, the convergence is usually to a local maximum. Example with Univariate GaussianWe demonstrate the mean-field variational inference with a simple case of observations from univariate Gaussian model. We first assume there are observations from a Gaussian distribution satisfying: Here is inverse of variance (hence one-to-one correspondence). From the derivation of we know we need to compute the log joint probability . We will first derive an explicit formula for it by expanding the join probability into conditional probability: where is a constant. Note that sometimes some latent variable has higher priority that others. The choice of this variable depends on the exact question in hand. 1. Compute independent and Next, we apply approximation via . By the mean-field assumption, we have . We proceed to find the optimal form of and : Compute the expression for : Note that here is a shortcut representation for , and all are constant terms not involved in the optimization update. From the expression above, it's easy to observe that follows a Gaussian distribution with , where: Compute the expression for A closer look at the result suggest that follows a Gaussian distribution with Gamma , where: 2. Variable update until ELBO convergence}Now that we have and , we only need to update their parameters: Using the updated and , we can then compute with Hence the coordinate ascent algorithm can be applied here: Compute and as they can be derived directly from the data and constants based on their formula Initialize to some random value Update with current values of and Update with current values of and Compute ELBO value with the variables &amp; updated with the parameters in step 1 - 4 Repeat the last 3 steps until ELBO value doesn't vary by much As a result of the algorithm, we obtain an approximation for the posterior distribution of and given observations . Extension and Further resultIn this section, we briefly outline some more theory and reflection about general variational Bayesian methods. Due to space limitations, we only provide a short discussion on each of these. Exponential family distributions in Varational InferenceA nice property of the exponential family distribution is the presence of conjugate priors in closed forms. This allows for less computationally intensive approaches when approximating posterior distributions (due to reasons like simpler optimization algorithm applicable and better analytical forms). Further more, Gharamani &amp; Beal even suggested in 2000 that if all the belong to the same exponential family, the update of latent variables in the optimization procedure can be exact. A great achievement in the field of variational inference is the generalized update formula for Exponentialfamily-conditional models. These models has conditional densities that are in exponential family. The nice property of exponential family leads to an amazing result that the optimal approximation form for posteriors are in the same exponential family as the conditional. This has benefits a lot of well-known models like Markov random field and Factorial Hidden Markov Model. Comparison to other Inference methodsThe ultimate results of variational inference are the approximation for the entire posterior distribution about the parameters and variables in the target problem with some observations instead of just a single point estimate. This serves the purpose of further statistical study of these latent variables, even if their true distributions are analytically intractable. Another group of inference methods commonly used to achieve the similar aim is Markov chain Monte Carlo (MCMC) methods like Gibbs sampling, which seeks to produce reliable resampling of given observations that help to approximate latent variables well. Another common Bayesian method that has a similar iterative variable update procedure is Expectation Maximization (EM). For EM, however, only point estimates of posterior distribution are obtained. The estimates are \"Expectation maximizing\" points, which means any information about the distribution around these points (or the parameters they estimate) are not preserved. On the other hand, despite the advantage of \"entire distribution\" Variational inference has, its point estimates are often derived just by the mean value of the approximated distributions. Such point estimates are often less significant compared to those derived using EM, as the optimum is not directly achieved from the Bayesian network itself, but the optimal distributions inferred from the network. Popular algorithms applying variational inferenceThe popularity of variational inference has grown to even surpass the classical MCMC methods in recent years. It is particularly successful in generative modeling as a replacement for Gibbs sampling. The methods often show better empirical result than Gibbs sampling, and are thus more well-adopted. We here showcase some popular machine learning models and even deep learning models that heavily rely on variational inference methods and achieved great success: Latent Dirichlet Allocation: With the underlying Dirichlet distribution, the model applies both variational method (for latent variable distribution) and EM algorithm to obtain an optimal topic separation and categorization. variational autoencoder: The latent Gaussian space (a representation for the input with all the latent variables and parameters) is derived from observations, and fine-tuned to generate some convincing counterparts (a copy for instance) of the input. These models often rely on a mixture of statistical learning theories, but variational inference is definitely one of the key function within them.","link":"/post/blogs/prob_and_stats/variational-inference/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part I","text":"IntroductionRecently I've been learning about reinforcement learning from amazing lectures from David Silver. These provide an overview of the classical algorithms in RL and potential challenges for future researches, in the subsequent blogs, I'll talk about the major aspects of RL and provide some solid math details on how algorithms in RL is executed. What is Reinforcement LearningAn RL agent may include one or more of these components: Policy: agent’s behaviour function It is a map from state to action Deterministic policy: a = Stochastic policy: Value function: how good is each state and/or action Value function is a prediction of future reward Used to evaluate the goodness/badness of states And therefore to select between actions Model: agent’s representation of the environment A model predicts what the environment will do next predicts the next state predicts the next (immediate) reward, this often takes the form of expectation It is derived for a classical problem - Exploration vs Exploitation. There is no supervisor, only a reward signal. Sometimes, feedback is delayed, not instantaneous. Time really matters (sequential, non i.i.d data); and Agent's actions affect the subsequent data it receives. Prediction vs ControlRL problems is often classified into a prediction problem or a control problem Prediction: Given a policy, evaluate the future Control: Find the best policy Markov Decision Process (MDP)Before venturing into the exact algorithms, let's lay out some fundamental math concepts here. Prior Knowledge Basic Probability Knowledge Basic Stochastic Process: Markov Chain Transition Matrix Problem setup This is an RL setting where the environment is fully observable The current state completely characterises the process Almost all RL problems can be formalised as MDPs (Bandit are MDP with 1 state &amp; finite/infinite actions) Terminologies Markov Reward Process A Markov reward process is a Markov chain with values. In addition to , we have reward function and a discount factor Return The return is the total discounted reward from time-step : Intuitively, the discount factor favours future rewards at a nearer date State-value function The state value function of an MRP is the expected return starting from state Bellman Equation We apply one-step analysis on and observe that: Therefore, we find that value function of any state is only dependent on its outgoing states (successors) We can then construct a matrix equation: , which can be solved in (hence very expensive) For large state set, more efficient algorithms utilising Dynamic programming (DP) is preferred MDP A Markov decision process (MDP) is a Markov reward process with decisions In addition to states, we have a set of actions The probability and reward functions now conditionally depend on and simultaneous as follows Policy A policy is a distribution over actions given states: A policy is stationary (the probability does not change for different iterations) State-value function and Action-value function for MDP (Differs from 3.) State-value function: the expectation taken w.r.t the policy Action-value function: Note that Applying Bellman equation on and Note that we cannot do simple one-step analysis since there are 2 variables and now. Instead, we try to apply OSA on w.r.t , and then do OSA on w.r.t to get Bellman Expectation Equation on , and swap the 2 variables’ order to derive equation for First step (Bellman Expectation Equation): Second step (Bellman Optimality Equation): Optimality We try to maximize and (Goal 1) Theorem suggests existence of policy that achieves Goal 1 deterministically. An optimal policy can be found by choosing actions for each state such that the choices maximise over Solving for optimality The Bellman equations in 8 is often nonlinear and has no closed form in general We often need to apply sequential methods to solve it Value iteration Policy Iteration Q-Learning Sarsa To read more on extensions, refer to Page 49 of this slides.","link":"/post/blogs/rl/reinfocement-1/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part II","text":"Dynamic Programming in RLIntroduction DP assumes full knowledge of the MDP A prediction problem: The input is an MDP/MRP and a policy . The output is a value function . A control problem: The input is an MDP. The output is the optimal value function and an optimal policy . Synchronous DPThe following table summarizes the type of problems that is solved synchronously via iteration/evaluation algorithms: Problem Bellman Equation Algorithm Prediction Bellman Expectation Equation Iterative Policy Evaluation Control Bellman Expectation Equation Policy Iteration + Greedy Policy Improvement Policy Iteration Control Bellman Optimality Equation Value Iteration Iterative Policy Evaluation Problem: evaluate a given policy Algo sketch: Assign each state with an initial value (for example: ) Following the policy, compute the updated value function using the Bellman Expectation Equation Iterate until convergence (proven later) Policy Improvement Upon Evaluation of a policy , we can seek to greedily improve the policy such that we obtain . (expr 1) The greedy approach acts as selecting . (eq 1) We can prove that eq 1 leads to expr 1 as follows: In one step: . Note that is a deterministic policy. Observe that Hence Basically, we find that this method is equivalent to solving the Bellman Optimality equation. So we obtain as an optimal policy Note that this process of policy iteration always converges to . Drawback: Policy Iteration always Evaluation an entire Policy before it starts to improve on the policy. This may be highly inefficient if the evaluation of a policy takes very long time (e.g. infinite MDP) To deal with the Drawback, we utilise DP Value Iteration. Value Iteration We improve the value function in each iteration Note that we are only improving the value function, where this value function is based on any explicit policy Intuition: start with final rewards (again all 0 for example) and work backwards Now assume we know the solution to a subproblem , then we can find by one-step look ahead: Therefore, we can always update the value function in each iteration backwards until convergence. Contraction Mapping Theorem To be updated upon publishing the markdown Refer to page 28 - 42 (DP) Asynchronous DPThere are 3 simple ideas, which I haven't learning in detail: In-place dynamic programming Prioritised sweeping Real-time dynamic programming","link":"/post/blogs/rl/reinfocement-2/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part III","text":"Model - Free Prediction1. IntroductionThis is an important chapter that lays the fundamental work for model-free control algorithms. In this blog we shall see a few important ideas (MC, TD, online/offline, forward/backward learning) being discussed. While this chapter is not math-intense, it is imperative for us to remember the concepts before moving onto control algorithms. To begin with, note that this is a prediction problem. Hence we are still only going to predict the final based on a given . However, the Model-free part suggests that we no longer require an MDP to be explicitly defined/given. This is because we are going to use a sampling strategy. Sampling:If a strategy derives certain functions (in this case ) directly via episodes of observations, we say that this strategy applies sampling method. 2. Monte Carlo (MC) Policy Evaluation MC learns from complete episodes (no bootstrapping) Monte-Carlo policy evaluation uses empirical mean return instead of expected return First-step MC For each state , only conisder the first time that is visited in each episode (update at most 1 time per run) Increment counter Increment total return Estimate mean return value The estimator converges to when number of visits approaches infinity Every-step MC For each state , conisder each time that is visited in each episode (update at most 1 time per run) For instance, , then we update and where The remaining part are the same as First-step MC Incremental Monte-Carlo Updates Based on the idea Hence we take perspective of each episodes observations instead of states . In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes. Hence the formula is tweeked. Limitation:Since MC only updates when the episode terminates (hence 'complete'), it cannot be applied to process which may run infinitely. 3. Temporal Difference (TD) Learning A solution to the 'incomplete' episodes problem by applying bootstrapping The above is replaced by an estimate (estimated return) based on the Bellman Expectation Equation. is called TD target is called TD error As now only depends on the next time-step and not the entire episode result , we say it is online (and hence the other method which updates everything when episode ends is offline). 4. Bias Variance trade-off Computation using is unbiased, but with high variance due to all rewards from transitions and actions selected at random Computation using TD target is biased, but with much lower variance, since we we only have one reward to be varied (note that is known and fixed during update of By the classical trade-off analysis, we see that TD is more sensitive to inital values TD is usually more efficient than MC 5. More comparisons between TD and MC MC TD converges to solution with minimum mean-squared error max likelihood Markov model Convergence with function approximation Yes No Exploits Markov property No Yes More efficient in Non-Markov env Markov env 6. TD()Now we explore alternative estimators for n-step return: -return: Here by utilising , we have TD() as a new evaluation policy. However, notice from the expression above that is forward-looking, meaning that this would require transition steps until the episodes end. This faces exactly the same limitation as MC!!! 7. Solution: Backward online evaluationInstead of look for values in future time-steps, we use values from earlier time-steps. This method is called a backward view of TD(). In essence, we update online, every step, from incomplete sequences. Based on a theorem, the sum of offline updates is identical for forward-view and backward-view TD(). 8. Eligibility TraceThe key to Backward TD() is eligbility trace, we intuitively derive contributing factors from earlier time steps as follows: Frequency heuristic: assign credit to most frequent states Recency heuristic: assign credit to most recent states ( discount rate) Combing 2 heuristics above, we obtain a rule and 𝟙.Hence the new update formula is . Observe now that we need to update for every state upon each time-step. 9. Additonal note TD(1) is roughly equivalent to every-visit Monte-Carlo TD(0) is exactly equivalent to simple TD Model Free Control1. Main objectiveInstead of estimating the value function, we try to optimize the value function with an unknown MDP. 2. Recap of On-Policy vs Off-Policy On-policyLearn about from experience sampled from previous rounds in .In essence trying to improve by running it using current agent iteself. Off-policyLearn about from experience sampled from previous rounds (or complete run) in .In essence trying to improve by Observing another policy getting run by another agent and deduce several directions to improve . 3. Generalised Policy Iteration With Monte-Carlo EvaluationIn general we can following the policy iteration method introduced in chapter 3 following 2 steps: Policy evaluation - Monte-Carlo policy evaluation: We want to evaluate . However, with out MDP, we cannot determine easily using a simple State-Value Function. So we must resort to a Action-Value Function, i.e., . We further observe that in each iteration, we must run the full policy to obtain for every action/state pair. This is highly inefficient as we do not know how long it takes. Hence instead we do episodic updates using . That is, we do not fully evaluate that policy, but sample state-action pair with current policy for times per episode and immediately improve the policy upon that. This manually set by us imposes guarantee on sampling complexity. We call the strategy above GILE MC control as it satisfies the GILE property. In conclusion, the evaluation phase is as follows: Sample kth episode using For each state and action in the episode: , Policy improvement: -Greedy policy improvement We choose actions that greedily maximizes the . We allow some degree of exploration by making such greedy choice with probability. Nothat this -Greedy policy works as we always have improvement like the proof shown in DP note. 4. GILE propertyGreedy in the Limit with Infinite Exploration (GLIE) has the following 2 parts: All state-action pairs are explored infinitely many times: The policy converges on a greedy policy, For example, -greedy is GLIE if reduces to zero at .This property enables the following theorem: GLIE Monte-Carlo control converges to the optimal action-value function: 5. Temporal Difference method From MC to TDTemporal-difference (TD) learning has several advantages over Monte-Carlo (MC) Lower variance Online Incomplete sequences Sarsa update of The most basic form is . Now recall from model-free prediction the variations of TD: n-step Sarsa Forward View Sarsa() Backward View Sarsa(): we use eligibility traces in an online algorithm . In each iteration, we upate for every pair. Off-policy learningIn this case, we evaluate target policy to compute or , but the evaluation was based on another (ongoing or completed) policy run It has the following advantages: Learn from observing humans or other agents Re-use experience generated from old policies Learn about optimal policy while following exploratory policy Learn about multiple policies while following one policy Importance sampling for Off-policyWe note that we can estimate the expectation of a different distribution via: One may trie to use returns generated from to evaluate via multiple sampling corrections: and then However, this multiple chaining may result in: Invalud computation when one of the while Dramatically increasing variance Hence, we consider adopting TD target for importance sampling instead of actual return . This removes the multiple chaining as the expression becomes: Unfortunately, in the above expression, we are still sticking to the policy in choosing when we update our for . This is not very reasonable, as our policy could potentially have a better choice of action. Importance sampling discounts this fact. Hence we may seek for alternative solution that removes to need to do importance sampling. 6. Q-LearningQ-learning is a method that resolves the above issue. We now consider update based on : we choose maximizer . This allows both behaviour and target policies to improve. Note that in this case, is improved via a -greedy case since is chosen randomly with probability and by theorem.","link":"/post/blogs/rl/reinfocement-3/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part IV","text":"Value Function ApproximationWe know various methods can be applied for function approximation. For this note, we will mainly consider those differentiable methods: Linear Approximation and Neural Nets 1. Stochastic Gradient Descent (SGD)Here let's review a basic approximation strategy for gradient-based method: Stochastic Gradient Descent. First our aim is to minimize the mean square error (MSE) between our estimator and the true function. The error is represented by To attain we need to update the gradient until convergence. A full gradient update has the issue of converging at local minimum. Hence stochastic sampling with will work better in general. 2. LinearizationWe begin by considering a linear model. So where is the feature vector/representation of the current state space. The stochastic update in SGD is also updated to . On the other hand, we don't have an oracle for a known in practice, so we need ways to estimate it. This is where algorithm design comes in. Algorithm analysis1. linear Monte-Carlo policy evaluation To represent , we use . In every epoch, we apply supervised learning to “training data”: . The update is now Note that Monte-Carlo evaluation converges to a local optimum As is unbiased, it works even when using non-linear value function approximation 2. TD Learning We use for . TD(0) has the update formula: Linear TD(0) converges (close) to global optimum On the other hand we can use -return as substitute. This is a TD() method. Forward view linear TD(): Backward view linear TD() requires eligibility trace: 3. Convergence of Prediction Algorithms On\\Off-policy Algorithm Table-lookup Linear Non-Linear On-Policy MC Y Y Y On-Policy TD(0) Y Y N On-Policy TD() Y Y N Off-Policy MC Y Y Y Off-Policy TD(0) Y N N Off-Policy TD() Y N N Action-Value Function ApproximationNow we don't simply approximate a value function , but approximate action-value function instead. The main idea is just find . Both MC and TD work the same way exactly by substituting these items inside the expressions. ImprovementsGradient TDSome more recent improves aim to resolve the failure of convergence of off-policy TD algorithms. This gave birth to a Gradient TD algorithm that converges in both linear and non-linear cases. This requires an additional parameter to be added and tuned which reprsents the gradient of projected Bellman error. In a similar fashion, a gradient Q-learning is also invented, but with no gurantee on non-linear model convergence. Least Squares Prediction and Experience ReplayLS estimator is known to approximate well in general. So instead of correctly approximating , it may also be ideal to approximate instead. It is found that SGD with Experience Replay converges in this case. By \"Experience Replay\" we are storing the history in each epoch instead of discarding them after each iteration. And we randomly selection some of these \"data\" for stochastic update in SGD. Deep Q-Networks (DQN) DQN uses experience replay and fixed Q-targets It takes actions based on a -greedy policy Store transition in replay memory (experience replay) Sample random mini-batch of transitions from Compute Q-learning targets w.r.t. old, fixed parameters (fixed Q-target: not the latest but a computed some batches ago) In general, LS-based methods work well in terms of convergence but suffers from computational complexity.","link":"/post/blogs/rl/reinfocement-4/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part V","text":"Policy Gradient1. General Overview Model based RL: Pros 'Easy' to learn: via procedure similar to supervised learning Learns everything from the data Cons Objective captures irrelevant info May focus on irrelevant details Computing policy (planning) is non-trivial and expensive (offline/static evaluation) Value based RL: Pros Closer to True objective Fairly well understandable - somewhat similar to regression Cons Still not the true objective (similar to model-based) Policy based RL: Pros Always try to find the true objective (directly targeting the policy) Cons Ignore some useful learnable knowledge (like policy state/action values) [but can be overcomed by combining with value-function approximation] 2. Model-free Policy based RL We directly parametrize the policy via; Advantages: Better convergence properties (gradient method) Effective in high-dimensional or continuous action spaces Can learn stochastic optimal policies (like a scissor-paper-stone game) Sometimes policies are simple while values and models are more complex (large environment but easy policy) Disadvantages: Susceptible to local optima (especially with non-linear function approximator) Often obtain knowledge that is specifc and does not always generalize well (high variance) Ignores a lot of information in the data (when used in isolation) 3. Policy Objective functionHere is a list of functions that can be used potentially measure the quality of policy . Each is evaluated depending on the things we are concerned about: In episodic environment, we can use the start value: In continuous environment, we can use the average value: where is the probability of being in state in the long run (long-term proportion) Otherwise, we replace the value function with the reward function so: Since the main target is now to optimize , we can now simply apply gradient-based method to solve the problem (in this case, gradient ascent) 4. Computing the policy gradient analyticallyWe know that the most important part of any policy function is just the policy expression . Hence, assuming that is differentiable, we find: We then say that the score function (gradient base) of a policy is just . We further note that if is an expectaion function dependent on , i.e., , we can always apply this gradient base inside the expectation for gradient computation: $$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}{\\pi{\\theta}}[f(S,A)\\nabla_{\\theta} \\log \\pi_{\\theta}(S,A)]$$ This is called the score function trick. One useful property is that if b does not depend on the action . (expr 1) Now consider formally, we have the following Policy Gradient Theorem: For any differentiable policy , the policy gradient where is the long-term value Proof: Let's consider the expected return as the objective where is the filtration. note here is dependent on as it affects the filtration. Now: where is policy probabilty of this filtration . (Applying the score function trick) So: We further notice that is now a constant for every pair in for . However, for any , does not depend on , and by expr 1 above, we can see that . 5. Actor-Critic Algorithm Most basic Q Actor-Critic policy gradient still has high variance We can use a critic to estimate the action-value function: Actor-critic algorithms maintain two sets of parameters Critic: Updates action-value function parameters Actor: Updates policy parameters , in direction suggested by critic Actor-critic algorithms follow an approximate policy gradient: The critic is solving a familiar problem: policy evaluation, which now can be solve using methods via value-based methods. However, this approximation of the policy gradient introduces bias. A biased policy gradient may not find the right solution. We can choose value function approximation carefully so that this bias is removed. This is possible because of the Compatible Function Approximation Theorem below: If the following two conditions are satisfied: Value function approximator is compatible to the policy Value function parameters w minimise the mean-squared error Then exactly. Advantage Actor-Critic Recall expr 1, we can again apply this on to further reduce the variance introduced by a large value. Consider . This is called an advantage function. Since does not depend on the actions, so plays no role here. Then expr 1 with formula results in the following TD Actor-Critic We now apply approximation again (Compatible Function Approximation Theorem) on using the an estimated TD error In practice we can use an approximate TD error So now the critic update is Note given this variant that Critic can estimate value function from many targets at different time-scales. (MC, TD(0), Forward/Backward TD() Natural Actor-Critic refers to Policy Gradient for more on this content.","link":"/post/blogs/rl/reinfocement-5/"},{"title":"Recommender Systems: II. Factorization Machine","text":"Factorization Machine1. Definition In essence, a generalized matrix factorization method Field: A type/column in the original dataset Feature: A value in the Field (Nike is a feature, Brand is a field) Movitation: Traditional regression methods cannot handle sparse matrix very well (too much waste in computation time on null values) FM solves the problem of considering pairwise feature interactions (linear time complexity). It allows us to train, based on reliable information (latent features) from every pairwise combination of features in the model. Main logic: Instead of using field as column, each feature has a column. So the columns are basically one-hot-encoding for each value in the field and the row is user id each row covers all the information a user has log-loss function to minimize: w_i: feature parameter vector (to be optimized) x_i: feature vector (column, given) v_i: latent vector of predefined low dimension k (to be optimized) The idea here is that except for individual feature, it consider the combination of 2 features (hence a degree = 2) as a factor Extension: Field-aware FM For each feature, the parameter vector is no longer unique A feature may interact with other features with different fields. Hence we differentiate the parameter vector for a feature based on the field of its interacting feature E.g: Gaming is an activity, Make is a gender; The parameter vector for Male may also be a if it is interacting with a brand like Nike Important note on numerical features Numerical features either need to be discretized (transformed to categorical features by breaking the entire range of a particular numerical feature into smaller ranges and label encoding each range separately). Another possibility is to add a dummy field which is the same as feature value will be numeric feature for that particular row (For example a feature with value 45.3 can be transformed to 1:1:45.3). However, the dummy fields may not be informative because they are merely duplicates of features. 2. Code Sample Note that the code below will faill because we haven’t installed the xlearn package (too tedious) Refer to the code to get an inspiration Only apply the code if you have the need to use FM or FFM in your model Note that usually DL method works better for the FM-integrated recommender 123456789101112131415import pandas as pdimport xlearn as xltrain = pd.read_csv('loan prediction/train_u6lujuX_CVtuZ9i.csv')import warningswarnings.filterwarnings('ignore')cols = ['Education','ApplicantIncome','Loan_Status','Credit_History']train_sub = train[cols]train_sub['Credit_History'].fillna(0, inplace = True)dict_ls = {'Y':1, 'N':0}train_sub['Loan_Status'].replace(dict_ls, inplace = True)## train-test splitfrom sklearn.model_selection import train_test_splitX_train, X_test = train_test_split(train_sub, test_size = 0.3, random_state = 5) Next, we need to convert the dataset to libffm format which is necessary for xLearn to fit the model. Following function does the job of converting dataset in standard dataframe format to libffm format. df = Dataframe to be converted to ffm format Type = 'Train' / 'Test'/ 'Valid' Numerics = list of all numeric fields Categories = list of all categorical fields Features = list of all features except the Label and Id 12345678910111213141516171819202122232425262728293031323334353637383940414243def convert_to_ffm(df,type,numerics,categories,features): currentcode = len(numerics) catdict = {} catcodes = {} # Flagging categorical and numerical fields for x in numerics: catdict[x] = 0 for x in categories: catdict[x] = 1 nrows = df.shape[0] ncolumns = len(features) with open(str(type) + \"_ffm.txt\", \"w\") as text_file: # Looping over rows to convert each row to libffm format for n, r in enumerate(range(nrows)): datastring = \"\" datarow = df.iloc[r].to_dict() datastring += str(int(datarow['Loan_Status'])) # Set Target Variable here # For numerical fields, we are creating a dummy field here for i, x in enumerate(catdict.keys()): if(catdict[x]==0): datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x]) else: # For a new field appearing in a training example if(x not in catcodes): catcodes[x] = {} currentcode +=1 catcodes[x][datarow[x]] = currentcode #encoding the feature # For already encoded fields elif(datarow[x] not in catcodes[x]): currentcode +=1 catcodes[x][datarow[x]] = currentcode #encoding the feature code = catcodes[x][datarow[x]] datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\" datastring += '\\n' text_file.write(datastring) the xLearn library can handle csv as well as libsvm format for implementation of FMs while we necessarily need to convert it to libffm format for using FFM. Once we have the dataset in libffm format, we could train the model using the xLearn library. xLearn can automatically performs early stopping using the validation/test logloss and we can also declare another metric and monitor on the validation set for each iteration of the stochastic gradient descent. 1234567891011121314151617181920212223ffm_model = xl.create_ffm()ffm_model.setTrain(\"train_ffm.txt\")param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'}# Start to train# The trained model will be stored in model.outffm_model.fit(param, './model.out')# The library also allows us to use cross-validation using the cv() function:ffm_model.cv(param)# Prediction taskffm_model.setTest(\"test_ffm.txt\") # Test dataffm_model.setSigmoid() # Convert output to 0-1# Start to predict# The output result will be stored in output.txtffm_model.predict(\"./model.out\", \"./output.txt\")","link":"/post/blogs/recom_sys/recommender-2/"},{"title":"Recommender Systems: I. Content-Based Filtering And Collaborative Filtering","text":"OverviewThe rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user. They are used to predict the rating or preference that a user would give to an item. Almost every major tech company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. Moreover, companies like Netflix and Spotify depend highly on the effectiveness of their recommendation engines for their business and success. Traditional recommender system modelsThere are basically three types of traditional recommender systems, let's use the example of movie recommendation (e.g. Netflix): Demographic Filtering: They offer generalized recommendations to every user, based on movie popularity and/or genre. The System recommends the same movies to users with similar demographic features. Since each user is different , this approach is considered to be too simple. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience. Content Based Filtering: They suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it. Collaborative Filtering: This system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like its content-based counterparts. In later blogs, we will talk about more recent models for recommender systems, including factorization machines and deep learning based models. Content Based Filtering1. Definition Use additional information about users and/or items. Example: User features: age, the sex, the job or any other personal information Exmaple: Item features: the category, the main actors, the duration or other characteristics for the movies. Main idea: given the set of features (both User and Item), apply a method to identify the model that explain the observed user-item interactions. Content Flow Little concern about \"Cold Start\": new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities One key tool used: Term Frequency-Inverse Document Frequency (TF-IDF): TF: the frequency of a word in a document IDF: the inverse of the document frequency among the whole corpus of documents. log: log function is taken to dampen the effect of high frequency word (0 vs 100 0 vs 2 (log100)) Note that normalization is needed before we apply TF-IDF because the initial feature map are all 1's and 0's, but the log function will remove all these differentiation. In the end the TF score will just be 1/0 2. Limitation They are not good at capturing inter-dependence or complex behaviours. For example: A user may prefer gaming + tv the most while a pure tv is not really his favourate. 3. Code Sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as np # linear algebraimport pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.pairwise import linear_kernelbooks = pd.read_csv('goodread/books.csv', encoding = \"ISO-8859-1\")ratings = pd.read_csv('goodread/ratings.csv', encoding = \"ISO-8859-1\")book_tags = pd.read_csv('goodread/book_tags.csv', encoding = \"ISO-8859-1\")tags = pd.read_csv('goodread/tags.csv')tags_join_DF = pd.merge(book_tags, tags, left_on='tag_id', right_on='tag_id', how='inner')to_read = pd.read_csv('goodread/to_read.csv')tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix = tf.fit_transform(books['authors'])cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)titles = books['title']indices = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of book authorsdef authors_recommendations(title): idx = indices[title] sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]books_with_tags = pd.merge(books, tags_join_DF, left_on='book_id', right_on='goodreads_book_id', how='inner')tf1 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix1 = tf1.fit_transform(books_with_tags['tag_name'].head(10000))cosine_sim1 = linear_kernel(tfidf_matrix1, tfidf_matrix1)# Build a 1-dimensional array with book titlestitles1 = books['title']indices1 = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of books tagsdef tags_recommendations(title): idx = indices1[title] sim_scores = list(enumerate(cosine_sim1[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]temp_df = books_with_tags.groupby('book_id')['tag_name'].apply(' '.join).reset_index()books = pd.merge(books, temp_df, left_on='book_id', right_on='book_id', how='inner')books['corpus'] = (pd.Series(books[['authors', 'tag_name']] .fillna('') .values.tolist() ).str.join(' '))tf_corpus = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix_corpus = tf_corpus.fit_transform(books['corpus'])cosine_sim_corpus = linear_kernel(tfidf_matrix_corpus, tfidf_matrix_corpus)# Build a 1-dimensional array with book titlestitles = books['title']indices = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of books tagsdef corpus_recommendations(title): idx = indices1[title] sim_scores = list(enumerate(cosine_sim_corpus[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]corpus_recommendations(\"The Hobbit\")corpus_recommendations(\"Twilight (Twilight, #1)\") Collaborative Filtering1. Definition Based solely on the past interactions recorded between users and items in order to produce new recommendations Main idea: Past user-item interactions are sufficient to detect similar users and/or similar items and make predictions based on these estimated proximities. Every user and item is described by a feature vector or embedding. It creates embedding for both users and items on its own. It embeds both users and items in the same embedding space. 2 Major Types: Memory Based Users and items are represented directly by their past interactions (large sparce vector) Recommendations are done following nearest neighbour information No latent model is assumed Theoretically a low bias but a high variance. Usualy recommend those items with high rating for a user : : the rating given to by user : users similar to / items similar to : respective ratings : similarity score for -th item/user similar to / (deduced using the similarity metircs shown below) Similarity Metrics: Cosine Similarity Dot Product Euclidean distance Pearson Similarity: Limitations: Don't scale easily KNN algorithm has a complexity of O(ndk) Users may easily fall into a \"information confinement area\" which only give too precise/general information Overcome Limitation: Use Approximate nearest neighbour (ANN) or take advantage of sparse matrix 2 Types: User-User: Identify users with the most similar \"interactions profile\" (nearest neighbours) in order to suggest items that are the most popular among these neighbours (and that are \"new\" to our user). We consider that two users are similar if they have interacted with a lot of common items in the same way (similar rating, similar time hovering…). Prevents overfitting As, in general, every user have only interacted with a few items, it makes the method pretty sensitive to any recorded interactions (high variance) Only based on interactions recorded for users similar to our user of interest, we obtain more personalized results (low bias) Item-Item: Find items similar to the ones the user already \"positively\" interacted with Two items are considered to be similar if most of the users that have interacted with both of them did it in a similar way. A lot of users have interacted with an item, the neighbourhood search is far less sensitive to single interactions (lower variance) Interactions coming from every kind of users are then considered in the recommendation, making the method less personalised (more biased) VS User-User: Less personalized, but more robust Model Based New reprensentations of users and items are build based on a model (small dense vectors) The model \"derives\" the relevant features of the user-item interactions Recommendations are done following the model information May contain interpretability issue Theoretically a higher bias but a lower variance 3 Types: Clustering Simple KNN/ANN will do on these metrices Matrix Factorization Main assumption: There exists a very low dimensional latent space of features in which we can represent both users and items and such that the interaction between a user and an item can be obtained by computing the dot product of corresponding dense vectors in that space. Generate the factor matrices as feature matrices for users and items. Idea: : Interaction matrix of ratings, usually sparse : User matrix : Item matrix : the dimension of the latent space Advanced Factorization methods: SVD: Not so well due to the sparsity of matrix : is item matrix; is user matrix WMF (Weighted Matrix Factorization) Weight applied to rated/non-rated entries Similar to NMF but also consider non-rated ones by associating a weight to each entry NMF: Uses only the observed or rated one Performs well with sparse matrices where indicates the -th item rated by -th user Minimizing the objective function Most common: Weighted Alternating Least Squares Formula: Regularized minimization of “rating reconstruction error” Optimization process via Gradient Descent (Reduce runtime by batch running) Instead of solving for and together, we alternate between the above two equations. Fixing and solving for Fixing and solving for This algorithm gives us an approximated result (two equations are not convex at the same time can't reach a global minimum local minimum close to the global minimum) For a fixed set of users and items, new interactions recorded over time bring new information and make the system more and more effective. Solution to \"Cold Start\" problem: Heuristics to generate embeddings for fresh items Recommending random items to new users/recommend new item to random users Recommending popular items to new usres/recommend new items to most active users Recomeending a set of various items to new users or a new item to a set of various users Use a non collaborative method for early life of the user/item Projection in WALS (given current optimal and ) 2. Pros &amp; ConsPros Require no information about users or items (more versatile) Cons Cold Start problem: Impossible to recommend anything to new users or to recommend a new item to any users Many users or items have too few interactions to be efficiently handled. 3. Comparison with Content Based Method Content based methods suffer far less from the cold start problem than collaborative approaches CB is much more constrained (because representation of users and/or items are given) CB tends to have the highest bias but the lowest variance 4. Code samples123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Import librariesimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlineimport warningswarnings.filterwarnings('ignore')import osfrom textwrap import wrap# Read the input training datainput_data_file_movie = \"movie.csv\"input_data_file_rating = \"rating.csv\"movie_data_all = pd.read_csv(input_data_file_movie)rating_data_all = pd.read_csv(input_data_file_rating)# Keep only required columnsmovie_data_all = movie_data_all.drop(['genres'], axis=1)rating_data_all = rating_data_all.drop(['timestamp'], axis=1)# Pick all ratings#num_ratings = 2000000rating_data = rating_data_all.iloc[:, :]movie_rating_merged_data = movie_data.merge(rating_data, on='movieId', how='inner')movie_rating_merged_pivot = pd.pivot_table(movie_rating_merged_data, index=['title'], columns=['userId'], values=['rating'], dropna=False, fill_value=0 )# Create a matrix R, such that, R(i,j) = 1 iff User j has selected a rating for Movie i. R(i,j) = 0 otherwise.R = np.ones(Y.shape)no_rating_idx = np.where(Y == 0.0)# Assign n_m (number of movies), n_u (number of users) and n_f (number of features)n_u = Y.shape[1]n_m = Y.shape[0]n_f = 2 # Because we want to cluster movies into 2 genres# Setting random seed to reproduce results laternp.random.seed(7)Initial_X = np.random.rand(n_m, n_f)Initial_Theta = np.random.rand(n_u, n_f)# Cost Functiondef collabFilterCostFunction(X, Theta, Y, R, reg_lambda): cost = 0 error = (np.dot(X, Theta.T) - Y) * R error_sq = np.power(error, 2) cost = np.sum(np.sum(error_sq)) / 2 cost = cost + ((reg_lambda/2) * ( np.sum(np.sum((np.power(X, 2)))) + np.sum(np.sum((np.power(Theta, 2)))))) return cost# Gradient Descentdef collabFilterGradientDescent(X, Theta, Y, R, alpha, reg_lambda, num_iters): cost_history = np.zeros([num_iters, 1]) for i in range(num_iters): error = (np.dot(X, Theta.T) - Y) * R X_grad = np.dot(error, Theta) + reg_lambda * X Theta_grad = np.dot(error.T, X) + reg_lambda * Theta X = X - alpha * X_grad Theta = Theta - alpha * Theta_grad cost_history[i] = collabFilterCostFunction(X, Theta, Y, R, reg_lambda) return X, Theta, cost_history# Tune hyperparametersalpha = 0.0001num_iters = 100000reg_lambda = 1# Perform gradient descent to find optimal parametersX, Theta = Initial_X, Initial_ThetaX, Theta, cost_history = collabFilterGradientDescent(X, Theta, Y, R, alpha, reg_lambda, num_iters)cost = collabFilterCostFunction(X, Theta, Y, R, reg_lambda)print(\"Final cost =\", cost)user_idx = np.random.randint(n_u)pred_rating = []print(\"Original rating of an user:\\n\", Y.iloc[:,user_idx].sort_values(ascending=False))predicted_ratings = np.dot(X, Theta.T)predicted_ratings = sorted(zip(predicted_ratings[:,user_idx], Y.index), reverse=True)print(\"\\nPredicted rating of the same user:\")_ = [print(rating, movie) for rating, movie in predicted_ratings]","link":"/post/blogs/recom_sys/recommender-1/"},{"title":"Recommender Systems: III. Deep-learning Methods","text":"A brief introThere are a wide variety of DL tools used for recommendation systems, we will outline a few below. We cite various information from the paper Deep Learning based Recommender System: A Survey and New Perspectives. You may find more details from that paper. Multilayer Perceptron (MLP) is a feed-forward neural network with multiple (one or more) hidden layers between the input layer and output layer. Here, the perceptron can employ arbitrary activation function and does not necessarily represent strictly binary classier. MLPs can be intrepreted as stacked layers of nonlinear transformations, learning hierarchical feature representations. MLPs are also known to be universal approximators. Autoencoder (AE) is an unsupervised model aempting to reconstruct its input data in the output layer. In general, the bottleneck layer (the middle-most layer) is used as a salient feature representation of the input data. ere are many variants of autoencoders such as denoising autoencoder, marginalized denoisingautoencoder, sparse autoencoder, contractive autoencoder and variational autoencoder (VAE). Convolutional Neural Network (CNN) is a special kind of feedforward neural network with convolution layers and pooling operations. It can capture the global and local features and significantly enhancing the eciency and accuracy. It performs well in processing data with grid-like topology. Recurrent Neural Network (RNN) is suitable for modelling sequential data. Unlike feedforward neural network, there are loops and memories in RNN to remember former computations. Variants such as Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) network are oen deployed in practice to overcome the vanishing gradient problem. Restricted Boltzmann Machine (RBM) is a two layer neural network consisting of a visible layer and a hidden layer. It can be easily stacked to a deep net. Restricted here means that there are no intra-layer communications in visible layer or hidden layer. Neural Autoregressive Distribution Estimation (NADE) is an unsupervised neural network built atop autoregressive model and feedforward neural networks. It is a tractable and efficient estimator for modelling data distribution and densities. Adversarial Networks (AN) is a generative neural network which consists of a discriminator and a generator. The two neural networks are trained simultaneously by competing with each other in a minimax game framework. Attentional Models (AM) are dierentiable neural architectures that operate based on soft content addressing over an input sequence (or image). Attention mechanism is typically ubiquitous and was incepted in Computer Vision and Natural Language Processing domains. However, it has also been an emerging trend in deep recommender system research. Deep Reinforcement Learning (DRL) . Reinforcement learning operates on a trial-and-error paradigm. The whole framework mainly consists of the following components: agents, environments, states, actions and rewards. The combination between deep neural networks and reinforcement learning formulate DRL which have achieved human-level performance across multiple domains such as games and selfdriving cars. Deep neural networks enable the agent to get knowledge from raw data and derive efficient representations without handcrafted features and domain heuristics. Pros and consPros Nonlinear Transformation: Contrary to linear models, deep neural networks is capable of modelling the non-linearity in data with nonlinear activations such as relu, sigmoid, tanh, etc. This property makes it possible to capture the complex and intricate user item interaction patterns. The linear assumption, acting as the basis of many traditional recommenders, is oversimplified and will greatly limit their modelling expressiveness. It is well-established that neural networks are able to approximate any continuous function with an arbitrary precision by varying the activation choices and combinations. Representation Learning: Deep neural networks is efficacious in learning the underlying explanatory factors and useful representations from input data. In general, a large amount of descriptive information about items and users is available in real-world applications. Making use of this information provides a way to advance our understanding of items and users, thus, resulting in a better recommender. As such, it is a natural choice to apply deep neural networks to representation learning in recommendation models. The advantages of using deep neural networks to assist representation learning are in two-folds: it reduces the efforts in hand-craft feature design. Feature engineering is a labor intensive work, deep neural networks enable automatically feature learning from raw data in unsupervised or supervised approach; it enables recommendation models to include heterogeneous content information such as text, images, audio and even video. Deep learning networks have made breakthroughs in multimedia data processing and shown potentials in representations learning from various sources. Sequence Modelling: Deep neural networks have shown promising results on a number of sequential modelling tasks such as machine translation, natural language understanding, speech recognition, chatbots, and many others. RNN and CNN play critical roles in these tasks. RNN achives this with internal memory states while CNN achieves this with filters sliding along with time. Both of them are widely applicable flexible in mining sequential structure in data. Modelling sequential signals is an important topic for mining the temporal dynamics of user behaviour and item evolution. For example, next-item/basket prediction and session based recommendation are typical applications. As such, deep neural networks become a perfect fit for this sequential pattern mining task Flexibility： Deep learning techniques possess high flexibility, especially with the advent of many popular deep learning frameworks. Cons Interpretability: Despite its success, deep learning is well-known to behave as black boxes, and providing explainable predictions seem to be a really challenging task. A common argument against deep neural networks is that the hidden weights and activations are generally non-interpretable, limiting explainability. However, this concern has generally been eased with the advent of neural attention models and have paved the world for deep neural models that enjoy improved interpretability. While interpreting individual neurons still pose a challenge for neural models (not only in recommender systems), present state-of-the-art models are already capable of some extent of interpretability, enablingexplainable recommendation. We discuss this issue in more detail in the open issues section. Data Requirement: A second possible limitation is that deep learning is known to be data-hungry, in the sense that it requires sufficient data in order to fully support its rich parameterization. However, as compared with other domains (such as language or vision) in which labeled data is scarce, it is relatively easy to garner a significant amount of data within the context of recommender systems research. Million/billion scale datasets are commonplace not only in industry but also released as academic datasets. Extensive Hyperparameter Tuning: A third well-established argument against deep learning is the need for extensive hyperparameter tuning. However, we note that hyperparameter tuning is not an exclusive problem of deep learning but machine learning in general (e.g., regularization factors and learning rate similarly have to be tuned for traditional matrix factorization etc) Granted, deep learning may introduce additional hyperparameters in some cases.","link":"/post/blogs/recom_sys/recommender-3/"},{"title":"SQL: Pick up the Basic within a day","text":"OverviewThis blog is for people who have learnt SQL at some points of their study (just like me): We can quickly recap on various important concepts in SQL. Basic Concepts Primary key: Always unique for each row of the table, it must be NOT NULL (automatically set when PRIMARY KEY is specified) Helps to identify each row even when row attributes are the same A table must have and only have 1 primary key Types: Surrogate key : An artificial key that has no mapping to anything (or business value) in real world. Natural key: A key that has mapping to real world thing: example: social security number/ NRIC / Passport number composiite key: 2 column entries combined to form a key Motivation: sometimes individuals of 2 entries cannot uniquely identify a row; Foreign key: Stores the primary key of a row in another database table The foreign key's column name NOT necessary to coincide with the foreign table's primary key column name A table can have more than 1 foreign key (or no foreign key at all) advance concept: Q: is it possible that TABLE A's foreign key is TABLE B's primary key and TABLE B's foreign key is TABLE A's primary key? A: Yes! cyclic dependency is valid in SQL. Example: employee's emp_id is department's manager_id; department's branch_id is employee's department_id. Q: is it possible that TABLE A's foreign key relates to itself? A: Yes! used to define relationships between rows within a table. Example: employee's super_id refers to a row in employee's table. Data Types: INT: – Whole number DECIMAL(M,N): – Decimal numbers - exact value, M digits, N after decimal point VARCHAR(K): – Sring of text of length K BLOB: – Binary Large Object, stores large data DATE: – 'YYYY-MM-DD' TIMESTAMP – 'YYYY-MM-DD HH:MM:SS' Difference between DROP and DELETE: DELETE DROP Data Manipulation Language command Data Definition Language Command To remove tuples from a table To remove entire schema, table, domain or constraints from the database Basic Operations Create Database 123SHOW DATABASES;CREATE DATABASE July_05;USE July_05; Logical Query Processing (IMPT) Step 1. FROM (includes JOINS) Step 2. WHERE Step 3. GROUP BY Step 4. HAVING Step 5. SELECT Step 6. ORDER BY CAUTION about column ordering: columns evaluated at later steps must be created in earlier steps Table Opeartions 123456789101112131415CREATE TABLE student ( student_id INT PRIMARY KEY, student_name VARCHAR(50), major VARCHAR(20), thr_id INT, -- can also remove the PRIMARY KEY above and add a line below -- PRIMARY KEY(student_id) FOREIGN KEY(thr_id) REFERENCES teacher(emp_id) ON DELETE SET NULL);-- Show all columns properties using the DESCRIBE keywordDESCRIBE student;-- Name Null Type-- student_id INT-- name VARCHAR(50)-- major VARCHAR(20) Delete or modify a table 123456DROP TABLE student;ALTER TABLE student ADD gpa DECIMAL(3,2);ALTER TABLE student DROP COLUMN gpa;ALTER TABLE student MODIFY COLUMN major TINYINT(1) UNSIGNED;ALTER TABLE student ADD CONSTRAINT pk_id PRIMARY KEY (student_id);ALTER TABLE student ADD CONSTRAINT fk_id FOREIGN KEY (thr_id) REFERENCES teacher(emp_id) ON DELETE SET NULL; Row Insertion 123-- Two ways of insertionINSERT INTO student VALUES(2, 'Kate', 'Sociology');INSERT INTO student(student_id, name) VALUES(3, 'Claire'); More properties of column 123456789CREATE TABLE student ( student_id INT AUTO_INCREMENT, -- id increase automaically if not specified student_id2 INT IDENTITY(1, 1) -- similar to AUTO_INCREMENT except -- IDENTITY(seed, increment) enables one to self define the starting value (seed) and the increment amount (increment) name VARCHAR(50) NOT NULL, -- name value cannot be empty major VARCHAR(20) UNIQUE, -- each row's major value must be unique across the table info VARCHAR(10) DEFAULT 'undecided', --info has 'undecided' as default value PRIMARY KEY(student_id)); Update the table 1234567-- Modify the contentUPDATE studentSET major = 'Biochemistry', name = 'What'WHERE major = 'Biology' or major = 'Chemistry'; -- if no WHERE is applied, the set applies to all-- Delete entriesDELETE FROM studentWHERE student_id = 5; SELECT keyword 1234567891011121314151617181920212223242526-- partial selectionSELECT student.name, student.majorFROM studentORDER BY major, name DESC; -- by default ascending order, DESC change to descending-- or both major and name descending by SELECT name, major FROM studentORDER BY major DESC, student_id DESC- other optional selection techniqueFROM student...WHERE major = 'chemistry' OR major = 'Bio';...WHERE name IN ('kate', 'Claire', 'Jack'); -- the use of IN keyword...WHERE birth_day BETWEEN '1970-01-01' AND '1975-01-01';...WHERE (birth_day &gt;= '1970-01-01' AND sex = 'F') OR salary &gt; 80000;...LIMIT 2 OFFSET 1;...SELECT TOP(100) -- select the 100 rows in the frontSELECT ... INTO samples -- select those columns into the \"sample\" table comparison keyword 1&lt;, &gt; , &lt;=, &gt;=, =, &lt;&gt; (means not equal to), AND, OR, ANY, ALL Functions to call 123456SELECT COUNT(sex), sexSELECT AVG(salary)SELECT SUM(salary)FROM employeeWHERE sex = 'F' AND birth_date &gt; '1971-01-01';GROUP BY sex; Wildcard 1234-- It is often used to find the string containing certain characters;SELECT *FROM clientWHERE client_name LIKE '%LLC'; UNION Motivation: row combine (fixed columns) Used to combine the multiple select statement into 1; Vertical join (add rows of the latter SELECT below the rows of former SELECT) Warning: each entry within the same column must have the same data-type1234567SELECT client.client_name AS Non_Employee_Entities, client.branch_id AS Branch_ID-- here the renaming using AS is very important to make the unioned row's column more logical-- e.g the client.branch_id and branch_supplier.branch_id unioned to be Branch_ID and branch_id separately in the table returnedFROM clientUNIONSELECT branch_supplier.supplier_name, branch_supplier.branch_idFROM branch_supplier; 12345678910111213SELECT * FROM( (SELECT CITY, LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) = (SELECT MIN(LENGTH(CITY)) FROM STATION) ORDER BY CITY) UNION (SELECT CITY, LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) = (SELECT MAX(LENGTH(CITY)) FROM STATION) ORDER BY CITY)) AS K -- note the use of AS is MUST includedORDER BY CITY JOIN Motivation: column combine (fixed row) The second table is used as an auxilary table for additional column entries in the first table1234SELECT employee.emp_id, employee.first_name, branch.branch_nameFROM employeeJOIN branch -- LEFT JOIN, RIGHT JOINON employee.emp_id = branch.mgr_id; 12345678910111213141516SELECT * FROM( (SELECT * FROM STATION AS P ORDER BY LENGTH(P.CITY) DESC ) AS A LEFT JOIN (SELECT * FROM STATION AS K ORDER BY LENGTH(K.CITY) DESC ) AS B ON A.ID = B.ID) -- here should not have AS ORDER BY A.CITY -- Must specify A or ambiguous warning Different types of join: INNER JOIN: the usual type of JOIN;Only those rows that match the ON criteria in both tables will be included and joined LEFT JOIN:All those rows in the left table are included but rows in the right table are included only when they match the ON criteria RIGHT JOIN:the symmetric idea with LEFT JOIN OUTER JOIN:All the rows in both tables are included (empty columns in the resultant table rows are treated with NULL) Nested query12345SELECT employee.first_name, employee.last_nameFROM employeeWHERE employee.emp_id IN (SELECT works_with.emp_id FROM works_with WHERE works_with.total_sales &gt; 50000); ON DELETE12ON DELETE SET NULL -- set the foreign key to null if the primary key which the foreign key refers to gets deletedON DELETE CASCADE -- delete the entire row if the primary key gets deleted, especially important if set null cannot be done (i.e the foreign key cannot be set to null) Trigger test123456789101112131415161718192021222324252627282930CREATE TABLE trigger_test ( message VARCHAR(100));-- the following code needs to be manually typed in mySQL codeDELIMITER $$ -- change the delimiter to $$CREATE TRIGGER my_trigger BEFORE INSERT ON employee FOR EACH ROW BEGIN INSERT INTO trigger_test VALUES('added new employee'); -- note the use of ; delimiter here END$$ -- we need to use the $$ as delimiter which is declared in line 168DELIMITER ; -- change the delimiter back to ;-- Conditional trigger_testDELIMITER $$CREATE TRIGGER my_trigger BEFORE INSERT -- can also be UPDATE, DELETE ON employee FOR EACH ROW BEGIN IF NEW.sex = 'M' THEN INSERT INTO trigger_test VALUES('added male employee'); ELSEIF NEW.sex = 'F' THEN INSERT INTO trigger_test VALUES('added female'); ELSE INSERT INTO trigger_test VALUES('added other employee'); END IF; END$$DELIMITER ;-- possible to drop the trigger case (done in client terminal):DROP TRIGGER my_trigger CTE: Common Table Expression1234567891011121314151617181920212223242526WITH Number -- here Number is the name of the CTE, can be anything AS(SELECT CustomerId , NTILE(1000) OVER(ORDER BY CustomerId) AS NFROM dbo.Customers),TopCustomer -- here we define the second CTE here, notice the comma \",\" above, indicates that the WITH keyword is still effectiveAS(SELECT MAX(CustomerId) AS CustIdFROM NumberGROUP BY N)SELECT -- this SELECT is together with the CTE Expression, not separate query C2.*INTO dbo.CustomersSample FROM TopCustomer AS C1INNER JOIN dbo.Customers AS C2 ON C1.CustId = C2.CustomerIdSELECT * FROM dbo.CustomersSample -- with the above cte method, we created a randomized sample in the dbo.customers table Functions and procedures Procedure Creation and Execution1234567891011DELIMITER $$CREATE PROCEDURE FizzBuzz()BEGIN DECLARE N INT DEFAULT 1; WHILE N &lt;= 100 DO SET N = N + 1; END WHILE;END$$DELIMITER ;CALL FizzBuzz(); Function Creation 12345678910111213141516DELIMITER $$CREATE FUNCTION multi( -- if function alrea exists, CREATE is changed to ALTER n INT , m INT) RETURNS INTDETERMINISTICBEGIN DECLARE result INT; SET result = m * n; RETURN result;END$$DELIMITER ;SELECT your_db_name.multi(2,3) AS result; Check if the function exists 1SHOW FUNCTION STATUS WHERE db = 'your database name'; String and numeric operations on valuesString operations 123456789101112SELECt UPPER(email) up , LOWER(last_name) low , CONCAT(first_name, ' ', last_name) full_name , LENGTH(email) email_len , CONCAT_WS(' | ', first_name, last_name) full_name_with_separator , TRIM(' hello ') AS trimmed , RIGHT(email, 3) AS right_three , LPAD(customer_id, 5, '000') AS left_zero_padding , FORMAT(address_id, 3) AS formated_3_float_pointFROM customerLIMIT 10; Regex Matching1 1234567SELECT CONCAT(first_name, ' ', last_name) FROM customerWHERE last_name ~ '^[^aeiou]' AND last_name ~* '[aeiou]$'ORDER BY right(first_name, 2);-- ~ : Case-sensitive, compares two statements, returns true if the first string is contained in the second-- ~* : Case-insensitive, compares two statements, returns true if the first string is contained in the second-- !~ : Case-sensitive, compares two statements, returns false if the first string is contained in the second-- !~* : Case-insensitive, compares two statements, return false if the first string is contained in the second Numeric Functions 1234567891011121314SELECT RAND() AS rand_num , ROUND(RAND() * 10, 2) AS rand_round_2_decimal , CEIL(RAND()) AS num_ceil , FLOOR(RAND()) as num_floor , RADIANS(180) AS pi_from_radian , DEGREES(3.141592653589793) AS pi_from_degree , ABS(-3) AS absolute_val , POWER(CUSTOMER_ID, 2) AS id_square , DATEDIFF(shop_date.date, return_date.date) AS usage_period , CONV(CUSTOMER_ID, 10, 16) AS to_hex , IFNULL(potential_NUll_column, 0) AS replacing_null_with_zeroFROM customerLIMIT 10; 1234-- A BlunderSELECT REPLACE(amount, 0, 1)FROM paymentLIMIT 5; Some Advanced operations: Window functions OVER clause determines window (the set of rows to operate on) PARTITION BY splits the result set into partitions on which the window function is applied Functions Available: Aggregate - COUNT, SUM, MIN, MAX, AVG Ranking - ROW_NUMBER, RANK, DENSE_RANK, NTILE Offset - FIRST_VALUE, LAST_VALUE, LEAD, LAG Statistical - PERCENT_RANK, CUME_DIST, PERCENTILE_CONT, PERCENTILE_DIST Windows Functions also have FRAMES ROWS RANGE 1. Demo on PARTITION BY1234567891011121314151617181920212223242526272829-- the non-window function wayWITH CTEAS(SELECT Sales_Id , SUM(Line_Total) AS TotalFROM Sales_DetailsGROUP BY Sales_Id);SELECT * FROM CTE AS AINNER JOIN Sales_Details AS B ON A.Sales_Id = B.Sales_Id; -- the window function waySELECT Sales_Id , Sales_Date , Item , Price , Quantity , Line_Total , COUNT(Line_Total) OVER(PARTITION BY Sales_Id) AS Line_Count , SUM(Line_Total) OVER(PARTITION BY Sales_Id) AS Sales_Total , SUM(Line_Total) OVER(PARTITION BY Sales_Date) AS Daily_Total , SUM(Line_Total) OVER() AS TotalFROM Sales_DetailsORDER BY Sales_Total; 2. On Ranking FunctionsRanking functions are available as part of Window Functions: ROW_NUMBER() unique incrementing integers RANK() same rank for same values, but keep the counting rolling 1, 1 (duplicate), 3, 4, 5 DENSE_RANK(): same rank for same values, but only increase rank by 1 when values change 1, 1 (duplicate), 2, 3, 4 RANK() vs DENSE_RANK(): RANK() will have rows with identical rank/ gaps in rank if we get tied values NTILE(N) assigns tile number based on the number of tiles required, just assign each row with a value from 0 - N ,in increase order Example: 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, …. ,N, N, N, N Usage: for partitioning/selective sampling of the data 12345678910111213141516171819SELECT -- note that here we use ORDER BY instead of PARTITION BY because order/rank is sort of important rather than the fixed set of value Sales_Id , Sales_Total , ROW_NUMBER() OVER(ORDER BY Sales_Total DESC) AS rownum , RANK() OVER(ORDER BY Sales_Total DESC) AS rnk , DENSE_RANK() OVER(ORDER BY Sales_Total DESC) AS dense , NTILE(3) OVER(ORDER BY Sales_Total DESC) AS ntleFROM dbo.Sales_2SELECT -- This is the modified way, we rank individual set of rows by adding on the PARTITION BY Sales_Id , Sales_Cust_Id , Sales_Total , ROW_NUMBER() OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS rownum , RANK() OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS rnk , DENSE_RANK() OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS dense , NTILE(3) OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS ntleFROM dbo.Sales_2ORDER BY Sales_Cust_Id 3. GROUP BY1234567891011121314151617181920SELECT Sales_Cust_Id , SUM(Sales_Total) AS Total , RANK() OVER(ORDER BY SUM(Sales_Total) DESC) AS rnk -- note that we used SUM(Sales_Total) not Sales_Total or Total because we need the order of SUM(Sales_Total) for each customer and Total is not defined well , DENSE_RANK() OVER(ORDER BY SUM(Sales_Total) DESC) AS dnseFROM dbo.Sales_2WHERE Sales_Date &gt;= '2019-03-01'GROUP BY Sales_Cust_IdORDER BY rnk-- special OVER clause operationSELECT Sales_Customer_Id , SUM(Sales_Amount) AS Cust_Total , SUM(SUM(Sales_Amount)) -- this declaration will be wrong as the system says cannot aggregate over another aggregation , SUM(SUM(Sales_Amount)) OVER(ORDER BY (SELECT NULL)) AS Grand_Total -- this is the proper way as the aggregation is down to the OVER Clause not the SUM(Sales_Amount) function , AVG(SUM(Sales_Amount)) OVER(ORDER BY (SELECT NULL)) AS Average_Cust_Total , CAST((SUM(Sales_Amount) / SUM(SUM(Sales_Amount)) OVER(ORDER BY (SELECT NULL))) * 100 AS DECIMAL(6,2)) AS PctFROM dbo.SalesGROUP BY Sales_Customer_Id 4. Window FRAMES123456789101112131415161718192021222324252627SELECT Sales_Id , Sales_Date , Sales_Total , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS [Running Total] -- note that SUM is a window function here -- ROWS BETWEEN ... AND CURRENT ROW gives FRAME that is the set of rows from UNBOUNDED PRECEDING to this CUR ROW -- [Runnig Total] =&gt; need to put [] between a phrase with empty space \" \" , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS BETWEEN k PRECEDING AND CURRENT ROW) AS [Running Total] -- this line has the FRAME only between the CURRENT ROW and the k rows before it; , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS UNBOUNDED PRECEDING) AS [Running Total] -- this line is a simplified version for BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW , FROM dbo.Sales_2WHERE Sales_Cust_Id = 3ORDER BY Sales_DateSELECT Sales_Id , Sales_Date , Sales_Total , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS UNBOUNDED PRECEDING) AS [Running Total] , CAST(AVG(Sales_Total) OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Date ROWS UNBOUNDED PRECEDING) AS DECIMAL(8, 2)) AS [Running Average] -- this line enables running average for individual customers for all of them -- CAST .. AS DECIMAL(8,2) reduces the resultant running average into 2 decimal pointsFROM dbo.Sales_2ORDER BY Sales_Date 5. Lag and Lead Useful for trend analysis LAG - return the value from the previous row LEAD - return the value from the next row Format: 1LAG([Column], [Offset], [Value if NULL]) Demo:123456789SELECT Sales_Customer_Id , Sales_Date , LAG(Sales_Amount, 2, 0) OVER(PARTITION BY Sales_Customer_Id ORDER BY Sales_Date) AS PrevValue -- get the Sales_Amount 2 days before, if no value is in the entry 2 days before, set it to 0 (default is NULL) , Sales_Amount , LEAD(Sales_Amount, 2, 0) OVER(PARTITION BY Sales_Customer_Id ORDER BY Sales_Date) AS NextValue -- idea is the same, just change it to laterFROM dbo.Sales 6. Rolling window12345678SELECT * , SUM(SalesAmount) OVER(ORDER BY [Date] ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS Total -- the Window FRAME method and SUM function together makings the window \"rolling\" , SUM(SalesAmount) OVER(ORDER BY [Date] ROWS BETWEEN CURRENT ROW AND 9 FOLLOWING) AS Forward -- we use FOLLOWING for the future rowsFROM #TempSales -- nothing fancy about # sign hereORDER BY [Date] -- here [] is needed because Date itself is a SQL keyword 7. Variable Specification123456789101112131415161718SET GLOBAL some_global_variable = 1;SET @n = 10;SELECT @n AS num;SET@id = (SELECT payment_id FROM payment WHERE customer_id = 2 LIMIT 1);SELECT @id AS new_id;WITH cte AS(SELECT customer_id, COUNT(payment_id) ccFROM payment pGROUP BY customer_id),cnt AS (SELECT cc, COUNT(*) AS tcc, MAX(cc) OVER() AS mcc FROM cte GROUP BY cc)SELECT *FROM cteINNER JOIN cnt ON cnt.cc = cte.cc AND (cnt.tcc = 1 OR cnt.cc = cnt.mcc)ORDER BY cte.cc DESC, customer_id ASC; ConclusionThe above codes demonstrate the majorities of the SQL codes formats an engineer would ever need in its daily CRUD operations already. Thanks for reading!","link":"/post/blogs/sql/sql-1/"},{"title":"SQL: Index and Optimization","text":"OverviewTo be honest, I'm not a pro-SQL programmer. I'm still on my journey to learn more about database and query optimization. In this blog I will just give whatever I've learnt about indexing and optimization and its mostly based on MySQL. Hope it helps! Guidelines Single Sheet query is much better than Multiple Sheet If multiple sheet is needed, Use JOIN well: Small Sheet drive Large Sheet (for e.g. left join in this case) Establish proper indexing Don't JOIN too many sheets as well Try best NOT to use subquery or Cartesian Product Window Funtions can be very helpful Indexes Allow faster retrieval of data Question: Why don't we just create loads of indexes? Ansewr: There is a trade-off, if loads of indexes exists on a table then those indexes need to be updated or maintained. In this case, DML operations suffer. 1. Index operations1234567891011-- show indicesSHOW INDEX FROM your_db_name.customer;-- Add indexALTER TABLE paymentADD INDEX idx_pay (payment_id); -- [index] can be appended by [unique] to ensure each index is uniqueCREATE FULLTEXT INDEX idx_staff ON customer (email); -- [fulltext] only applicable to string data-- Drop IndexDROP INDEX idx_pay ON payment For the full list of operations, you may refer to the official documentation of MySQL1 2. Clustered Indexes ALTER TABLE Permission WHen a table does not have a clustered index then the table is stored as a heap, if the table has a clustered index it is stored as a B-tree Data is stored in order of clustered index Only one clustered index can exists on one table Clustered indexes are effective on columns that consistent of unique increasing integers (like identity_set) When a primary key is created a unique clustered index is automatically created - this can be beneficial for queries that involve joins on this column. TODO Discuss B-Tree Study B+Tree and update","link":"/post/blogs/sql/sql-2/"},{"title":"SQL: Going into Applications with MySQL and MongoDB","text":"IntroductionThis is a blog to note down some important concepts revolving MongoDB and MySQL, two of the most popular databases reprensentative of their respective domains: NoSQL and SQL. Many people know how to use these DBMS, but fail to appreciate their characteristics, when and why they are used in certain business solutions. I try to give as much high level comparisons as possible. This ensures that People can at least answer some basic interview questions when they look for a job using these tools. MongoDB A NoSQL database for high volumn data storage Dynamic schemas: creating entries without prior restriction of the data structure Represent data as of JSON documents and use JSON Query (JavaScript) Supports sharding and replication: it partitions data across multiple servers 1. ShardingThe components of a Shard include: A Shard – A MongoDB instance which holds the subset of the data. In production environments, ALL shards need to be part of replica sets. Config server – A mongodb instance which holds metadata about the cluster, basically information about the various mongodb instances which will hold the shard data. A Router – A mongodb instance responsible to re-directing the commands send by the client to the right servers. 2. The benefits of NoSQL in MongoDB Schema Free: MongoDB has a pre-defined structure that can be defined and adhered to, but also, if you need different documents in a collection, it can have different structures. Scaled both Horizontal and Vertical: Improve system's processing power via Horizontal: Adding more machines to expand the pool of resources Vertical: Adding more power to a single machine (CPU/Storage) Optimized for WRITE performances 3. The disadvantages of Non-SQL (without fixed schema) in MongoDB Does not support use of Foreign Keys Does not support optimization of JOIN operations MongoDB is not strong ACID (Atomic, Consistency, Isolation &amp; Durability) No Stored Procedure or functions, business logic must be implemented in the backend after data is retrieved (like Node.js). This may cause the operations to slow down. MySQL Relational Database (RDBMS) Represents data in tables and rows Predefine the Schema for the tables in the database Use SQL Supports Master-slave replication and master-master replication, i.e. copy data from one server to another Optimized for high performance JOIN across multiple tables 1. Disadvantages of MySQL (or traditional RDBMS) Scaled Only Vertically Transactions related to system catalog are not ACID compliant Sometimes a server crash can corrupt the system catalog Stored procedures are not cacheable MYSQL tables which is used for the procedure or trigger are most pre-locked. Risk of SQL injection attacks (if there is no predefined schema design, there is less of such a problem) Which to choose Characteristics MongDB MySQL Data nature A lot of unstructured data Mostly Structured data Application Real-time analytics, content management, various mobile apps Applications that requires multi-row transactions such as an accounting system Service priority Cloud Based Security and ACID/BASE rules are very improtant Data Volumn Large, high-speed volumn of data Stable data flow TODO Update content on MySQL (All the interview questions &amp; all the basic knowledge) InnoDB storage engine Sharding Indexing B+Tree Red-Black Tree Update MongoDB sharding policies Discuss Distrbuted Concensus policies","link":"/post/blogs/sql/sql-3/"},{"title":"Ensemble Models: Overview","text":"OverviewAn important techinque in machine learning is ensemble models. It includes some very popular techniques like bootstraping and boosting. In the upcoming blogs, I will outline these models in detail, and give comparisons when necessary. The mathematical proofs are omitted for simplicity. However, I highly recommend interested readers to take a look at the theoretical foundations of these models to gain great intuitions about the ideas behind ensemble models. Intro An ensemble model is a composite model which combines a series of low performing or weak classifiers with the aim of creating a strong classifier. Here, individual classifiers vote and final prediction label returned that performs majority voting. Now, these individual classifiers are combined according to some specific criterion to create an ensemble model. These ensemble models offer greater accuracy than individual or base classifiers. These models can parallelize by allocating each base learner to different mechanisms. So, we can say that ensemble learning methods are meta-algorithms that combine several machine learning algorithms into a single predictive model to increase performance. Ensemble models are created according to some specific criterion as stated below: Bagging - They can be created to decrease model variance using bagging approach. Boosting - They can be created to decrease model bias using a boosting approach. Stacking - They can be created to improve model predictions using stacking approach. It can be depicted with the help of following diagram. Ensemble Machine Learning 1. Bagging Bagging stands for bootstrap aggregation. It combines multiple learners in a way to reduce the variance of estimates. For example, random forest trains N Decision Trees where we will train N different trees on different random subsets of the data and perform voting for final prediction. Bagging ensembles methods are Random Forest and Extra Trees. 2. Boosting Boosting algorithms are a set of the weak classifiers to create a strong classifier. Strong classifiers offer error rate close to 0. Boosting algorithm can track the model who failed the accurate prediction. Boosting algorithms are less affected by the overfitting problem. The following three algorithms have gained massive popularity in data science competitions AdaBoost (Adaptive Boosting) Gradient Tree Boosting (GBM) XGBoost We will discuss AdaBoost in this kernel and GBM and XGBoost in future posts. 3. Stacking Stacking (or stacked generalization) is an ensemble learning technique that combines multiple base classification models predictions into a new data set. This new data are treated as the input data for another classifier. This classifier employed to solve this problem. Stacking is often referred to as blending. Blending (average) ensemble model: Fits the base learners to the training data and then, at test time, average the predictions generated by all the base learners. Use VotingClassifier from sklearn that: fit all the base learners on the training data at test time, use all base learners to predict test data and then take the average of all predictions. Stacked ensemble model: Fits the base learners to the training data. Next, use those trained base learners to generate predictions (meta-features) used by the meta-learner (assuming we have only one layer of base learners). There are few different ways of training stacked ensemble model: Fitting the base learners to all training data and then generate predictions using the same training data it was used to fit those learners. This method is more prune to overfitting because the meta learner will give more weights to the base learner who memorized the training data better, i.e. meta-learner won't generate well and would overfit. Split the training data into 2 to 3 different parts that will be used for training, validation, and generate predictions. It's a suboptimal method because held out sets usually have higher variance and different splits give different results as well as learning algorithms would have fewer data to train. Use k-folds cross validation where we split the data into k-folds. We fit the base learners to the (k - 1) folds and use the fitted models to generate predictions of the held out fold. We repeat the process until we generate the predictions for all the k-folds. When done, refit the base learners to the full training data. This method is more reliable and will give models that memorize the data less weight. Therefore, it generalizes better on future data. 4. How are base-learners classified Base-learners are classified into two types. On the basis of the arrangement of base learners, ensemble methods can be divided into two groups. Parallel ensemble: base learners are generated in parallel for example - Random Forest. Sequential ensemble: base learners are generated sequentially for example AdaBoost. On the basis of the type of base learners, ensemble methods can be divided into two groups. Homogenous ensemble: uses the same type of base learner in each iteration. Heterogeneous ensemble: uses the different type of base learner in each iteration. Bagging vs Boosting1. Selecting the best technique- Bagging or Boosting Depends on the data, the simulation and the circumstances. Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability. If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model. By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting. In fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting. 2. Similarities between Bagging and Boosting Both are ensemble methods to get N learners from 1 learner. Both generate several training data sets by random sampling. Both make the final decision by averaging the N learners (or taking the majority of them i.e Majority Voting). Both are good at reducing variance and provide higher stability. 3. Differences between Bagging and Boosting Bagging is the simplest way of combining predictions that belong to the same type while Boosting is a way of combining predictions that belong to the different types. Bagging aims to decrease variance, not bias while Boosting aims to decrease bias, not variance. In Baggiing each model receives equal weight whereas in Boosting models are weighted according to their performance. In Bagging each model is built independently whereas in Boosting new models are influenced by performance of previously built models. In Bagging different training data subsets are randomly drawn with replacement from the entire training dataset. In Boosting every new subsets contains the elements that were misclassified by previous models. Bagging tries to solve over-fitting problem while Boosting tries to reduce bias. If the classifier is unstable (high variance), then we should apply Bagging. If the classifier is stable and simple (high bias) then we should apply Boosting. Bagging is extended to Random forest model while Boosting is extended to Gradient boosting.","link":"/post/blogs/supervised/ensemble-0/"},{"title":"Ensemble Models: Bagging Techniques","text":"OverviewWe have learnt about what bagging is in Ensemble Models: Overview, to recap, bagging is: In bagging (Bootstrap Aggregating), a set of weak learners are combined to create a strong learner that obtains better performance than a single one. Bagging helps to decrease the model’s variance. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.In this blog, we will use random forest as an example to illustrate how bagging worksBagging works as follows: Multiple subsets are created from the original dataset, selecting observations with replacement. A base model (weak model) is created on each of these subsets. The models run in parallel and are independent of each other. The final predictions are determined by combining the predictions from all the models. Next let's consider random forest, a model that fully utilized the idea of bagging in its procedure. Random Forest1. Definition A random forest consists of multiple random decision trees. Two types of randomnesses are built into the trees. First, each tree is built on a random sample from the original data. Second, at each tree node, a subset of features are randomly selected to generate the best split. (Key difference from Bagging algorithms) An ensemble model that is widely applied (as it can be parallelized) Designed to solve the overfitting issue in Decision Tree. The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias. Procedure Execute until every last combination is exhausted Bootstrapping: Create a bootstraped dataset: randomly select samples from the dataset until it reaches the same size as the original sample (we're allowed to pick the same sample more than once); Decision Tree Construction: Create a decision tree using the bootstraped dataset, but only use a random subset of variables/features at each step (i.e each decision node selection); Bagging: defined as bootstrapping the data plus using the aggregation to make a decision given a new instance, run through all the decision trees (the enitre random forest) and obtain the sum of votes for y = 1 and y = 0; (this step is called “aggregation”) decide on result of aggregation using the result with higher vote; Choose the most accurate random forest: Measure Accuracy based on the Out-of-Bag samples (CV) compute the Out-of-Bag Error as the # of samples which Bagging classifies wrongly Choice of number of variable used per step affects accuracy (optimized during CV): Usually choose the square root of the # of total variables and try a few settings above and below that value The low correlation between models is the key WARNING: RF is often considered as Bagging model while it is not always true, see this link 2. Pros &amp; ConsPros The power of handle large data sets with higher dimensionality (as each tree select much less features in its construction) The model outputs importance of variable, which can be a very handy feature (rf.feature_importance_) Balancing errors in data sets where classes are imbalanced. It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing. Using the out of bag error estimate for selection the most accurate random forest removes the need for a set aside test set. Cons It has very poor interpretability Does not work well for extrapolation to predict for data that is outside of the bounds of your original training data Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds. 3. Simple ImplementationThis is a template inspired by the Kaggle notebooks. I shall thank those writers whose code I borrowed from. Also note that here an aws s3 connection is made, which automatically makes the process parallelized. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierRSEED = 50# Load in datadf = pd.read_csv('https://s3.amazonaws.com/projects-rf/clean_data.csv')# Full dataset: https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system# Extract the labelslabels = np.array(df.pop('label'))# 30% examples in test datatrain, test, train_labels, test_labels = train_test_split(df, labels, stratify = labels, test_size = 0.3, random_state = RSEED)# Imputation of missing valuestrain = train.fillna(train.mean())test = test.fillna(test.mean())# Features for feature importancesfeatures = list(train.columns)# Create the model with 100 treesmodel = RandomForestClassifier(n_estimators=100, random_state=RSEED, max_features = 'sqrt', n_jobs=-1, verbose = 1)# Fit on training datamodel.fit(train, train_labels)n_nodes = []max_depths = []# Stats about the trees in random forestfor ind_tree in model.estimators_: n_nodes.append(ind_tree.tree_.node_count) max_depths.append(ind_tree.tree_.max_depth) print(f'Average number of nodes {int(np.mean(n_nodes))}')print(f'Average maximum depth {int(np.mean(max_depths))}')# Training predictions (to demonstrate overfitting)train_rf_predictions = model.predict(train)train_rf_probs = model.predict_proba(train)[:, 1]# Testing predictions (to determine performance)rf_predictions = model.predict(test)rf_probs = model.predict_proba(test)[:, 1]from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curveimport matplotlib.pyplot as plt# Plot formattingplt.style.use('fivethirtyeight')plt.rcParams['font.size'] = 18def evaluate_model(predictions, probs, train_predictions, train_probs): \"\"\"Compare machine learning model to baseline performance. Computes statistics and shows ROC curve.\"\"\" baseline = {} baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))]) baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))]) baseline['roc'] = 0.5 results = {} results['recall'] = recall_score(test_labels, predictions) results['precision'] = precision_score(test_labels, predictions) results['roc'] = roc_auc_score(test_labels, probs) train_results = {} train_results['recall'] = recall_score(train_labels, train_predictions) train_results['precision'] = precision_score(train_labels, train_predictions) train_results['roc'] = roc_auc_score(train_labels, train_probs) for metric in ['recall', 'precision', 'roc']: print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}') # Calculate false positive rates and true positive rates base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))]) model_fpr, model_tpr, _ = roc_curve(test_labels, probs) plt.figure(figsize = (8, 6)) plt.rcParams['font.size'] = 16 # Plot both curves plt.plot(base_fpr, base_tpr, 'b', label = 'baseline') plt.plot(model_fpr, model_tpr, 'r', label = 'model') plt.legend(); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves'); plt.show();evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)","link":"/post/blogs/supervised/ensemble-1/"},{"title":"Ensemble Models: Boosting Techniques","text":"Overview Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model. Let’s understand the way boosting works in the below steps. A subset is created from the original dataset. Initially, all data points are given equal weights. A base model is created on this subset. This model is used to make predictions on the whole dataset. Errors are calculated using the actual values and predicted values. The observations which are incorrectly predicted, are given higher weights. (Here, the three misclassified blue-plus points will be given higher weights) Another model is created and predictions are made on the dataset. (This model tries to correct the errors from the previous model) Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble. We will discuss 3 major boosting models: AdaBoost, Gradient Boost and XGBoost. AdaBoost1. Definition AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Stump: a tree with only 1 node and 2 leaves; Generally stumps does not perform as good as forest does; The AdaBoost uses the forest of stumps AdaBoost should meet two conditions: The classifier should be trained interactively on various weighed training examples. In each iteration, it tries to provide an excellent fit for these examples by minimizing training error. Complete Procedure Assign each sample with a weight (initially set to equal weight) each row in Dataframe has a equal weight Use the feature selection in decision node method to choose the first stump; Measure how well a stump classifies the samples using: where is the weight of and is the set of misclassified datapoints Determine the vote significance for the stump using Laplace smoothing for the vote significance: in case Total Error = 1 or 0, the formula will return error, we add a small value in the formula Modify the weight of samples so that next stump will take the errors that current stump made into account: 6.1 Run each sample down the stump 6.2 Compute new weight using: Formula: = New Sample Weight = Current Sample weight. = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and = place holder for 1 if stump correctly classified, -1 if misclassified. 6.3 Normalize the new weights With the new sample weight we can either: Use Weighted Gini Index to construct the next stump (Best feature for split) Use a new set of sample derived from the previous sample: pick until number of samples reach the size of original set construct an interval-selection scheme using the sum of new sample weight as cutoff value if a number falls in i-th interval between (0,1), choose i-th sample; - e.g (0-0.07:1; 0.07-0.14:2; 0.14-0.60:3;0.60-0.67:4; etc) randomly generate a number x between 0 and 1 pick the sample according to the scheme (note that the same sample can be repeatly picked) Repeat Step 1 to 7 until the entire forest is built 2. Pros and ConsPros Achieves higher performance than bagging when hyper-parameters tuned properly. Can be used for classification and regression equally well. Easily handles mixed data types. Can use \"robust\" loss functions that make the model resistant to outliers. AdaBoost is easy to implement. We can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. Cons Difficult and time-consuming to properly tune hyper-parameters. Cannot be parallelized like bagging (bad scalability when vast amounts of data). More risk of overfitting compared to bagging. AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. Slower as compared to XGBoost 3. Comparison with Random Forest Random Forest VS AdaBoost (Bagging vs Boosting) Random Forest uses full grown trees while Adaboost uses stumps (one root node with two leafs) In a Random Forest all the trees have similar amount of say, while in Adaboost some trees have more say than the other. In a random forest the order of the tree does not matter, while in Adaboost the order is important (especially since each tree is built by taking the error of the previous error). 4. Sample Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pandas as pdimport numpy as npfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.model_selection import train_test_split,GridSearchCV, StratifiedShuffleSplitfrom sklearn.metrics import accuracy_scorefrom sklearn.preprocessing import StandardScalertrain = pd.read_pickle(\"train.pkl\")X = train.drop(['Survived'], axis = 1)y = train[\"Survived\"]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state=0)# Feature Scaling## We will be using standardscaler to transformst_scale = StandardScaler()## transforming \"train_x\"X_train = st_scale.fit_transform(X_train)## transforming \"test_x\"X_test = st_scale.transform(X_test)adaBoost = AdaBoostClassifier(base_estimator=None, learning_rate=1.0, n_estimators=100)adaBoost.fit(X_train, y_train)y_pred = adaBoost.predict(X_test)accuracy_score(y_test, y_pred)n_estimators = [100,140,145,150,160, 170,175,180,185];cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)learning_r = [0.1,1,0.01,0.5]parameters = {'n_estimators':n_estimators, 'learning_rate':learning_r }grid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree. ), param_grid=parameters, cv=cv, n_jobs = -1)grid.fit(X,y) print (grid.best_score_)print (grid.best_params_)print (grid.best_estimator_)adaBoost_grid = grid.best_estimator_adaBoost_grid.score(X,y) GBM (Gradient Boosting) Gradient Boosting trains many models in a gradual, additive and sequential manner (sequential + homogeneous). Major Motivation: allows one to optimise a user specified cost function, instead of a loss function that usually offers less control and does not essentially correspond with real world applications. Main logic: utilizes the gradient descent to pinpoint the challenges in the learners' predictions used previously. The previous error is highlighted, and, by combining one weak learner to the next learner, the error is reduced significantly over time. Procedure: For Regression Start by Compute the average of the , this is our 'initial prediction' for every sample Then compute the ; : Pseudo Residual at i-th sample : True value of i-th sample : Estimated value of i-th sample (here in first iteration) Construct a new decision tree (fixed size) with the goal of predicting the residuals (a DT of , not the true value!!!) If in a leaf, # of leaves &lt; # of samples, then put the of samples that fall into same category into the same leaf; Then take average of all values on that leaf as output values; Compute the new predicted value () : Newly Estimated value of i-th sample : learning rate, usually between 0 ~ 0.1 : Estimated pseudo residual values (deduced from the decision tree) Compute the new of each sample = Construct the new tree with the new pseudo residual in step 5: Repeat step 2, 3 Compute the new predicted value (here is deduced from a new DT): Compute the new pseudo residual of each sample Loop through the process UNTIL: adding additional trees does not significantly reduce the size of the pseudo residuals For Classification Set the initial prediction for every sample using ( is th probability of a sample being classified as 1) : (same for every sample) : log(odds) prediction for i-th sample, initially the same value, but value for each sample will change upon future iterations Using logistic function for classification: ; Decide on the classification: if &gt; threshold, then \"Yes\"; else \"No\"; here the threshold may not be 0.5 (AUC and ROC to decide on the value); Compute Build a DT using the pseudo residual Transformation of the pseudo residual to obtain the output values on each leaf: e.g: if a leaf has (0.3, -0.7), then the leaf output value Compute the new prediction : log(odds) prediction for i-th sample in new iteration : learning rate, usually between 0 ~ 0.1 Compute the new Probability Compute the new predicted value for each sample; Compute the new pseudo residual for each sample; Build the new tree; Loop until the pseudo residual does not change significantly; Early Stopping: Early Stopping performs model optimisation by monitoring the model’s performance on a separate test data set and stopping the training procedure once the performance on the test data stops improving beyond a certain number of iterations. It avoids overfitting by attempting to automatically select the inflection point where performance on the test dataset starts to decrease while performance on the training dataset continues to improve as the model starts to overfit. In the context of gbm, early stopping can be based either on an out of bag sample set (\"OOB\") or cross- validation (\"cv\"). 1. Pros &amp; ConsPros Robust against bias/outliers GBM can be used to solve almost all objective function that we can write gradient out, some of which RF cannot resolve Able to reduce bias and remove some extreme variances Cons More sensitive to overfitting if the data is noisy GBDT training generally takes longer because of the fact that trees are built sequentially Prone to overfitting, but can be overcame by parameter optimization 2. AdaBoost vs GBM Both AdaBoost and Gradient Boosting build weak learners in a sequential fashion. Originally, AdaBoost was designed in such a way that at every step the sample distribution was adapted to put more weight on misclassified samples and less weight on correctly classified samples. The final prediction is a weighted average of all the weak learners, where more weight is placed on stronger learners. Later, it was discovered that AdaBoost can also be expressed as in terms of the more general framework of additive models with a particular loss function (the exponential loss). So, the main differences between AdaBoost and GBM are as follows: The main difference therefore is that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function (Exponential loss function). Hence, gradient boosting is much more flexible. AdaBoost can be interepted from a much more intuitive perspective and can be implemented without the reference to gradients by reweighting the training samples based on classifications from previous learners. In Adaboost, shortcomings are identified by high-weight data points while in Gradient Boosting, shortcomings of existing weak learners are identified by gradients. Adaboost is more about 'voting weights' and Gradient boosting is more about 'adding gradient optimization'. Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances. AdaBoost use simple stumps as learners, while the fixed size trees of GBM are usually of maximum leaf number between 8 and 32; Adaboost corrects its previous errors by tuning the weights for every incorrect observation in every iteration, but gradient boosting aims at fitting a new predictor in the residual errors committed by the preceding predictor. 3. Random Forest vs GBM GBMs are harder to tune than RF. There are typically three parameters: number of trees, depth of trees and learning rate, and each tree built is generally shallow. RF is harder to overfit than GBM. RF runs in parallel while GBM runs in sequence 4. Application A great application of GBM is anomaly detection in supervised learning settings where data is often highly unbalanced such as DNA sequences, credit card transactions or cybersecurity. 5. Sample Code implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auctrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")train.set_index(\"PassengerId\", inplace=True)test.set_index(\"PassengerId\", inplace=True)# generate training target set (y_train)y_train = train[\"Survived\"]# delete column \"Survived\" from train settrain.drop(labels=\"Survived\", axis=1, inplace=True)train_test = train.append(test)# delete columns that are not used as features for training and predictioncolumns_to_drop = [\"Name\", \"Age\", \"SibSp\", \"Ticket\", \"Cabin\", \"Parch\", \"Embarked\"]train_test.drop(labels=columns_to_drop, axis=1, inplace=True)# convert objects to numbers by pandas.get_dummiestrain_test_dummies = pd.get_dummies(train_test, columns=[\"Sex\"])train_test_dummies.fillna(value=0.0, inplace=True)# generate feature sets (X)X_train = train_test_dummies.values[0:891]X_test = train_test_dummies.values[891:]scaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)X_test_scale = scaler.transform(X_test)X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0)learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]for learning_rate in learning_rates: gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0) gb.fit(X_train_sub, y_train_sub) print(\"Learning rate: \", learning_rate) print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub))) print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub))) print() gb = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 2, random_state = 0)gb.fit(X_train_sub, y_train_sub)predictions = gb.predict(X_validation_sub)print(\"Confusion Matrix:\")print(confusion_matrix(y_validation_sub, predictions))print()print(\"Classification Report\")print(classification_report(y_validation_sub, predictions))y_scores_gb = gb.decision_function(X_validation_sub)fpr_gb, tpr_gb, _ = roc_curve(y_validation_sub, y_scores_gb)roc_auc_gb = auc(fpr_gb, tpr_gb) XGBoost An optimized GBM Evolution of XGBoost from Decision Tree Procedure: For Regression Set initial value (by default is 0.5 [for both regression and classification]) Build the first XGBoost Tree (a unique type of regression tree): Start with a root containing all the residuals ; Compute similarity score where is the Regularization parameter. Make a decision on spliting condition: For each consecutive samples, compute the mean k of 2 input as the threshold for decision node; then split by the condition feature_value &lt; k Decide the best thresold for spliting: Adopt the threshold that gives the largest Gain For example: we have points {}: Step 1: set the first threshold be Step 2: now left node has {x1}, right node has {x2, x3}, we have and Step 3: Compute Step 4: Compute the second threshold and new Gain using this thresold Step 5: Since gain of threshold 1 is greater than that of threshold 2, we use as the spliting threshold If the leaf after spliting has &gt; 1 residual, consider whether to split again (based on the residuals in the leaf); continue until it reaches the max_depth (default is 6) or no more spliting is possible Notes on A larger leads to greater likelihood of prunning as the are lower; The reduce the prediction’s sensitivity to nodes with low # of observations Prune the Tree From bottom branch up, decide on whether to prune the node/branch : The threshold to determine if a Gain is large enough to be kept if then prune (remove the branch); Note that setting does not turn off prunnig!!! If we prune every branch until it reaches the root, then remove the tree; Compute Compute New prediction : Learning rate, default value = 0.3 : Output value of in each residual tree Compute the new residuals for all samples, build the next tree and prune the tree; Repeat the process just like Gradient Boost does; As more trees are built, the Gains will decease; We stop until the Gain &lt; terminating value For Classification Set initial value (by default is 0.5) Build the first XGBoost Tree: Start with a root containing all the residuals Compute similarity score where is the Regularization parameter. Repeat the same procedure as the regression does; Compute all the Gains Warning of Cover: defined for the minimum number of residuals in each leaf (by default is 1) in Regression: = # of Residuals in the leaf (always &gt;= 1) in Classification: = (not necessarily &gt;= 1), hence some leafs violating the Cover threshold will be removed. Here Cover needs to be carefully chosen (like 0, 0.1, etc) Prune the tree: same procedure as Regression case Compute Compute New prediction : Learning rate, default value = 0.3 : Output value of in each residual tree (here is the first tree) Convert into using logistic regression: Compute the new residuals for all samples, build the next tree and prune the tree; Repeat the process just like Gradient Boost does; As more trees are built, the Gains will decease; We stop until the Gain &lt; terminating value; Prediction: ; 1. Advantage of XGBoost Parallelized Tree Building Unlike GBM, XGBoost is able to build the sequential tree using a parallelized implementation This is possible due to the interchangeable nature of loops used for building base learners: the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation. Tree Pruning using depth-first approach The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses 'max_depth' parameter as specified instead of criterion first, and starts pruning trees backward. (This 'depth-first' approach improves computational performance significantly.) Cache awareness and out-of-core computing allocating internal buffers in each thread to store gradient statistics. Further enhancements such as 'out-of-core' computing optimize available disk space while handling big data-frames that do not fit into memory. Regularization It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting. Efficient Handling of missing data XGboost decides at training time whether missing values go into the right or left node. It chooses which to minimise loss. If there are no missing values at training time, it defaults to sending any new missings to the right node. In-built cross-validation capability The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run. LightGBM A follow-up (and competitor) from XGBoost Generally the same as GBM, except that a lot of optimizations are done, see this page to view all of them Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. It is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. 1. Advantages of Light GBM Faster training speed and higher efficiency: Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. Lower memory usage: Replaces continuous values to discrete bins which result in lower memory usage. Better accuracy than any other boosting algorithm: It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter. Compatibility with Large Datasets: It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. Parallel learning supported 2. Code sample: XGBoost vs LightGBM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#importing standard libraries import numpy as np import pandas as pd from pandas import Series, DataFrame #import lightgbm and xgboost import lightgbm as lgb import xgboost as xgb #loading our training dataset 'adult.csv' with name 'data' using pandas data=pd.read_csv('adult.csv',header=None) #Assigning names to the columns data.columns=['age','workclass','fnlwgt','education','education-num','marital_Status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','Income'] #glimpse of the dataset data.head() # Label Encoding our target variable from sklearn.preprocessing import LabelEncoder,OneHotEncoderl=LabelEncoder() l.fit(data.Income) l.classes_ data.Income=Series(l.transform(data.Income)) #label encoding our target variable data.Income.value_counts() #One Hot Encoding of the Categorical features one_hot_workclass=pd.get_dummies(data.workclass) one_hot_education=pd.get_dummies(data.education) one_hot_marital_Status=pd.get_dummies(data.marital_Status) one_hot_occupation=pd.get_dummies(data.occupation)one_hot_relationship=pd.get_dummies(data.relationship) one_hot_race=pd.get_dummies(data.race) one_hot_sex=pd.get_dummies(data.sex) one_hot_native_country=pd.get_dummies(data.native_country) #removing categorical features data.drop(['workclass','education','marital_Status','occupation','relationship','race','sex','native_country'],axis=1,inplace=True) #Merging one hot encoded features with our dataset 'data' data=pd.concat([data,one_hot_workclass,one_hot_education,one_hot_marital_Status,one_hot_occupation,one_hot_relationship,one_hot_race,one_hot_sex,one_hot_native_country],axis=1) #removing dulpicate columns _,i = np.unique(data.columns, return_index=True) data=data.iloc[:, i] #Here our target variable is 'Income' with values as 1 or 0. #Separating our data into features dataset x and our target dataset y x=data.drop('Income',axis=1) y=data.Income #Imputing missing values in our target variable y.fillna(y.mode()[0],inplace=True) #Now splitting our dataset into test and train from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3)#The data is stored in a DMatrix object #label is used to define our outcome variabledtrain=xgb.DMatrix(x_train,label=y_train)dtest=xgb.DMatrix(x_test)#setting parameters for xgboostparameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}#training our model num_round=50from datetime import datetime start = datetime.now() xg=xgb.train(parameters,dtrain,num_round) stop = datetime.now()#Execution time of the model execution_time_xgb = stop-start print(f'execution_time_xgb: {execution_time_xgb}')#datetime.timedelta( , , ) representation $\\implies$ (days , seconds , microseconds) #now predicting our model on test set ypred=xg.predict(dtest) display(ypred)#Converting probabilities into 1 or 0 for i in range(0, 9769): if ypred[i] &gt;= .5: ypred[i] = 1 else: ypred[i]=0 #calculating accuracy of our model from sklearn.metrics import accuracy_score accuracy_xgb = accuracy_score(y_test,ypred) print(f'accuracy_xgb: {accuracy_xgb}')train_data=lgb.Dataset(x_train,label=y_train)#setting parameters for lightgbmparam = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}param['metric'] = ['auc', 'binary_logloss']#Here we have set max_depth in xgb and LightGBM to 7 to have a fair comparison between the two.#training our model using light gbmnum_round=50start=datetime.now()lgbm=lgb.train(param,train_data,num_round)stop=datetime.now()#Execution time of the modelexecution_time_lgbm = stop-startprint(f'execution_time_lgbm: {execution_time_lgbm}')#predicting on test setypred2=lgbm.predict(x_test)display(ypred2[0:5]) # showing first 5 predictions#converting probabilities into 0 or 1for i in range(0,9769): if ypred2[i]&gt;=.5: # setting threshold to .5 ypred2[i]=1 else: ypred2[i]=0 #calculating accuracyaccuracy_lgbm = accuracy_score(ypred2,y_test)print(f'accuracy_lgbm: {accuracy_lgbm}')display(y_test.value_counts())from sklearn.metrics import roc_auc_score#calculating roc_auc_score for xgboostauc_xgb = roc_auc_score(y_test,ypred)print(f'auc_xgb: {auc_xgb}')#calculating roc_auc_score for light gbm. auc_lgbm = roc_auc_score(y_test,ypred2)print(f'auc_lgbm: {auc_lgbm}')comparison_dict = {'accuracy score':(accuracy_lgbm, accuracy_xgb),'auc score':(auc_lgbm,auc_xgb),'execution time':(execution_time_lgbm, execution_time_xgb)}#Creating a dataframe ‘comparison_df’ for comparing the performance of Lightgbm and xgb. comparison_df = DataFrame(comparison_dict) comparison_df.index= ['LightGBM','xgboost'] display(comparison_df) 3. General Pros and cons of boostingPros Achieves higher performance than bagging when hyper-parameters tuned properly. Can be used for classification and regression equally well. Easily handles mixed data types. Can use “robust” loss functions that make the model resistant to outliers. Cons Difficult and time consuming to properly tune hyper-parameters. Cannot be parallelized like bagging (bad scalability when huge amounts of data). More risk of overfitting compared to bagging. ConclusionHere we end the discussion about ensemble models. It was a fun and challenging topic. While most users of these model won't need to understand every nitty-gritty of these models, these profound theories laid significant foundations for future research on supervised ensemble learning models (and even meta-learning). In the next month, I'll share some posts about unsupervised learning. This is even large a topic, and I expect the content to be even deeper. Good luck, me and everyone!","link":"/post/blogs/supervised/ensemble-2/"},{"title":"Regression Models: Logistic Regression","text":"Definition We have a mathematical function which gives a value between and , and to convert it to a value between (0,1), we need a Sigmoid function or a logistic function We can visualize it as a boundary (the decision boundary) to separate 2 categories on a hyperplane, where each dimension is a variable (a certain type of information) The algorithm used is also gradient descent Common Questions What is a logistic function? Answer: . What is the range of values of a logistic function? Answer: The values of a logistic function will range from 0 to 1. The values of Z will vary from to . What are the cost functions of logistic function? Answer: The popular 2 are Cross-entropy or log loss. Note that MSE is not used as squaring sigmoid violates convexity (cause local extrema to appear). Basic Implementation12345678from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionX, y = load_iris(return_X_y=True)clf = LogisticRegression(random_state=2).fit(X, y)clf.predict(X[:2, :])clf.predict_proba(X[:2, :])clf.score(X, y) NotesIn fact, logistic regression is simple, but the key thing here is actually on the mathematics behind gradient descent and its multi-dimensional variations. I'll discuss about them in future posts.","link":"/post/blogs/supervised/regressions-2/"},{"title":"Regression Models: Linear Regression and Regularization","text":"Definition It is used for predicting the continuous dependent variable with the help of independent variables. The goal is to find the best fit line that can accurately predict the output for the continuous dependent variable. The model is usually fit by minimizing the sum of squared errors (OLS (Ordinary Least Square) estimator for regression parameters) Major algorithm is gradient descent: the key is to adjust the learning rate Explanation in layman terms: - provides you with a straight line that lets you infer the dependent variables - estimate the trend of a continuous data by a straight line. using input data to predict the outcome in the best possible way given the past data and its corresponding past outcomes Various RegulationsRegularization is a simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression. Convergence conditions differ note that regularization only apply on variables (hence is not regularized!) L2 norm: Euclidean distance from the origin L1 norm: Manhattan distance from the origin Elastic Net: Mixing L1 and L2 norms Ridge regression: where is cofficient; more widely used as compared to Ridge when number of variables increases Lasso regression: ; better when the data contains suspicious collinear variables Comparison with Logistic Regression Linear Regression: the outcomes are continuous (infinite possible values); error minimization technique is ordinary least square. Logistic Regression: outcomes usually have limited number of possible values; error minimization technique is maximal likelihood. ImplementationsBasic operations using sklearn packages 1234567891011from sklearn.linear_model import LinearRegressionX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])y = np.dot(X, np.array([1, 2])) + 3reg = LinearRegression(normalize=False, fit_intercept = True).fit(X, y)display(reg.score(X, y))display(reg.coef_) # regression coefficientsdisplay(reg.intercept_) # y-intercept / offsetreg.predict(np.array([[3, 5]])) Common Questions Is Linear regression sensitive to outliers? Yes! Is a relationship between residuals and predicted values in the model ideal? No, residuals should be due to randomness, hence no relationship is an ideal property for th model What is the range of learning rate? 0 to 1 Advanced: Analytical solutionsHere let's discuss some more math-intensive stuff. Those who are not interested can ignore this part (though it gives a very important guide on regression models) 1. A detour into Hypothesis representationWe will use to denote the independent variable and to denote dependent variable. A pair of is called training example. The subscripe in the notation is simply index into the training set. We have training example then . The goal of supervised learning is to learn a hypothesis function , for a given training set that can used to estimate based on . So hypothesis fuction represented as where are parameter of hypothesis.This is equation for Simple / Univariate Linear regression. For Multiple Linear regression more than one independent variable exit then we will use to denote indepedent variable and to denote dependent variable. We have independent variable then . The hypothesis function represented as where are parameter of hypothesis, Number of training exaples, Number of independent variable, is training exaple of feature. 2. Matrix FormulationIn general we can write above vector as Now we combine all aviable individual vector into single input matrix of size and denoted it by input matrix, which consist of all training exaples, We represent parameter of function and dependent variable in vactor form as So we represent hypothesis function in vectorize form . 3. Cost functionA cost function measures how much error in the model is in terms of ability to estimate the relationship between and .We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference of observed dependent variable in the given the dataset and those predicted by the hypothesis function. To implement the linear regression, take training example add an extra column that is feature, where . ,where and input matrix will become as Each of the m input samples is similarly a column vector with n+1 rows being 1 for our convenience, that is . Now we rewrite the ordinary least square cost function in matrix form as Let's look at the matrix multiplication concept,the multiplication of two matrix happens only if number of column of firt matrix is equal to number of row of second matrix. Here input matrix of size , parameter of function is of size and dependent variable vector of size . The product of matrix will return a vector of size , then product of will return size of unit vector. 4. Normal EquationThe normal equation is an analytical solution to the linear regression problem with a ordinary least square cost function. To minimize our cost function, take partial derivative of with respect to and equate to . The derivative of function is nothing but if a small change in input what would be the change in output of function. where Now we will apply partial derivative of our cost function, I will throw part away since we are going to compare a derivative to . And solve , Here because of unit vector. Partial derivative , ,, hence this is the normal equation for linear regression. Advanced: Model Evaluation and Model Validation1. Model evaluationWe will predict value for target variable by using our model parameter for test data set. Then compare the predicted value with actual valu in test set. We compute Mean Square Error using formula is statistical measure of how close data are to the fitted regression line. is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it's mean. 100% indicated that model explains all the variablity of the response data around the mean. where = Sum of Square Error, = Sum of Square Total. Here is predicted value and is mean value of .Below is a sample code for evaluation 12345678910111213141516171819202122232425262728# Normal equationy_pred_norm = np.matmul(X_test_0,theta)#Evaluvation: MSEJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]# R_square sse = np.sum((y_pred_norm - y_test)**2)sst = np.sum((y_test - y_test.mean())**2)R_square = 1 - (sse/sst)print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)print('R square obtain for normal equation method is :',R_square)&gt;&gt;&gt; The Mean Square Error(MSE) or J(theta) is: 0.17776161210877062&gt;&gt;&gt; R square obtain for normal equation method is : 0.7886774197617128# sklearn regression moduley_pred_sk = lin_reg.predict(X_test)#Evaluvation: MSEfrom sklearn.metrics import mean_squared_errorJ_mse_sk = mean_squared_error(y_pred_sk, y_test)# R_squareR_square_sk = lin_reg.score(X_test,y_test)print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)print('R square obtain for scikit learn library is :',R_square_sk)&gt;&gt;&gt; The Mean Square Error(MSE) or J(theta) is: 0.17776161210877925&gt;&gt;&gt; R square obtain for scikit learn library is : 0.7886774197617026 The model returns value of 77.95%, so it fit our data test very well, but still we can imporve the the performance of by diffirent technique. Please make a note that we have transformer out variable by applying natural log. When we put model into production antilog is applied to the equation. 2. Model ValidationIn order to validated model we need to check few assumption of linear regression model. The common assumption for Linear Regression model are following Linear Relationship: In linear regression the relationship between the dependent and independent variable to be linear. This can be checked by scatter ploting Actual value Vs Predicted value The residual error plot should be normally distributed. The mean of residual error should be 0 or close to 0 as much as possible The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot. Linear regession assumes that there is little or no *Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor VIF identifies correlation between independent variables and strength of that correlation. , If VIF &gt;1 &amp; VIF &lt;5 moderate correlation, VIF &lt; 5 critical level of multicollinearity. Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern. The model assumption linear regression as follows In our model the actual vs predicted plot is curve so linear assumption fails The residual mean is zero and residual error plot right skewed Q-Q plot shows as value log value greater than 1.5 trends to increase The plot is exhibit heteroscedastic, error will insease after certian point. Variance inflation factor value is less than 5, so no multicollearity. Linearity plot and Residual plot. Q-Q Plot and HomoScedasticity plot","link":"/post/blogs/supervised/regressions-1/"},{"title":"Regression Models: GAM, GLM and GLMM","text":"OverviewGeneralized linear model (GLM) is a cure to some issues posted by ordinary linear regression. In the well-known linear regression model, we often assume . However, it often assumes that is not bounded when is not bounded. However, very often, we must restrict the values of within a fixed range. This may invalidated the ordinary linear model as the function behaviors near the boundary points can be very off. Generalized linear models aim to deal with this issue by allowing for that have arbitrary distributions (not just gaussian distribution), a function of (the link function) to vary linearly with (rather than assuming that a direct linear relationship between and ). Generalized Additive Model (GAM) and Generalized Linear Mixed Model (GLMM) are extensions to GLM with special functions applied to differenet elements in . GLMWe note that GLM has three major parts: An exponential family of probability distributions: , some examples include: normal exponential gamma chi-squared beta Dirichlet Bernoulli categorical Poisson A function of predictor (in GLM it is , in extended models, it can be other things, see GAM and GLMM), we can estimate via maximum likelihood or Bayesian methods like laplace approximation and Gibbs sampling, etc. A link function such that (sometime we may have tractable distribution for variance 1. Pros and Cons for GLM and GLMM Pros: Easy to interpret Easy to grasp Coefficients can be further used in numerical models Easy to extend: link functions, fixed and random effects, correlation structures Cons: Not good for dynamic models (the model is not linear and transformation may not help or would loose information Generalized additive models (GAMs) GAMs are extensions to GLMs in which the linear predictor is not restricted to be linear in the covariates but is the sum of smoothing functions applied to the each . For example, Is useful if relationship between Y and X is likely to be non-linear but we don't have any theory or any mechanistic model to suggest a particular functional form Each is linked with by a smoothing function instead of a coefficient GAMS are data-driven rather than model-driven, that is, the resulting fitted values do not come from an a priori model (non-parametric) All of the distribution families allowed with GLM are available with GAM 1. Pros and Cons for GAM Pros: By combining the basis functions GAMs can represent a large number of functional relationship (to do so they rely on the assumption that the true relationship is likely to be smooth, rather than wiggly) Particularly useful for uncovering nonlinear effects of numerical covariates, and for doing so in an \"automatic\" fashion More Flexible as now each sample's Y is associated with its X by a smoothing function instead of a coefficient Cons: Interpretability of the coefficient need to be estimated graphically Coefficients are not easily transferable to other datasets and parameterization Very sensitive to gaps in the data and outliers Lack underlying theory for the use of hypothesis tests one solution is to do bootstrapping and get aggregated result for more reliable confidence bands 2. Examples of GAM (different predictor representation functions): Loess (Locally weighted regression smoothing) The key factor is the span width (usually set to be a proportion of the data set: 0.5 as a standard starting point) Main idea: Split the data into separate blobs using sliding windows and fit linear regressions in each blob/interval Pros: Easily interpretable. At each test case, a local linear model is fit (eventually explained by linear behaviours) a popular way to see smooth trends on scatterplots Cons: If there are a lot of data points, fitting a LOESS over the entire range of the predictor can be slow because so many local linear regressions must be fit. Regression Splines (piecewise polynomials over usually a finite range) Main constraint is that the splines must remain smooth and continuous at knots To avoid overfitting of splines, penalty terms are added The penalty term also reflects the degree of smoothness in the regression The less smooth the regression is (after fitting the spline functions), the higher the penalty terms Pros: cover all sorts of nonlinear trends and are computationally very attractive because spline terms fit exactly into a least squares linear regression framework. Least squares models are very easy to fit computationally Cons: It is possible to create multidimensional splines by creating interactions between spline terms for different predictors. This suffers from the curse of dimensionality like KNN because we are trying to estimate a wavy surface in a large dimensional (many variable) space where data points will only sparsely cover the many many regions of the space GLMMThe model has the form: where is the design matrix for the random effects (the random complement to the fixed ). is a vector of the random effects (the random complement to the fixed ). The random effects are just deviations around the value in , which is the mean. Usually is a sparse matrix that assigns random effects to each element. We nearly always assume that with being the covariance matrix of the random effects. Assuming that the random effects are independent, we can have being a diagonal matrix with entries and . 1. Code implementationI recommend beginners to use statsmodels package because the output via .summary() function is very clear to read. For advanced users, you may implement the function yourself by referring to the mathematical expressions and package documentations from the following statsmodels: statsmodels.formula.api.mixedlm pymc3 theano pystan tensorflow keras 2. A sample code using statsmodels12345678910111213141516171819202122import statsmodels.formula.api as smffrom patsy import dmatricesformula = \"rt ~ group*orientation*identity\"#formula = \"rt ~ -1 + cbcond\"md = smf.mixedlm(formula, tbltest, groups=tbltest[\"subj\"])mdf = md.fit()print(mdf.summary())fe_params = pd.DataFrame(mdf.fe_params,columns=['LMM'])random_effects = pd.DataFrame(mdf.random_effects)random_effects = random_effects.transpose()random_effects = random_effects.rename(index=str, columns={'groups': 'LMM'})#%% Generate Design Matrix for later useY, X = dmatrices(formula, data=tbltest, return_type='matrix')Terms = X.design_info.column_names_, Z = dmatrices('rt ~ -1+subj', data=tbltest, return_type='matrix')X = np.asarray(X) # fixed effectZ = np.asarray(Z) # mixed effectY = np.asarray(Y).flatten()nfixed = np.shape(X)nrandm = np.shape(Z)","link":"/post/blogs/supervised/regressions-3/"},{"title":"Some Supervised Learning Models","text":"OverviewAlmost everyone who learned about data science or machine learning knows what supervised learning is. However, not many have dived deep into the details of those well-known models. In this blog, I will share some critical aspects of these models (mainly mathematical) that will become helpful in both research and practical work. One note on functionality is that these models work for both regression and classification problems. KNN1. Definition K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions) Non-parametric technique Distance functions can be Euclidean: Manhattan (Or Hamming in the case of Classification): Minkowski: Preprocessing Standardized Distance: One major drawback in calculating distance measures directly from the training set is in the case where variables have different measurement scales or there is a mixture of numerical and categorical variables. The solution is to do standardization on each variable Dimension Reduction: Usually KNN's speed gets much slower when number of attributes increase. Hence we need to reduce the number of dimensions using techniques such as PCA and SVD 2. Choice of K In general, a large K value is more precise as it reduces the overall noise; however, the compromise is that the distinct boundaries within the feature space are blurred (Lower prediction accuracy if K is too large). Need to use cross validation to determine an optimal K 3. Strength and Weakness Advantage The algorithm is simple and easy to implement. There's no need to build a model, tune several parameters, or make additional assumptions. The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section). Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates \"We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset\". Disadvantages The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase. KNN doesn't know which attributes are more import ant Doesn't handle missing data gracefully Slow during prediction (not training) 4. Suitable scenario KNN is bad if you have too many data points and speed is important In ensemble model: k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending When you are solving a problem which directly focusses on finding similarity between observations, K-NN does better because of its inherent nature to optimize locally (i.e: KNN-search) Real Life Example: a simple recommender system (e.g: Given our movies data set, what are the 5 most similar movies to a movie query) 5. Interview Questions Use 1 line to describe KNN Answer: KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression). 6. Simple implementations123456789101112131415161718192021222324252627282930313233343536373839404142434445## For Regressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neighbors import KNeighborsRegressorfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import confusion_matrix,mean_squared_error,accuracy_scorefrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_split##Randomly generate some datadata = pd.DataFrame(np.random.randint(low = 2,high = 100,size = (1000, 4)), columns=[\"Target\",\"A\",\"B\",\"C\"])data.head()train_x,test_x,train_y,test_y = train_test_split(data.iloc[:,1:],data.Target,test_size = 0.2)print(train_x.shape, test_x.shape)scaler = MinMaxScaler(feature_range=(0,1))scaler.fit(train_x)scaled_train_x = pd.DataFrame(scaler.transform(train_x),columns=['A','B','C'])scaled_test_x = pd.DataFrame(scaler.transform(test_x),columns=[\"A\",\"B\",\"C\"])###Basic Performance testingknn_regressor = KNeighborsRegressor(n_neighbors=3,algorithm=\"brute\",weights=\"distance\")knn_regressor.fit(scaled_train_x, train_y)train_pred = knn_regressor.predict(scaled_train_x)test_pred = knn_regressor.predict(scaled_test_x)print(mean_squared_error(train_y,train_pred))print(mean_squared_error(test_y,test_pred))###Grid Search to determine Kknn_regressor = KNeighborsRegressor(algorithm=\"brute\",weights=\"distance\")params = {\"n_neighbors\": [1,3,5],\"metric\": [\"euclidean\", \"cityblock\"]}grid = GridSearchCV(knn_regressor,param_grid=params,scoring=\"neg_mean_squared_error\",cv=5)grid.fit(scaled_train_x, train_y)print(grid.best_params_)print(grid.best_score_)best_knn = grid.best_estimator_train_pred = best_knn.predict(scaled_train_x)test_pred = best_knn.predict(scaled_test_x) SVM1. Definition Normally a binary classification Basic Ideology: SVM is based on the idea of finding a hyperplane that best separates the features into different domains. Both for Regression and classification SVR SVC Support vectors: The points closest to the hyperplane margin maximizing hyperplane: the bound that maximize the distances from support vectors hard margin SVM: If the points are linearly separable then only our hyperplane is able to distinguish between them. Then we have very strict constraints to correctly classify each and every datapoint Soft margin SVM: If the points are not linearly separable then we need an update so that our function may skip few outliers and be able to classify almost linearly separable points. For this reason, we introduce a new Slack variable(ξ) We use CV to determine whether allowing certain amount of misclassification results in better classification in the long run Kernel: used to systematically find the specfic transformation that leads to class separation Polynomial Kernel: where = constant term and = degree of kernel done via Dot Product of a Feature Engineered Matrix Radial basis function kernel (RBF)/ Gaussian Kernel: where = Euclidean distance between &amp; γ: As the value of increases the model gets overfits. As the value of decreases the model underfits For Gaussian kernel: Most Important Idea abut Kernel: Our powerful kernel function actually calculate the high-dimensional relationships WITHOUT actually transforming the data to higher dimensions Multiclass classification: 2 types of strategy One vs. All: N-class instances then N binary classifier models, then pick the prediction of a non-zero class which is the most certain. One-vs-Rest classification One vs. One: N-class instances then N* (N-1)/2 binary classifier models (adopted in SVM). At prediction time, a voting scheme is applied: all C(C−1)/2 classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier. decision_function_shape='ovo' in the parameter to specify one-vs-one, else default is ovr 2. Pros &amp; ConsPros It is really effective in the higher dimension. Its solution is global optimal Effective when the number of features are more than training examples. Great when the data is noise-free and separable Less affected by outliers (if they are not the support vectors) SVM is suited for extreme case binary classification. Cons For larger dataset, it requires a large amount of time to process. Does not perform well in case of overlapped classes Cannot handle categorical data must convert via proper encoding Selection of hyperparameter/Kernel can be difficult resulting boundary plane are very difficult to interpret 3. Application When you need a non-linear approximator, use it When your dataset has a lot of features, use it When the matrix is sparse, use it When the data is unstructured, it is not used 4. Simple Implementation1234567from sklearn.svm import SVCsvc=SVC(kernel='linear') # Choices include 'rbf', 'poly', 'sigmoid'svc.fit(X_train,y_train)y_pred=svc.predict(X_test)print('Accuracy Score:')print(metrics.accuracy_score(y_test,y_pred)) Decision Tree1. Definition Decision Tree is a tree-based model that predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data). use a layered splitting process, where at each layer they try to split the data into two or more groups, so that data that fall into the same group are most similar to each other (homogeneity), and groups are as different as possible from each other (heterogeneity). It apples a top-down approach to data, so that given a data set, DTs try to group and label observations that are similar between them, and look for the best rules that split the observations that are dissimilar between them until they reach certain degree of similarity. Non-parametric technique Pruning: a technique used to deal with overfitting, that reduces the size of DTs by removing sections of the Tree that provide little predictive or classification power. Simpler trees prefered (according to Occam's Razor) Post-prune: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made. 2. Types of DT CHAID (Chi-squared Automatic Interaction Detection) multiway DT chooses the independent variable that has the strongest interaction with the dependent variable. The selection criteria: For regression: F-test For classification: chi-square test Has no pruning function CART (Classification And Regression Tree) binary DT handles data in its raw form (no preprocessing needed), can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables. The selection metric: For Classification: Gini Impurity Index where is the % of data with label in the split The lower value indicates a better spliting For Regression: Least Square Deviation (LSD) the sum of the squared distances (or deviations) between the observed values and the predicted values. Often refered as 'sqaured residual', lower LSD means better split doesn't use an internal performance measure for Tree selection/testing Iterative Dichotomiser 3 (ID3) classification DT Entropy: Single Attribute: Multiple Attribute: where → Current state and → Selected attribute The higher the entropy, the harder it is to draw any conclusions from that information. Follows the rule — A branch with an entropy of zero is a leaf node and A brach with entropy more than zero needs further splitting The selection metric: Information Gain: where is number of splits and is a particular split The higher the gain, the better the split Limitation: it can't handle numeric attributes nor missing values C4.5 The successor of ID3 and represents an improvement in several aspects can handle both continuous and categorical data (regression + classification) can deal with missing values by ignoring instances that include non-existing data The selection metric: Gain ratio: a modification of Information gain that reduces its bias and is usually the best option Windowing: the algorithm randomly selects a subset of the training data (called a \"window\") and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. It captures all the \"rare\" instances together with sufficient \"ordinary\" cases. Can be pruned: pruning method is based on estimating the error rate of every internal node, and replacing it with a leaf node if the estimated error of the leaf is lower. 3. Strength and Weakness Advantage The algorithm is simple and easy to implement. Require very little data preparation The cost of using the tree for inference is logarithmic in the number of data points used to train the tree. Hence the training speed is high Good interpretability. Disadvantages Overfitting is quite common with decision trees simply due to the nature of their training. It's often recommended to perform some type of dimensionality reduction such as PCA so that the tree doesn't have to learn splits on so many features high variance, which means that a small change in the data can result in a very different set of splits, making interpretation somewhat complex. vulnerable to becoming biased to the classes that have a majority in the dataset. It's always a good idea to do some kind of class balancing such as class weights, sampling, or a specialised loss function. In more technical terms: it always look for a greedy option to split, thus more inclined towards a locally optimal split instead of a gloablly optimal one 4. Suitable scenarioConsideration: If the goal is better predictions, we should prefer RF, to reduce the variance. If the goal is exploratory analysis, we should prefer a single DT , as to understand the data relationship in a tree hierarchy structure. If there is a high non-linearity &amp; complex relationship between dependent &amp; independent variables, a tree model will outperform a classical regression method. When computational power is low, DT should be used When important features in the attributes are already identified, DT can be used When you demand more interpretability, DT should be used Use cases: healthcare industry: the screening of positive cases in the early detection of cognitive impairment Environment/Agriculture: DTs are used in agriculture to classify different crop types and identify their phenological stages/recognize different causes of forest loss from satellite imagery Sentiment Analysis: identify emotion from text Finance: Fraud Detection 5. Simple implementations123456789from sklearn import treedt = tree.DecisionTreeClassifier(random_state=1, max_depth=4)dt.fit(data_train, label_train)dt_score_train = dt.score(data_train, label_train) print(\"Training score: \",dt_score_train)dt_score_test = dt.score(data_test, label_test)print(\"Testing score: \",dt_score_test)dt2.predict(data_pred) Naive Bayes1. Definition The Naïve Bayes Classifier belongs to the family of probability classifier, using Bayesian theorem. The reason why it is called 'Naïve' because it requires rigid independence assumption between input variables. The classification formula is simple: Why is it called 'Naive': It is naive because while it uses conditional probability to make classifications, the algorithm simply assumes that all features of a class are independent. This is considered naive because, in reality, it is not often the case. Laplace Smoothing is also applied in some cases to solve the problem of zero probability. Different types of NB: Gaussian: It is used in classification and it assumes that features follow a normal distribution. Multinomial: It is used for discrete counts. For example, let's say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of 'word occurring in the document', we have 'count how often word occurs in the document', you can think of it as 'number of times outcome number x_i is observed over the n trials'. Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with 'bag of words' model where the 1s &amp; 0s are 'word occurs in the document' and 'word does not occur in the document' respectively. You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, \"ensembling, boosting, bagging\" won't help since their purpose is to reduce variance. Naive Bayes has no variance to minimize. 2. Pros &amp; ConsPros It is easy and fast to predict the class of the test data set. It also performs well in multi-class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption). Cons Can't learn the relationship among the features because assumes feature independence the assumption of independent predictors unlikely to hold. In real life, it is almost impossible that we get a set of predictors which are completely independent. 3. Applications Realtime prediction (because it's fast) When Dataset is Huge (high-dimension) When training dataset is small Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments) Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not. 4. Simple Implementation123456789from sklearn.naive_bayes import GaussianNBmodel = GaussianNB()# fit the model with the training datamodel.fit(train_x,train_y)# predict the target on the train datasetpredict_train = model.predict(train_x)print('Target on train data',predict_train)","link":"/post/blogs/supervised/supervised-learning/"},{"title":"Topic Modeling with Latent Dirichlet Allocation","text":"OverviewTopic modeling: Topic modelling refers to the task of identifying topics that best describes a set of documents. In this blog, we discuss about an popular advanced model that Definition To explain in plain word: LDA imagines a fixed set of topics. Each topic represents a set of words. The goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics. An important note to take is that LDA aims to explain the document-level idea, meaning it has less focus on the meaning of each word/phrase in the document, but rather the topic the document falls under Dirichlet Process: A family of stochastic process to produce a probability distribution Used in Bayesian Inference to describe the prior knowledge about the distribution of random variables Dirichlet Distribution: Basically a multivariate generalisation of the Beta distribution: where is a beta distribution Outputs: where Often called “a distribution of distribution” symmetric Dirichlet distribution: a special case in the Dirichlet distribution where all are equal, hence use a single scalar in the model representation Impact of : (a scaling vector for each dimension in ) : Sparsity increases The distribution is likely bowl-shaped (most probable vectors are sparse vectors like or In LDA, it means a document is likely to be represented by just a few of the topics Sparcity decreases We will have a unimodel distribution (most probable vectors are in the center) In LDA, it means a document is likely to contain most of the topics makes documents more similar to each other The conjugate prior of multinomial distribution is a Dirichlet distribution LDA\\s keywords k: Number of topics a document belongs to (a fixed number) V : Size of the vocabulary M: Number of documents N: Number of words in each document w: A word in a document. This is represented as a one hot encoded vector of size V W: represents a document (i.e. vector of \"w\"s) of N words D: Corpus, a collection of M documents z: A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil) θ: The topic distribution for each of the document based on a parameter α β: The Dirichlet distribution based on parameter η LDA's procedure This is quite complicated LDA's document generation α has a topic distribution for each document (θ ground for each document) a (M x K) shape matrix η has a parameter vector for each topic. η will be of shape (k x V) In the above drawing, the constants actually represent matrices, and are formed by replicating the single value in the matrix to every single cell. θ is a random matrix based on dirichlet distribution, where represents the probability of the th document to containing words belonging to the th topic a relatively low β is also a dirichlet distribution as θ, represents the probability of the th topic containing the th word in a vocabulary of size ; The higher the , the more th topic is likely to contain more of the words, and makes the topics more similar to each other Detailed steps: For each topic, draw a distribution over words For each document Draw a vector of topic proportions . E.g: [climate = 0.7, trade = 0.2, housing = 0.1, economy = 0] For each word slot allocated, draw a topic assignment , then draw a word We want to infer the join probability given our observations, We infer the hidden variables or latent factors by observing the corpse of documents, i.e. finding The learning part Idea 1: Gibbs sampling: A point-wise method (Possible but not optimal) Intuition: The setting which generates the original document with the highest proability is the optimal machine The mathematics of collapsed gibbs sampling (cut back version) Recall that when we iterate through each word in each document, we unassign its current topic assignment and reassign the word to a new topic. The topic we reassign the word to is based on the probabilities below. where - number of word assignments to topic in document - number of assignments to topic in document - smoothing parameter (hyper parameter - make sure probability is never 0) - number of words in document - don’t count the current word you’re on - total number of topics - number of assignments, corpus wide, of word to topic - number of assignments, corpus wide, of word to topic - smoothing parameter (hyper parameter - make sure probability is never 0) - sum over all words in vocabulary currently assigned to topic size of vocabulary i.e. number of distinct words corpus wide Done with each word in a document (to classify them into a topic) Done in an iterative way (different topics for same words in a document: 1st \"happy\" may be topic 1, which affects 2nd \"happy\" to be topic 2 in the same document) Main steps: For each word in a document : The word will be allocated to Note that is the one used in the original and in Iterate until each document &amp; word's topic is upadted Aggregate the results from all documents to update the word distribution for each topic Repeat the previous steps until corpus objective converges Idea 2: variational inference: The key concept of variance inference is approximate posterior with a distribution using some known families of distribution that is easy to model and to analyze. Then, we train the model parameters to minimize the KL-divergence between q and p. KL-divergence: ,also called \"relative entropy\" Further reduction in complexity for high dimensional distribution is possible Idea 3: Mean-field variational inference breaks up the joint distribution into distributions of individual variables that are tractable and easy to analyze It is not easy to optimize KL-divergence directly. So let us introduce the Evidence lower bound (ELBO) by maximizing ELBO, we are minimizing KL-divergence: view explanation here When minimizing ELBO, we don’t need Z. No normalization is needed. In contrast, KL’s calculation needs the calculated entity to be a probability distribution. Therefore, we need to compute the normalization factor Z if it is not equal to one. Calculating Z is hard. This is why we calculate ELBO instead of KL-divergence. There are a lot of math details involving exponential family operations, but the general picutre is captured by the graph below Evaluation using similarity query Ok, now that we have a topic distribution for a new unseen document, let's say we wanted to find the most similar documents in the corpus. We can do this by comparing the topic distribution of the new document to all the topic distributions of the documents in the corpus. We use the Jensen-Shannon distance metric to find the most similar documents. What the Jensen-Shannon distance tells us, is which documents are statisically \"closer\" (and therefore more similar), by comparing the divergence of their distributions. Jensen-Shannon is symmetric, unlike Kullback-Leibler on which the formula is based. This is good, because we want the similarity between documents A and B to be the same as the similarity between B and A. The formula is described below. For discrete distirbutions and , the Jensen-Shannon divergence, is defined as where and is the Kullback-Leibler divergence The square root of the Jensen-Shannon divergence is the Jensen-Shannon Distance: The smaller the Jensen-Shannon Distance, the more similar two distributions are (and in our case, the more similar any 2 documents are) Pros &amp; ConsPros An effective tool for topic modeling Easy to understand/interpretable variational inference is tractable θ are document-specific, so the variational parameters of θ could be regarded as the representation of a document , hence the feature set is reduced. z are sampled repeatedly within a document — one document can be associated with multiple topics. Cons Must know the number of topics K in advance Hard to know when LDA is working - topics are soft-clusters so there is no objective metric to say \"this is the best choice\" of hyperparameters LDA does not work well with very short documents, like twitter feeds Dirichlet topic distribution cannot capture correlations among topics Stopwords and rare words should be excluded, so that the model doesnt overcompensate for very frequent words and very rare words, both of which do not contribute to general topics. Real-word application Text classification Book recommender Article clustering/image clustering understanding the different varieties topics in a corpus (obviously) getting a better insight into the type of documents in a corpus (whether they are about news, wikipedia articles, business documents) quantifying the most used / most important words in a corpus document similarity and recommendation. Long Code example123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235# import dependencies%matplotlib inlineimport pandas as pdimport numpy as npimport nltkfrom nltk.corpus import stopwordsimport gensimfrom gensim.models import LdaModelfrom gensim import models, corpora, similaritiesimport refrom nltk.stem.porter import PorterStemmerimport timefrom nltk import FreqDistfrom scipy.stats import entropyimport matplotlib.pyplot as pltimport seaborn as sns# Read in data; only keep essential columns and English language articlesdf = pd.read_csv('lda_fake.csv', usecols = ['uuid','author','title','text','language','site_url','country'])df = df[df.language == 'english']df = df[df['text'].map(type) == str]df['title'].fillna(value=\"\", inplace=True)df.dropna(axis=0, inplace=True, subset=['text'])# shuffle the datadf = df.sample(frac=1.0)df.reset_index(drop=True,inplace=True)# Define some functions to clean and tokenize the datadef initial_clean(text): \"\"\" Function to clean text of websites, email addresess and any punctuation We also lower case the text \"\"\" text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text) text = re.sub(\"[^a-zA-Z ]\", \"\", text) text = text.lower() # lower case the text text = nltk.word_tokenize(text) return textstop_words = stopwords.words('english')def remove_stop_words(text): \"\"\" Function that removes all stopwords from text \"\"\" return [word for word in text if word not in stop_words]stemmer = PorterStemmer()def stem_words(text): \"\"\" Function to stem words, so plural and singular are treated the same \"\"\" try: text = [stemmer.stem(word) for word in text] text = [word for word in text if len(word) &gt; 1] # make sure we have no 1 letter words except IndexError: # the word \"oed\" broke this, so needed try except pass return textdef apply_all(text): \"\"\" This function applies all the functions above into one \"\"\" return stem_words(remove_stop_words(initial_clean(text)))# clean text and title and create new column \"tokenized\"t1 = time.time()df['tokenized'] = df['text'].apply(apply_all) + df['title'].apply(apply_all)t2 = time.time()print(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")# We'll use nltk to get a word frequency (by count) here and only keep the top most used words to train the LDA model on# first get a list of all wordsall_words = [word for item in list(df['tokenized']) for word in item]# use nltk fdist to get a frequency distribution of all wordsfdist = FreqDist(all_words)len(fdist) # number of unique words# choose k and visually inspect the bottom 10 words of the top kk = 50000top_k_words = fdist.most_common(k)top_k_words[-10:]# choose k and visually inspect the bottom 10 words of the top kk = 15000top_k_words = fdist.most_common(k)top_k_words[-10:]# k = 50,000 is too high, as the bottom words aren't even real words and are very rarely used (once in entire corpus)# k = 15,000 is much more reasonable as these have been used at least 13 times in the corpus# define a function only to keep words in the top k wordstop_k_words,_ = zip(*fdist.most_common(k))top_k_words = set(top_k_words)def keep_top_k_words(text): return [word for word in text if word in top_k_words] df['tokenized'] = df['tokenized'].apply(keep_top_k_words)# document lengthdf['doc_len'] = df['tokenized'].apply(lambda x: len(x))doc_lengths = list(df['doc_len'])df.drop(labels='doc_len', axis=1, inplace=True)print(\"length of list:\",len(doc_lengths), \"\\naverage document length\", np.average(doc_lengths), \"\\nminimum document length\", min(doc_lengths), \"\\nmaximum document length\", max(doc_lengths)) # plot a histogram of document lengthnum_bins = 1000fig, ax = plt.subplots(figsize=(12,6));# the histogram of the datan, bins, patches = ax.hist(doc_lengths, num_bins)ax.set_xlabel('Document Length (tokens)', fontsize=15)ax.set_ylabel('Normed Frequency', fontsize=15)ax.grid()ax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))plt.xlim(0,2000)ax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-', label='average doc length')ax.legend()ax.grid()fig.tight_layout()plt.show()### Drop short articlesLDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.From the histogram above, droping all articles less than 40 tokens seems appropriate.# only keep articles with more than 30 tokens, otherwise too shortdf = df[df['tokenized'].map(len) &gt;= 40]# make sure all tokenized items are listsdf = df[df['tokenized'].map(type) == list]df.reset_index(drop=True,inplace=True)print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df), \"articles\")# create a mask of binary valuesmsk = np.random.rand(len(df)) &lt; 0.999train_df = df[msk]train_df.reset_index(drop=True,inplace=True)test_df = df[~msk]test_df.reset_index(drop=True,inplace=True)def train_lda(data): \"\"\" This function trains the lda model We setup parameters like number of topics, the chunksize to use in Hoffman method We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize \"\"\" num_topics = 100 chunksize = 300 dictionary = corpora.Dictionary(data['tokenized']) corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']] t1 = time.time() # low alpha means each document is only represented by a small number of topics, and vice versa # low eta means each topic is only represented by a small number of words, and vice versa lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2) t2 = time.time() print(\"Time to train LDA model on \", len(df), \"articles: \", (t2-t1)/60, \"min\") return dictionary,corpus,lda dictionary,corpus,lda = train_lda(train_df)# show_topics method shows the the top num_words contributing to num_topics number of random topicslda.show_topics(num_topics=10, num_words=20)# select and article at random from train_dfrandom_article_index = np.random.randint(len(train_df))bow = dictionary.doc2bow(train_df.iloc[random_article_index,7])# get the topic contributions for the document chosen at random abovedoc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])# bar plot of topic distribution for this documentfig, ax = plt.subplots(figsize=(12,6));# the histogram of the datapatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)ax.set_xlabel('Topic ID', fontsize=15)ax.set_ylabel('Topic Contribution', fontsize=15)ax.set_title(\"Topic Distribution for Article \" + str(random_article_index), fontsize=20)ax.set_xticks(np.linspace(10,100,10))fig.tight_layout()plt.show()# select and article at random from test_dfrandom_article_index = np.random.randint(len(test_df))print(random_article_index)new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])print(test_df.iloc[random_article_index,3])new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])# bar plot of topic distribution for this documentfig, ax = plt.subplots(figsize=(12,6));# the histogram of the datapatches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)ax.set_xlabel('Topic ID', fontsize=15)ax.set_ylabel('Topic Contribution', fontsize=15)ax.set_title(\"Topic Distribution for an Unseen Article\", fontsize=20)ax.set_xticks(np.linspace(10,100,10))fig.tight_layout()plt.show()def jensen_shannon(query, matrix): \"\"\" This function implements a Jensen-Shannon similarity between the input query (an LDA topic distribution for a document) and the entire corpus of topic distributions. It returns an array of length M where M is the number of documents in the corpus \"\"\" # lets keep with the p,q notation above p = query[None,:].T # take transpose q = matrix.T # transpose matrix m = 0.5*(p + q) return np.sqrt(0.5*(entropy(p,m) + entropy(q,m))) def get_most_similar_documents(query,matrix,k=10): \"\"\" This function implements the Jensen-Shannon distance above and retruns the top k indices of the smallest jensen shannon distances \"\"\" sims = jensen_shannon(query,matrix) # list of jensen shannon distances return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances # this is surprisingly fastmost_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)most_similar_df = train_df[train_df.index.isin(most_sim_ids)]most_similar_df['title']","link":"/post/blogs/supervised/topic-modeling/"},{"title":"","text":"LoRASoft Prompt TuningPrefix TuningAdapterRLHFDPO","link":"/post/blogs/temp/fine-tuning-in-llm/"},{"title":"","text":"","link":"/post/blogs/temp/more-on-model-deployment/"},{"title":"","text":"Feature Engineeringeasy conversion of jupyter notebook1. Jupyter Notebook based-developmentIntuition, develop and test all logic in .ipynb file, then runjupyter nbconvert --to script train_model.ipynb to convert .ipynb file to .py file directly. This is an amazing trick!!! Handling Missing Values Cause of missing The value itself (e.g. certain groups of people) Another variable No reason Handling Deletion By Row By Column Imputation ScalingDiscretizationCategorical Feature Encoding[IMPT] industrially adopted encoding trick - hashing Hash each category New incoming category gets hashed to an existing index Random collision not too bad Significantly resolved “Unknown category” problem","link":"/post/blogs/temp/some_tricks_in_real_world_MLE/"},{"title":"","text":"A typical software testing suite will include: unit tests which operate on atomic pieces of the codebase and can be run quickly during development, regression tests replicate bugs that we’ve previously encountered and fixed, integration tests which are typically longer-running tests that observe higher-level behaviors that leverage multiple components in the codebase, For machine learning systems, we should be running model evaluation and model tests in parallel. Model evaluation covers metrics and plots which summarize performance on a validation or test dataset. Model testing involves explicit checks for behaviors that we expect our model to follow. How do you write model tests? Pre-train test Early bug discovery + training short-circuiting (saves training cost) Things to check: output distribution gradient-related information (training loss curve) data quality label leakage Post-train test post mortem issue discovery and model behavior analysis Things to check: Invariance Test (use a set of perturbations we should be able to make to the input without affecting the model’s output) Directional Expectation Test Data Unit Test (similar to regression test, with failued model scenarios) Organizing tests structuring your tests around the “skills” we expect the model to acquire while learning to perform a given task. Model Dev Pipeline {source: https://www.jeremyjordan.me/testing-ml/}","link":"/post/blogs/temp/testing/"},{"title":"","text":"IntroductionToo many poor code in ml research and ml production 1. Proof-of-Concept Style codeIssue: POC code hard to extend (5 to 10 ops rewritten to complete preprocessing, feature engineering, training, deployment and monitoring) Rationale: Common in Startup, could be fast in the short-term, but detrimental in long-term] Solution: Use library or custom packages for argparse and other argument management. Example: typer, FastAPI 2. No high-level separation of concernsIssue: number of cyclic dependencies present between what seems to be low-level packages and high-level ones increases Rationale: What the ML package is doing (under the name ML lib) also include administrative code, etc Solution: Use Docker and Microservices architecture. Make sure to achieve good distributed system hygiene with middlewares like RabbitMQ, Kafka and Redis 3. No low-level separation of concernsIssue: very bad code structure. no OOP or FP Solution: checkout my code architecture in “/Users/criss_w/Desktop/Research_and_ML/Self_Study/sample_full_stack_ml/model” 4. No configuration Data ModelIssue: Debugging is really a nightmare Solution: Pydantic 5. Handling legacy modelsIssue: When trying to achieve backward compatibility, poor coding structure give much pain Solution: cron, plotly, tmux. Understand basic deployment strategies 6. Code quality: type hinting, documentation, complexity, dead codeSolution: autopep8, flake, mypy, pylint, unittest, pydeps, sourcery","link":"/post/blogs/temp/write-quality-code-for-ml/"},{"title":"Clustering: K Means and Gaussian Mixture Models","text":"OverviewIn this blog we talk about K means and GMM algorithms, the famous and intuitively useful algorithms. As we venture further into unsupervised learning/clustering problems, we will see more interesting problem formulations as well as diverse evaluation metrics. Hope we would enjoy this learning journey along the way :) K means1. Defintion Clustering: A cluster refers to a collection of data points aggregated together because of certain similarities. K-means: an iterative algorithm that tries to partition the dataset into 𝐾 pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group Kmeans gives more weight to the bigger clusters. Kmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn't work well when clusters are in different shapes such as elliptical clusters. Full procedure: Specify number of clusters . Initialize centroids by first shuffling the dataset and then randomly selecting data points for the centroids without replacement. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn't changing. Compute the sum of the squared distance between data points and all centroids. Assign each data point to the closest cluster (centroid). Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster. Expectation-Maximization: The approach kmeans follows to solve the problem is called Expectation-Maximization. The EM algorithm attempts to find maximum likelihood estimates for models with latent variables. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it). The objective function is: where for data point if it belongs to cluster ; otherwise, . Also, is the centroid of 's cluster. It's a minimization problem of two parts. We first minimize J w.r.t. and treat fixed. Then we minimize J w.r.t. and treat fixed. Technically speaking, we differentiate J w.r.t. first and update cluster assignments (E-step). Then we differentiate J w.r.t. and recompute the centroids after the cluster assignments from previous step (M-step). Therefore, E-step is: In other words, assign the data point to the closest cluster judged by its sum of squared distance from cluster's centroid. And M-step is: Which translates to recomputing the centroid of each cluster to reflect the new assignments. Standardization: Since clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it's recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income. Cold start the code may lead to Local optimum: Need to use different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance. Evaluation Method: Contrary to supervised learning where we have the ground truth to evaluate the model's performance, clustering analysis doesn't have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires as an input and doesn't learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different clusters since clusters are used in the downstream modeling. In this notebook we'll cover two metrics that may give us some intuition about : Elbow method Elbow method gives us an idea on what a good number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters' centroids. We pick at the spot where SSE starts to flatten out and forming an elbow. We'll use the geyser dataset and evaluate SSE for different values of and see where the curve might form an elbow and flatten out. Silhouette analysis Silhouette analysis can be used to determine the degree of separation between clusters. For each sample: Compute the average distance from all data points in the same cluster (). Compute the average distance from all data points in the closest cluster (). Compute the coefficient: The coefficient can take values in the interval [-1, 1]. If it is 0 –&gt; the sample is very close to the neighboring clusters. It it is 1 –&gt; the sample is far away from the neighboring clusters. It it is -1 –&gt; the sample is assigned to the wrong clusters. Therefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. We'll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely only two groups of data points. 2. Pros &amp; ConsPros Easy to interpret Relatively fast Scalable for large data sets Able to choose the positions of initial centroids in a smart way that speeds up the convergence Guarantees convergence Cons The globally optimal result may not be achieved The number of clusters must be selected beforehand k-means is limited to linear cluster boundaries: this one may be solved using Similar technique as SVM does. One possible solution is the “Spectral Clustering”: i.e Kernelized K-means below 3. Applications Not to use if it contains heavily overlapping data/full of outliers Not so well if there are many categorical fields Not so well if the clusters have a complicated geometric shapes Real-world samples Market Segmentation Document clustering Image segmentation Image compression 4. Code ImplementationSklearn package's GMM12345from sklearn.cluster import KMeanskm = KMeans(n_clusters=2, max_iter=100)km.fit(X_std)centroids = km.cluster_centers_ GMM1. Definition The Gaussian mixture model (GMM) can be regarded as an optimization of the k-means model. It is not only a commonly used in industry but also a generative model. A model composed of K single Gaussian models. These K submodels are the hidden variables of the hybrid model Single Gaussian model: A Univariate Gaussian Distribution for the data Attempts to find a mixed representation of the probability distribution of the multidimensional Gaussian model, thereby fitting a data distribution of arbitrary shape. Also uses EM algorithm1: if our observations come from a mixture model with mixture components, the marginal probability distribution of is of the form: where is the latent variable representing the mixture component for is the mixture component, and is the mixture proportion representing the probability that belongs to the -th mixture component. Let denote the probability distribution function for a normal random variable. In this scenario, we have that the conditional distribution so that the marginal distribution of is: Similarly, the joint probability of observations is therefore: This note describes the EM algorithm which aims to obtain the maximum likelihood estimates of and given a data set of observations . Likelihood expression is ; Take log, compute and simplify the expected value of the complete log-likelihood: From here, we derive the expressions for each parameter: The EM algorithm, motivated by the two observations above, proceeds as follows: Initialize the 's, 's and 's and evaluate the log-likelihood with these parameters. E-step: Evaluate the posterior probabilities using the current values of the 's and 's with equation (2) M-step: Estimate new parameters and with the current values of using equations (3), (4) and (5). Evaluate the log-likelihood with the new parameter estimates. If the loglikelihood has changed by less than some small , stop. Otherwise, go back to step 2 . The EM algorithm is sensitive to the initial values of the parameters, so care must be taken in the first step. However, assuming the initial values are \"valid\", one property of the EM algorithm is that the log-likelihood increases at every step. This invariant proves to be useful when debugging the algorithm in practice. 2. Pros &amp; ConsPros GMM is a lot more flexible in terms of cluster covariance It is a soft-clustering method, which assign sample membersips to multiple clusters. This characteristic makes it the fastest algorithm to learn mixture models Cons Slower than k-means does not work if the mixture is not really a gaussian distribution It is very sensitive to the initial values which will condition greatly its performance. GMM may converge to a local minimum, which would be a sub-optimal solution. When having insufficient points per mixture, the algorithm diverges and finds solutions with infinite likelihood unless we regularize the covariances between the data points artificially. 3. Application perform GMM when you know that the data points are mixtures of a gaussian distribution if you think that your model is having some hidden, not observable parameters, then you can try to use GMM. 4. Simple codeSklearn package's GMM1234from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components = 3) gmm.fit(X_principal)gmm.fit_predict(X_principal)","link":"/post/blogs/unsupervised/clustering-1/"},{"title":"Clustering: Hierachial, BIRCH and Spectral","text":"Hierachial Clustering1. Definition 2 Main approaches Agglomerative : This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive : This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. Agglomerative Clustering Initially each data point is considered as an individual cluster. At each iteration, the most similar clusters merge with other clusters until 1/ K clusters are formed. No need to specify number of clusters, performance In sklearn, if we specify the number of clusters, performance can be improved Procedure Compute the proximity matrix Let each data point be a cluster Repeat: Merge two closest clusters and update the proximity matrix until 1/ K cluster remains Divisive Clustering Opposite of agglomerative clustering. We start with one giant cluster including all data points. Then data points are separated into different clusters. Similarity score: Basically the proximity between two clusters Distance calculation Euclidean Distance Squared Euclidean Distance Manhattan Distance Maximum Distance: Mahalanobis Distance: where is Covariance matrix For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used. For details, see Distance metrics &amp; Evaluation method[Unsupervised Learning/0. Distance metrics and Evaluation Methods/Distance_Metrics_Evaluation_Methods.ipynb] Distance references Complete-linkage: The maximum distance between elements of each cluster Single-linkage: The minimum distance between elements of each cluster Average linkage: The mean distance between elements of each cluster Ward’s linkage: Minimizes the variance of the clusters being merged. Least increase in total variance around cluster centroids is aimed. 2. Pros &amp; ConsPros Do not have to specify the number of clusters beforehand It is easy to implement and interpretable with the help of dendrograms Always generates the same clusters (Stability) Cons Exponential runtime for larger datasets 3. Application Text grouping: However, it is a highly complex task due the high-dimensionality of data. Social network analysis Outlier detection 4. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.cluster import AgglomerativeClustering from sklearn.preprocessing import StandardScaler, normalizefrom sklearn.decomposition import PCAfrom sklearn.metrics import silhouette_scoreimport scipy.cluster.hierarchy as shcraw_df = pd.read_csv('CC GENERAL.csv')raw_df = raw_df.drop('CUST_ID', axis = 1) raw_df.fillna(method ='ffill', inplace = True) # Standardize datascaler = StandardScaler() scaled_df = scaler.fit_transform(raw_df) # Normalizing the Data normalized_df = normalize(scaled_df) # Converting the numpy array into a pandas DataFrame normalized_df = pd.DataFrame(normalized_df) # Reducing the dimensions of the data pca = PCA(n_components = 2) X_principal = pca.fit_transform(normalized_df) X_principal = pd.DataFrame(X_principal) X_principal.columns = ['P1', 'P2'] plt.figure(figsize =(6, 6)) plt.title('Visualising the data') Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) # Determine the optimal number of clusters using [Silhouette Score]silhouette_scores = []for n_cluster in range(2, 8): silhouette_scores.append( silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) # Plotting a bar graph to compare the results k = [2, 3, 4, 5, 6,7] plt.bar(k, silhouette_scores) plt.xlabel('Number of clusters', fontsize = 10) plt.ylabel('Silhouette Score', fontsize = 10) plt.show() agg = AgglomerativeClustering(n_clusters=3)agg.fit(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = AgglomerativeClustering(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) plt.show() BIRCH Clustering1. Definition Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) Rationale: Existing data clustering methods do not adequately address the problem of processing large datasets with a limited amount of resources (i.e. memory and cpu cycles). In consequence, as the dataset size increases, they scale poorly in terms of running time, and result quality. Main logic of BIRCH: Deals with large datasets by first generating a more compact summary that retains as much distribution information as possible, and then clustering the data summary instead of the original dataset Metric attributes Definition: values can be represented by explicit Euclidean coordinates (no categorical variables). BIRCH can only deal with metric attributes Clustering Features BIRCH summarize the information contained in dense regions as Clustering Feature (CF); where = # of data points in a cluster, = linear sum of data; = square sum of data; CF additivity theorem: ; CF Tree Clustering Feature tree structure is similar to the balanced B+ tree A very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster. Each non-leaf node contains at most entries. Each leaf node contains at most entries, and each entry is a CF Threshold for leaf entry: all sample points in this CF must be in the radius In a hyper-sphere less than T. Insertion Algo: (Insert a new CF/Point entry in to the tree) Starting from the root, recursively traverse down the tree by choosing the node that has shortest Euclidean distance to the inserted entry; Upon reaching a leaf node, find the shorest distance CF and see if it can include the new CF/Point into the cluster without radius threshold violation;If can: do not create a new leaf, but update all the CF triplets on the path, the insertion ends;If cannot: go to 3; If the number of CF nodes of the current leaf node is less than the threshold , create a new CF node, put in a new sample and the new CF node into this leaf node, update all CF triplets on the path, and insertion Ends.Otherwise, go to 4 If the leaf node has &gt; L entires after addition, then split the leaf node by choosing the 2 entries that are farthest apart and redistribute CF based on distance to each of the 2 entries; Modify the path to leaf: Since the leaf node is updated, we need to update the entire path from root to leaf; In the event of split, we need to insert a nonleaf entry into the parent node, and if parent node has &gt; nodes, then we need to split again; do so until it reaches the root Complete procedure Phase 1: The algorithm starts with an initial threshold value (ideally start from low), scans the data, and inserts points into the tree. If it runs out of memory before it finishes scanning the data, it increases the threshold value, and rebuilds a new, smaller CF-tree, by re-inserting the leaf entries of the old CF-tree into the new CF-tree. After all the old leaf entries have been re-inserted, the scanning of the data and insertion into the new CF-tree is resumed from the point at which it was interrupted. (Optional) Filter the CF Tree created in the first step to remove some abnormal CF nodes. (Optional) Use other clustering algorithms such as K-Means to cluster all CF tuples to get a better CF Tree. Phase 2: Given that certain clustering algorithms perform best when the number of objects is within a certain range, we can group crowded subclusters into larger ones resulting in an overall smaller CF-tree. Phase 3: Almost any clustering algorithm can be adapted to categorize Clustering Features instead of data points. For instance, we could use KMEANS to categorize our data, all the while deriving the benefits from BIRCH Additional passes over the data to correct inaccuracies caused by the fact that the clustering algorithm is applied to a coarse summary of the data. The complexity of the algorithm is 2. Pros &amp; ConsPros Save memory, all samples are on disk, CF Tree only stores CF nodes and corresponding pointers. The clustering speed is fast, and it only takes one scan of the training set to build the CF Tree, and the addition, deletion, and modification of the CF Tree are very fast. Noise points can be identified, and preliminary classification pre-processing can be performed on the data set. Cons There is need to specify number of clusters; The clustering result may be different from the real category distribution. Does not perform well on non-convex dataset distribution Apart from number of clusters we have to specify two more parameters; Birch doesn’t perform well on high dimensional data (if there are &gt;20 features, you’d better use something else). 3. Applications If the dimension of the data features is very large, such as greater than 20, BIRCH is not suitable. At this time, Mini Batch K-Means performs better. 4. Code implementation12345678910111213141516import numpy as npfrom matplotlib import pyplot as pltimport seaborn as snssns.set()from sklearn.datasets import make_blobsfrom sklearn.cluster import BirchX, clusters = make_blobs(n_samples=450, centers=6, cluster_std=0.70, random_state=0)plt.scatter(X[:,0], X[:,1], alpha=0.7, edgecolors='b')# Predict and visualizebrc = Birch(branching_factor=50, n_clusters=None, threshold=1.5)brc.fit(X)labels = brc.predict(X)plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow', alpha=0.7, edgecolors='b') Spectral Clustering1. Definition In spectral clustering, data points are treated as nodes of a graph. Thus, spectral clustering is a graph partitioning problem. The nodes are mapped to a low-dimensional space that can be easily segregated to form clusters. No assumption is made about the shape/form of the clusters. The goal of spectral clustering is to cluster data that is connected but not necessarily compact or clustered within convex boundaries. In general, spectral clustering is a generalized version of k-means: it does not assume a circular shape, but apply different affinity functions in its similarity matrix Procedures Project data into matrix Define an Affinity matrix A , using a Gaussian Kernel K or an Adjacency matrix Construct the Graph Laplacian from A (i.e. decide on a normalization) Solve the Eigenvalue problem Select k eigenvectors corresponding to the k lowest (or highest) eigenvalues to define a k-dimensional subspace Form clusters in this subspace using k-means Similarity Graph We first create an undirected graph G = (V, E) with vertex set V = {v1, v2, …, vn} = 1, 2, …, n observations in the data. -neighbourhood Graph: : Each point is connected to all the points which lie in it’s -radius. If all the distances between any two points are similar in scale then typically the weights of the edges (i.e. the distance between the two points) are not stored since they do not provide any additional information. Hence, the graph built is an undirected and unweighted graph. K-Nearest Neighbours: : For two vertices and , an edge is directed from to only if is among the k-nearest neighbours of u. The graph is a weighted and directed graph because it is not always the case that for each u having v as one of the k-nearest neighbours, it will be the same case for v having u among its k-nearest neighbours. To make this graph undirected, one of the following approaches are followed: Direct an edge from u to v and from v to u if either v is among the k-nearest neighbours of u OR u is among the k-nearest neighbours of v. Direct an edge from u to v and from v to u if v is among the k-nearest neighbours of u AND u is among the k-nearest neighbours of v. Fully-Connected Graph: Each point is connected with an undirected edge-weighted by the distance between the two points to every other point. Since this approach is used to model the local neighbourhood relationships thus typically the Gaussian similarity metric is used to calculate the distance: Thus, when we create an adjacency matrix for any of these graphs, when the points are close and if the points are far apart.Consider the following graph with nodes 1 to 4, weights (or similarity) wij and its adjacency matrix: Adjacency Matrix Adjacency Matrix Affinity metric determines how close, or similar, two points our in our space. We will use a Gaussian Kernel and not the standard Euclidean metric. Given 2 data points (projected in ), we define an Affinity that is positive, symmetric, and depends on the Euclidian distance between the data points We might provide a hard cut off threshold , so that if when the points are close in , and if the points , are far apart. Close data points are in the same cluster. Data points in different clusters are far away. But data points in the same cluster may also be far away–even farther away than points in different clusters. Our goal then is to transform the space so that when 2 points , are close, they are always in same cluster, and when they are far apart, they are in different clusters. Generally we use the Gaussian Kernel K directly, or we form the Graph Laplacian . Degree Matrix The degree matrix of a graph is the matrix defined by where of a vertex is the number of edges that terminate at Graph Laplacian The whole purpose of computing the Graph Laplacian was to find eigenvalues and eigenvectors for it, in order to embed the data points into a low-dimensional space. Just another matrix representation of a graph. It can be computed as: Simple Laplacian where is the Adjacency matrix and is the Degree Matrix Normalized Laplacian Generalized Laplacian Relaxed Laplacian Ng, Jordan, &amp; Weiss Laplacian , where The Cluster Eigenspace Problem To identify good clusters, Laplacian should be approximately a block-diagonal, with each block defining a cluster. If we have 3 major clusters (C1, C2, C3), we would expect - We also expect that the 3 lowest eigenvalues &amp; eigenvectors of each correspond to a different cluster. - For K clusters, compute the first K eigen vectors. . Stack the vectors vertically to form the matrix with eigen vecttors as columns. Represent every node as the corresponding row of this new matrix, these rows form the feature vector of the nodes. Use Kmeans to cluster these points into k clusters 2. Pros &amp; ConsPros Clusters not assumed to be any certain shape/distribution, in contrast to e.g. k-means. This means the spectral clustering algorithm can perform well with a wide variety of shapes of data. Works quite well when relations are approximately transitive (like similarity) Do not necessarily need the actual data set, just similarity/distance matrix, or even just Laplacian Because of this, we can cluster one dimensional data as a result of this; other algos that can do this are k-medoids and heirarchical clustering. Cons Need to choose the number of clusters k, although there is a heuristic to help choose Can be costly to compute, although there are algorithms and frameworks to help 3. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import SpectralClustering from sklearn.preprocessing import StandardScaler, normalize from sklearn.decomposition import PCA from sklearn.metrics import silhouette_scoreraw_df = pd.read_csv('CC GENERAL.csv')raw_df = raw_df.drop('CUST_ID', axis = 1) raw_df.fillna(method ='ffill', inplace = True) # Preprocessing the data to make it visualizable # Scaling the Data scaler = StandardScaler() X_scaled = scaler.fit_transform(raw_df) # Normalizing the Data X_normalized = normalize(X_scaled) # Converting the numpy array into a pandas DataFrame X_normalized = pd.DataFrame(X_normalized) # Reducing the dimensions of the data pca = PCA(n_components = 2) X_principal = pca.fit_transform(X_normalized) X_principal = pd.DataFrame(X_principal) X_principal.columns = ['P1', 'P2'] ## Affinity matrix with Gaussian Kernel## affinity = \"rbf\"# Building the clustering model spectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf') # Training the model and Storing the predicted cluster labels labels_rbf = spectral_model_rbf.fit_predict(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = SpectralClustering(n_clusters = 2, affinity ='rbf') .fit_predict(X_principal), cmap =plt.cm.winter) plt.show() ## Affinity matrix with Eucledean Distance## affinity = ‘nearest_neighbors’# Building the clustering model spectral_model_nn = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') # Training the model and Storing the predicted cluster labels labels_nn = spectral_model_nn.fit_predict(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') .fit_predict(X_principal), cmap =plt.cm.winter) plt.show() ### Evaluate performance# List of different values of affinity affinity = ['rbf', 'nearest-neighbours'] # List of Silhouette Scores s_scores = [] # Evaluating the performance s_scores.append(silhouette_score(raw_df, labels_rbf)) s_scores.append(silhouette_score(raw_df, labels_nn)) # Plotting a Bar Graph to compare the models plt.bar(affinity, s_scores) plt.xlabel('Affinity') plt.ylabel('Silhouette Score') plt.title('Comparison of different Clustering Models') plt.show() print(s_scores)","link":"/post/blogs/unsupervised/clustering-2/"},{"title":"Clustering: DBSCAN","text":"DBSCAN Introduction Density-based spatial clustering of applications with noise (DBSCAN) Summary: DBSCAN is a density-based clustering method that discovers clusters of nonspherical shape. Main Concept: Locate regions of high density that are separated from one another by regions of low density. It also marks as outliers the points that are in low-density regions. Implicit assumptions about the method: Densities across all the clusters are the same. Cluster sizes or standard deviations are the same. Density of region: Mainly defined by 2 parameters Density at a point P: Number of points within a circle of Radius from point P. Dense Region: For each point in the cluster, a circle with radius contains at least minimum number of points () The Epsilon neighborhood of a point P in the database D is defined as The function is usually defined by Euclidean Distance 3 classification of points: Core point: if the point has Border point: if the point has but it lies in the neighborhood of another Core point. Noise: any data point that is neither Core nor Border point Density Reachable/Density Connected/Directly Density Reachable Directly Density Reachable: Data-point is directly density reachable from a point if is a core point is in the epsilon neighborhood of Density Reachable: Data-point is density reachable from a point if For a chain of points , , is directly density reachable from . Density reachable is transitive in nature but, just like direct density reachable, it is not symmetric Density Connected: Data-point is density connected to a point if with respect to and there is a point such that, both and are density reachable from w.r.t. to and Procedure Starts with an arbitrary point which has not been visited and its neighborhood information is retrieved from the parameter. If this point contains neighborhood points, cluster formation starts.Otherwise the point is labeled as noise.- This point can be later found within the neighborhood of a different point and, thus can be made a part of the cluster. If a point is found to be a core point then the points within the neighborhood is also part of the cluster. So all the points found within neighborhood are added, along with their own neighborhood, if they are also core points. Continue the steps above (1-3) until the density-connected cluster is completely found. The process restarts with a new point which can be a part of a new cluster or labeled as noise. 2. Pros &amp; ConsPros Identifies randomly shaped clusters doesn’t necessitate to know the number of clusters in the data previously (as opposed to K-means) Handles noise Cons If the database has data points that form clusters of varying density, then DBSCAN fails to cluster the data points well, since the clustering depends on ϵ and MinPts parameter, they cannot be chosen separately for all clusters May Overcome this issue by running additional rounds over large clusters If the data and features are not so well understood by a domain expert then, setting up and could be tricky Computational complexity — when the dimensionality is high, it takes 3. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport numpy as npfrom scipy import statsfrom sklearn.cluster import DBSCANfrom sklearn.metrics import silhouette_score# To choose the best combination of the algorithm parameters I will first create a matrix of investigated combinations.from itertools import productmall_data = pd.read_csv('Mall_Customers.csv')X_numerics = mall_data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']] # subset with numeric variables onlyeps_values = np.arange(8,12.75,0.25) # eps values to be investigatedmin_samples = np.arange(3,10) # min_samples values to be investigatedDBSCAN_params = list(product(eps_values, min_samples))no_of_clusters = []sil_score = []for p in DBSCAN_params: DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_numerics) no_of_clusters.append(len(np.unique(DBS_clustering.labels_))) sil_score.append(silhouette_score(X_numerics, DBS_clustering.labels_))tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples']) tmp['No_of_clusters'] = no_of_clusterspivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')fig, ax = plt.subplots(figsize=(12,6))sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)ax.set_title('Number of clusters')plt.show()tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples']) tmp['Sil_score'] = sil_scorepivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')fig, ax = plt.subplots(figsize=(18,6))sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)plt.show()DBS_clustering = DBSCAN(eps=12.5, min_samples=4).fit(X_numerics)DBSCAN_clustered = X_numerics.copy()DBSCAN_clustered.loc[:,'Cluster'] = DBS_clustering.labels_ # append labels to pointsDBSCAN_clust_sizes = DBSCAN_clustered.groupby('Cluster').size().to_frame()DBSCAN_clust_sizes.columns = [\"DBSCAN_size\"]display(DBSCAN_clust_sizes)outliers = DBSCAN_clustered[DBSCAN_clustered['Cluster']==-1]fig2, (axes) = plt.subplots(1,2,figsize=(12,5))sns.scatterplot('Annual Income (k$)', 'Spending Score (1-100)', data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1], hue='Cluster', ax=axes[0], palette='Set1', legend='full', s=45)sns.scatterplot('Age', 'Spending Score (1-100)', data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1], hue='Cluster', palette='Set1', ax=axes[1], legend='full', s=45)axes[0].scatter(outliers['Annual Income (k$)'], outliers['Spending Score (1-100)'], s=5, label='outliers', c=\"k\")axes[1].scatter(outliers['Age'], outliers['Spending Score (1-100)'], s=5, label='outliers', c=\"k\")axes[0].legend()axes[1].legend()plt.setp(axes[0].get_legend().get_texts(), fontsize='10')plt.setp(axes[1].get_legend().get_texts(), fontsize='10')plt.show()","link":"/post/blogs/unsupervised/clustering-3/"},{"title":"Clustering: Affinity Propagation","text":"Affinity Propagation Introduction Developed recently (2007), a centroid based clustering algorithm similar to k Means or K medoids Affinity propagation finds \"exemplars\" i.e. members of the input set that are representative of clusters. It uses a graph based approach to let points 'vote' on their preferred 'exemplar'. The end result is a set of cluster 'exemplars' from which we derive clusters by essentially doing what K-Means does and assigning each point to the cluster of it's nearest exemplar. We need to calculate the following matrices: Here we must specify to notation: = row, = column Similarity matrix Responsibility matrix Availability matrix Criterion matrix Similarity matrix Rationale: information about the similarity between any instances, for an element i we look for another element j for which is the highest (least negative). Hence the diagonal values are all set to the most negative to exclude the case where i find i itself Barring those on the diagonal, every cell in the similarity matrix is calculated by the negative sum of the squares differences between participants. Note that the diagonal values will not be just 0: It is Responsibility matrix Rationale: quantifies how well-suited element k is, to be an exemplar for element i , taking into account the nearest contender k’ to be an exemplar for i. We initialize R matrix with zeros. Then calculate every cell in the responsibility matrix using the following formula: Interpretation: R_{i,k} can be thought of as relative similarity between i and k. It quantifies how similar is i to k, compared to some k’, taking into account the availability of k’. The responsibility of k towards i will decrease as the availability of some other k’ to i increases. Availability matrix Rationale: It quantifies how appropriate is it for i to choose k as its exemplar, taking into account the support from other elements that k should an exemplar. The Availability formula for different instances is The Self-Availability is Interpretation of the formulas Availability is self-responsibility of k plus the positive responsibilities of k towards elements other than i. We include only positive responsibilities as an exemplar should be positively responsible/explain at least for some data points well, regardless of how poorly it explains other data points. If self-responsibility is negative, it means that k is more suitable to belong to another exemplar, rather than being an exemplar. The maximum value of is 0. reflects accumulated evidence that point k is suitable to be an exemplar, based on the positive responsibilities of k towards other elements. and matrices are iteratively updated. This procedure may be terminated after a fixed number of iterations, after changes in the values obtained fall below a threshold, or after the values stay constant for some number of iterations. Criterion Matrix Criterion matrix is calculated after the updating is terminated. Criterion matrix is the sum of and : An element i will be assigned to an exemplar k which is not only highly responsible but also highly available to i. The highest criterion value of each row is designated as the exemplar. Rows that share the same exemplar are in the same cluster. Sample run Data Similarity Matrix Responsibility Matrix (First round) Availability Matrix (First round) Criterion Matrix 2. Pros &amp; ConsPros Does not need to specify the cluster number Allows for non-metric dissimilarities (i.e. we can have dissimilarities that don't obey the triangle inequality, or aren't symmetric) Providebetter stability over runs Cons Similar issue as K-means: susceptible to outliers Affinity Propagation tends to be very slow. In practice running it on large datasets is essentially impossible without a carefully crafted and optimized implementation 3. Code Implementation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from sklearn.cluster import AffinityPropagationfrom sklearn import metricsfrom sklearn.datasets import make_blobs# ############################################################################## Generate sample datacenters = [[1, 1], [-1, -1], [1, -1]]X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)# ############################################################################## Compute Affinity Propagationaf = AffinityPropagation(preference=-50).fit(X)cluster_centers_indices = af.cluster_centers_indices_labels = af.labels_n_clusters_ = len(cluster_centers_indices)print('Estimated number of clusters: %d' % n_clusters_)print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels, metric='sqeuclidean'))# ############################################################################## Plot resultimport matplotlib.pyplot as pltfrom itertools import cycleplt.close('all')plt.figure(1)plt.clf()colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')for k, col in zip(range(n_clusters_), colors): class_members = labels == k cluster_center = X[cluster_centers_indices[k]] plt.plot(X[class_members, 0], X[class_members, 1], col + '.') plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14) for x in X[class_members]: plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)plt.title('Estimated number of clusters: %d' % n_clusters_)plt.show()","link":"/post/blogs/unsupervised/clustering-4/"},{"title":"Clustering: Apriori","text":"Association RuleAssociation rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. For example, we may want to find 1-1 product category assocaition rule: product cateogry 1 -&gt; product category 2 This is often used for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. Because we don't have initial associations in our data, it is an unsupervised learning problem for marketing activities such as, e.g., promotional pricing or product placements. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. [Wikipedia] Evaluation Metrics1 Support : % of transactions where items in X AND Y are bought together Property of down-ward closure which means that all sub sets of a frequent set (support &gt; min. support threshold) are also frequent Cons: Items that occur very infrequently in the data set are pruned although they would still produce interesting and potentially valuable rules. Confidence : % of transactions amongst all customers who bought Y given that they have bought X While support is used to prune the search space and only leave potentially interesting rules, confidence is used in a second step to filter rules that exceed a min. confidence threshold Cons: sensitive to the frequency of the consequent (Y) in the data set. Caused by the way confidence is calculated, Ys with higher support will automatically produce higher confidence values even if they exists no association between the items. Lift An association rule X -&gt; Y is only useful if the lift value &gt; 1 Want to consider also the presence of Y being bought independently without knowledge about X Largely solves to problem of confidence threshold: sensitive to the frequency of the consequent (Y) Conviction : How poor can the association be. A directed measure monotone in confidence and lift. Leverage : difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically independent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells. Cons: suffer from the rare item problem. Apriori PropertyAll subsets of a frequent itemset must be frequent (Apriori propertry). If an itemset is infrequent, all its supersets will be infrequent. Applying the apriori property, we get the following algorithm. Algorithm Generating Support Value for Itemsets containing one items (One Itemset) With a pre-defined support threshold, identify itemsets worth exploring With the shortlisted One Itemset that are above the support threshold, generate Itemsets containing two items (Two Itemsets) With the same pre-definited support threshold, identify associations in Two Itemsets that are worth exploring With the shortlisted Two Itemsets, association rule is generated between the two items Confidence value is generated for each association rule With a pre-defined confidence threshold, association rules are being shortlisted With shortlisted association rules, the lift values are computed for each of them Only association rules with lift value &gt; 1 is considered as meaningful associations","link":"/post/blogs/unsupervised/clustering-5/"},{"title":"Unsupervised Learning: Measures about Clustering","text":"OverviewUnsupervised learning is a vast topic, and clustering is really a big part (yet not all) of it. Whenever we have some ideas about clusteirng, we should first ask: is this idea comparable to some existing works? Now to answer this, we need some evaluation strategies and choose measures for such evaluations. This is what today's blog will talk about. Distance MetricsWe have four most popular distance metrics outlined below. In essense, one should understand the structure of each metric, and when to use them. Minkowski Distance: Minkowski distance is a metric in Normed vector space. Formula p = 1, Manhattan Distance p = 2, Euclidean Distance p = ∞, Chebychev Distance Manhattan Distance: We use Manhattan Distance if we need to calculate the distance between two data points in a grid like path. Euclidean Distance: Euclidean distance formula can be used to calculate the distance between two data points in a plane. Cosine Distance: Mostly Cosine distance metric is used to find similarities between different documents. In cosine metric we measure the degree of angle between two documents/vectors(the term frequencies in different documents collected as metrics). This particular metric is used when the magnitude between vectors does not matter but the orientation. Formula: Evaluation Methods1. Clustering Tendency Before evaluating the clustering performance, making sure that data set we are working has clustering tendency and does not contain uniformly distributed points is very important. Hopkins Test: a statistical test for spatial randomness of a variable, can be used to measure the probability of data points generated by uniform data distribution. Null Hypothesis () : Data points are generated by non-random, uniform distribution (implying no meaningful clusters) Alternate Hypothesis (): Data points are generated by random data points (presence of clusters) If the H value is between {0.01, …,0.3}, the data is regularly spaced. If the H value is around 0.5, it is random. If the H value is between {0.7, …, 0.99}, it has a high tendency to cluster. The Hopkins Test is highly influenced by outliers Sample Code123456789101112131415161718import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)from sklearn.decomposition import PCAfrom sklearn import datasetsfrom sklearn.preprocessing import scalefrom pyclustertend import hopkins ## the hopkins testfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltheart_df = pd.read_csv(\"heart.csv\")X = heart_df[heart_df.columns[~heart_df.columns.isin([\"target\"])]].valuesy = heart_df[heart_df.columns[heart_df.columns.isin([\"target\"])]].values.flatten()display(hopkins(X, X.shape[0]))display(hopkins(scale(X),X.shape[0])) 2. Number of Optimal ClustersMainly 2 Direction Domain knowledge — Domain knowledge might give some prior knowledge on finding number of clusters. For example, in case of clustering iris data set, if we have the prior knowledge of species (sertosa, virginica, versicolor) , then k = 3. Domain knowledge driven k value gives more relevant insights. Data driven approach — If the domain knowledge is not available, mathematical methods help in finding out right number of clusters. Mainly 2 Methods Statistical approach: Gap statistic is a powerful statistical method to find the optimal number of clusters, k. Sum of within-cluster (intra-cluster) variance is calculated for different values of k. : Sum-of-within-Cluster variance of original data set for k clusters : Sum-of-within-cluster variance of reference data set (null reference data set of uniform distribution) of k clusters Formula: As Gap statistic quantifies this deviation, More the Gap statistic means more the deviation. Cluster number with maximum Gap statistic value corresponds to optimal number of cluster. Sample Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Gap Statistics%matplotlib inlineimport timeimport hashlibimport scipyimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom sklearn.cluster import KMeansfrom sklearn.datasets.samples_generator import make_blobsplt.rcParams['figure.figsize'] = 10, 10x, y = make_blobs(750, n_features=2, centers=12)plt.scatter(x[:, 0], x[:, 1])plt.show()def optimalK(data, nrefs=3, maxClusters=15): \"\"\" Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie Params: data: ndarry of shape (n_samples, n_features) nrefs: number of sample reference datasets to create maxClusters: Maximum number of clusters to test for Returns: (gaps, optimalK) \"\"\" gaps = np.zeros((len(range(1, maxClusters)),)) resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]}) for gap_index, k in enumerate(range(1, maxClusters)): # Holder for reference dispersion results refDisps = np.zeros(nrefs) # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop for i in range(nrefs): # Create new random reference set randomReference = np.random.random_sample(size=data.shape) # Fit to it km = KMeans(k) km.fit(randomReference) refDisp = km.inertia_ refDisps[i] = refDisp # Fit cluster to original data and create dispersion km = KMeans(k) km.fit(data) origDisp = km.inertia_ # Calculate gap statistic gap = np.log(np.mean(refDisps)) - np.log(origDisp) # Assign this loop's gap statistic to gaps gaps[gap_index] = gap resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True) return (gaps.argmax() + 1, resultsdf) # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal k, gapdf = optimalK(x, nrefs=5, maxClusters=15)print(f'Optimal k is: {k}') Elbow method: Within-cluster variance is a measure of compactness of the cluster. Lower the value of within cluster variance, higher the compactness of cluster formed. Sample Code12345678910111213141516171819202122232425262728293031# Elbow Methodimport matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn.datasets.samples_generator import (make_blobs, make_circles, make_moons)from sklearn.cluster import KMeans, SpectralClusteringfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import silhouette_samples, silhouette_score# Import the datadf = pd.read_csv('old_faithful.csv')# Standardize the dataX_std = StandardScaler().fit_transform(df)sse = []list_k = list(range(1, 10))for k in list_k: km = KMeans(n_clusters=k) km.fit(X_std) sse.append(km.inertia_)# Plot sse against kplt.figure(figsize=(6, 6))plt.plot(list_k, sse, '-o')plt.xlabel(r'Number of clusters $k$')plt.ylabel('Sum of squared distance') 3. Clustering QualityThere are majorly two types of measures to assess the clustering performance. For more details, check sklearn document on cluster performance evaluation. Extrinsic Measures: Require ground truth labels. Examples are Adjusted Rand index, Fowlkes-Mallows scores, Mutual information based scores, Homogeneity, Completeness and V-measure. Intrinsic Measures: Does not require ground truth labels. Examples are Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index etc. 123456789101112131415161718192021222324252627282930313233343536373839404142434445# silhouette analysisfor i, k in enumerate([2, 3, 4]): fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) # Run the Kmeans algorithm km = KMeans(n_clusters=k) labels = km.fit_predict(X_std) centroids = km.cluster_centers_ # Get silhouette samples silhouette_vals = silhouette_samples(X_std, labels) # Silhouette plot y_ticks = [] y_lower, y_upper = 0, 0 for i, cluster in enumerate(np.unique(labels)): cluster_silhouette_vals = silhouette_vals[labels == cluster] cluster_silhouette_vals.sort() y_upper += len(cluster_silhouette_vals) ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1) ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1)) y_lower += len(cluster_silhouette_vals) # Get the average silhouette score and plot it avg_score = np.mean(silhouette_vals) ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green') ax1.set_yticks([]) ax1.set_xlim([-0.1, 1]) ax1.set_xlabel('Silhouette coefficient values') ax1.set_ylabel('Cluster labels') ax1.set_title('Silhouette plot for the various clusters', y=1.02); # Scatter plot of data colored with labels ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels) ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250) ax2.set_xlim([-2, 2]) ax2.set_xlim([-2, 2]) ax2.set_xlabel('Eruption time in mins') ax2.set_ylabel('Waiting time to next eruption') ax2.set_title('Visualization of clustered data', y=1.02) ax2.set_aspect('equal') plt.tight_layout() plt.suptitle(f'Silhouette analysis using k = {k}', fontsize=16, fontweight='semibold', y=1.05);","link":"/post/blogs/unsupervised/unsupervised-learning/"}],"tags":[{"name":"Software Engineering","slug":"Software-Engineering","link":"/tags/Software-Engineering/"},{"name":"Python Project","slug":"Python-Project","link":"/tags/Python-Project/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Data Mining&#x2F;Data Engineering","slug":"Data-Mining-Data-Engineering","link":"/tags/Data-Mining-Data-Engineering/"},{"name":"Big Data","slug":"Big-Data","link":"/tags/Big-Data/"},{"name":"Distributed System","slug":"Distributed-System","link":"/tags/Distributed-System/"},{"name":"Cloud Computing","slug":"Cloud-Computing","link":"/tags/Cloud-Computing/"},{"name":"Database System","slug":"Database-System","link":"/tags/Database-System/"},{"name":"Data Analytics","slug":"Data-Analytics","link":"/tags/Data-Analytics/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"Statistics","slug":"Statistics","link":"/tags/Statistics/"},{"name":"A&#x2F;B testing","slug":"A-B-testing","link":"/tags/A-B-testing/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"System Design","slug":"System-Design","link":"/tags/System-Design/"},{"name":"Matrix Computation","slug":"Matrix-Computation","link":"/tags/Matrix-Computation/"},{"name":"Dimensionality Reduction","slug":"Dimensionality-Reduction","link":"/tags/Dimensionality-Reduction/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","link":"/tags/Unsupervised-Learning/"},{"name":"Distributed Systems","slug":"Distributed-Systems","link":"/tags/Distributed-Systems/"},{"name":"Distributed Training","slug":"Distributed-Training","link":"/tags/Distributed-Training/"},{"name":"Project Management","slug":"Project-Management","link":"/tags/Project-Management/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Hidden Markov Models","slug":"Hidden-Markov-Models","link":"/tags/Hidden-Markov-Models/"},{"name":"Dynamic Programming","slug":"Dynamic-Programming","link":"/tags/Dynamic-Programming/"},{"name":"Statistical Inference","slug":"Statistical-Inference","link":"/tags/Statistical-Inference/"},{"name":"Representaiton Learning","slug":"Representaiton-Learning","link":"/tags/Representaiton-Learning/"},{"name":"Bayesian Statistics","slug":"Bayesian-Statistics","link":"/tags/Bayesian-Statistics/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Control Theory","slug":"Control-Theory","link":"/tags/Control-Theory/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Recommender Systems","slug":"Recommender-Systems","link":"/tags/Recommender-Systems/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Ensemble","slug":"Ensemble","link":"/tags/Ensemble/"},{"name":"Boosting","slug":"Boosting","link":"/tags/Boosting/"},{"name":"Supervised Learning","slug":"Supervised-Learning","link":"/tags/Supervised-Learning/"},{"name":"Bagging","slug":"Bagging","link":"/tags/Bagging/"},{"name":"Random Forest","slug":"Random-Forest","link":"/tags/Random-Forest/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Regularization","slug":"Regularization","link":"/tags/Regularization/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Clustering","slug":"Clustering","link":"/tags/Clustering/"},{"name":"Evaluation Methods","slug":"Evaluation-Methods","link":"/tags/Evaluation-Methods/"}],"categories":[{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"Full Stack Web Dev","slug":"Projects/Full-Stack-Web-Dev","link":"/categories/Projects/Full-Stack-Web-Dev/"},{"name":"Java","slug":"Projects/Java","link":"/categories/Projects/Java/"},{"name":"C++","slug":"Projects/C","link":"/categories/Projects/C/"},{"name":"Frontend Development","slug":"Projects/Frontend-Development","link":"/categories/Projects/Frontend-Development/"},{"name":"C++ 17","slug":"Projects/C-17","link":"/categories/Projects/C-17/"},{"name":"GPT","slug":"Projects/GPT","link":"/categories/Projects/GPT/"},{"name":"PyTorch","slug":"Projects/PyTorch","link":"/categories/Projects/PyTorch/"},{"name":"Recommendation System","slug":"Projects/Recommendation-System","link":"/categories/Projects/Recommendation-System/"},{"name":"Software","slug":"Software","link":"/categories/Software/"},{"name":"Node.js","slug":"Projects/Full-Stack-Web-Dev/Node-js","link":"/categories/Projects/Full-Stack-Web-Dev/Node-js/"},{"name":"Blogs","slug":"Blogs","link":"/categories/Blogs/"},{"name":"GUI","slug":"Projects/Java/GUI","link":"/categories/Projects/Java/GUI/"},{"name":"Python","slug":"Projects/C/Python","link":"/categories/Projects/C/Python/"},{"name":"JavaScript","slug":"Projects/Frontend-Development/JavaScript","link":"/categories/Projects/Frontend-Development/JavaScript/"},{"name":"Static Code Analysis","slug":"Projects/C-17/Static-Code-Analysis","link":"/categories/Projects/C-17/Static-Code-Analysis/"},{"name":"Backend Devlopment","slug":"Projects/Java/Backend-Devlopment","link":"/categories/Projects/Java/Backend-Devlopment/"},{"name":"Backend Web Dev","slug":"Projects/GPT/Backend-Web-Dev","link":"/categories/Projects/GPT/Backend-Web-Dev/"},{"name":"Diffusion","slug":"Projects/PyTorch/Diffusion","link":"/categories/Projects/PyTorch/Diffusion/"},{"name":"Full Stack Web Dev","slug":"Projects/Recommendation-System/Full-Stack-Web-Dev","link":"/categories/Projects/Recommendation-System/Full-Stack-Web-Dev/"},{"name":"React","slug":"Projects/Full-Stack-Web-Dev/Node-js/React","link":"/categories/Projects/Full-Stack-Web-Dev/Node-js/React/"},{"name":"Healthcare","slug":"Projects/Java/GUI/Healthcare","link":"/categories/Projects/Java/GUI/Healthcare/"},{"name":"CUDA","slug":"Projects/C/Python/CUDA","link":"/categories/Projects/C/Python/CUDA/"},{"name":"React","slug":"Projects/Frontend-Development/JavaScript/React","link":"/categories/Projects/Frontend-Development/JavaScript/React/"},{"name":"Testing","slug":"Projects/C-17/Static-Code-Analysis/Testing","link":"/categories/Projects/C-17/Static-Code-Analysis/Testing/"},{"name":"Microservices","slug":"Projects/Java/Backend-Devlopment/Microservices","link":"/categories/Projects/Java/Backend-Devlopment/Microservices/"},{"name":"Go","slug":"Projects/GPT/Backend-Web-Dev/Go","link":"/categories/Projects/GPT/Backend-Web-Dev/Go/"},{"name":"Deep Learning","slug":"Projects/PyTorch/Diffusion/Deep-Learning","link":"/categories/Projects/PyTorch/Diffusion/Deep-Learning/"},{"name":"Java","slug":"Projects/Recommendation-System/Full-Stack-Web-Dev/Java","link":"/categories/Projects/Recommendation-System/Full-Stack-Web-Dev/Java/"},{"name":"MongoDB","slug":"Projects/Full-Stack-Web-Dev/Node-js/React/MongoDB","link":"/categories/Projects/Full-Stack-Web-Dev/Node-js/React/MongoDB/"},{"name":"Deep Learning System","slug":"Projects/C/Python/CUDA/Deep-Learning-System","link":"/categories/Projects/C/Python/CUDA/Deep-Learning-System/"},{"name":"Node.js","slug":"Projects/Frontend-Development/JavaScript/React/Node-js","link":"/categories/Projects/Frontend-Development/JavaScript/React/Node-js/"},{"name":"CI&#x2F;CD","slug":"Projects/C-17/Static-Code-Analysis/Testing/CI-CD","link":"/categories/Projects/C-17/Static-Code-Analysis/Testing/CI-CD/"},{"name":"MySQL","slug":"Projects/GPT/Backend-Web-Dev/Go/MySQL","link":"/categories/Projects/GPT/Backend-Web-Dev/Go/MySQL/"},{"name":"Autonomous Vehicle","slug":"Projects/PyTorch/Diffusion/Deep-Learning/Autonomous-Vehicle","link":"/categories/Projects/PyTorch/Diffusion/Deep-Learning/Autonomous-Vehicle/"},{"name":"D3.js","slug":"Projects/Frontend-Development/JavaScript/React/Node-js/D3-js","link":"/categories/Projects/Frontend-Development/JavaScript/React/Node-js/D3-js/"},{"name":"Deployment","slug":"Projects/GPT/Backend-Web-Dev/Go/MySQL/Deployment","link":"/categories/Projects/GPT/Backend-Web-Dev/Go/MySQL/Deployment/"},{"name":"AWS","slug":"Projects/GPT/Backend-Web-Dev/Go/MySQL/Deployment/AWS","link":"/categories/Projects/GPT/Backend-Web-Dev/Go/MySQL/Deployment/AWS/"}],"pages":[{"title":"404","text":"Maybe try clicking on the icon at top left?","link":"/404.html"},{"title":"Research","text":"Characterizing Out-of-Distribution Error via Optimal Transport Yuzhe Lu, Yilong Qin, Runtian Zhai, Andrew Shen, Ketong Chen, Zhenlin Wang, Soheil Kolouri, Simon Stepputtis, Joseph Campbell, Katia Sycara NeurIPS 2023 Main Conference[paper][code] Predicting Out-of-Distribution Error with Confidence Optimal Transport Yuzhe Lu, Zhenlin Wang, Runtian Zhai, Soheil Kolouri, Joseph Campbell, Katia Sycara. ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML[paper][code] Best Arm Identification with Safety Constraints Zhenlin Wang, Andrew Wagenmaker, Kevin Jamieson AISTATS 2022 Main Conference[paper] Max-min Grouped Bandits Zhenlin Wang, Jonathan Scarlett AAAI 2022 Main Conference[paper][poster] An Information-Theoretic Approach for Distributionally Robust Bayesian Optimization Thesis for NUS B.S. in Applied Mathematics[paper][slides]","link":"/research/index.html"}]}