{"posts":[{"title":"A brief Intro to A&#x2F;B Testing","text":"IntroductionA/B testing is a type of hypothesis testing commonly used in business. It is mainly useful on for products that are mature and is suitable for fast iterative product development. The main idea is to assume a new modification is useful and take trials/experiments upon the modified product. Next, using the old version as a reference, see how significant is the improvement brought by the new modification. Note: When we say new modification, it must be warned that the change should contain only one factor, otherwise the influence can be compounded. 10 Steps in A/B Testing First time trying something new: run an A/A testing simultaneously to check for systematic biases Systematic Bias: is sampling error that stems from the way in which the research is conducted. There are three types: a. Selection bias: Biased way to select the factors/samples. b. Non-response bias: the participants/customers involved in your tests are different in their behavioral patterns from the people that are not involved in your study (other general public) c. Response bias: The result/inference that are given does not follow the truth/real observations. Define the goal and form hypothesis Null hypothesis : a general statement or default position that there is no relationship between two measured phenomena. Often claimed to be the case when there is no association between the performance of product and the a feature change of the product. If we believe otherwise, then we are arguing for a Alternative hypothesis . Identify control and treatment groups Identify KPI/metrics to measure e.g: click through rate, conversion rate, renewal rate, bounce rate, average retention, etc… Identify what data needs to be collected Sometime people use the Customer Funnel analysis: credit: https://clevertap.com/blog/funnel-analysis. Example: [Netflix - Funnel Description] Customer will enter the home page, which invites customer to further enter via the button free trial for one month, then customer will try and then register and pay Example: [Netflix - Funnel Analysis]Funnel analysis: here the funnel will be how the product will affect each steps like improving converted customer size or improving returning customer size, during the procedure to reach click through/subscription, what side-effects/main effects are triggered Make sure that appropriate logging is in place to collect all necessary data Determine how small of a difference can be (define significance level and thus power of the experiment) Significance Level: or p-value, is the probability that we reject the null hypothesis while it is true (type I error). Power: is the probability of rejecting the null hypothesis while it is false (type II error). Determine what fraction of visitors should be in the treatment group (control/treatment split ratio) Run a power analysis to decie how much data is needed to collet and how long to run the test compute running time given customer flow Run the test for AT LEAST this running time long Before you run A/B testing…1. Why should you run a A/B test? A/B testing is the key to understand what drives your business and make data-informed business decisions To understand the causal relationship and not simply the correlations 2. When to run experiments Deciding whether or not to launch a new product or feature To quantify the impact of a feature or product Compare data with intuition (Understand how users respond to certain parts of a product) 3. When not to run experiments No clear comparison between the control and experimental group Emotional changes need time: Logo/Brand name Response data hard to obtain Too time consuming/costly A closer look at Type I and Type II errors1. Terminology significance level: The significance level for a given hypothesis test is a value for which a p-value less than or equal to is considered statistically significant. Often denoted by α region of acceptance: The range of values that leads the researcher to accept the null hypothesis. [significance level↓ region of acceptance↑ : harsher condition to produce a winner] effective size: The difference between the true value and the value specified in the null hypothesis. [Effect size = True value - Hypothesized value] 2. Type 1 Error When the Null Hypothesis is true but rejected 'false positive' - happen when the tester validates a statistically significant difference even though there isn’t one (no winner situation). positive here means a valid winner concluded Type 1 errors have a probability of \"α\" correlated to the level of confidence that you set. A test with a 95% confidence level (α = 0.05) means that there is a 5% chance of getting a type 1 error. [In business sense] This means that you will wrongfully assume that your hypothesis testing has worked even though it hasn’t. [The business consequence] Potential loss of money after adjustment is made, because your variation didn’t actually beat your control version in the long run. 3. Type 2 Error When the Null Hypothesis is false but accepted 'false negative' - Happens when you inaccurately assume that no winner has been declared between a control version and a variation although there actually is a winner (can be either) The probability of a type 2 error is \"β\". β depends on the power of the test (i.e the probability of not committing a type 2 error, which is equal to 1-β). There are 3 parameters that can affect the power of a test: Sample size (n): Other things being equal, the greater the sample size, the greater the power of the test (but also the more expensive of the test). Significance level of test (α): The lower the significance level, the lower the power of the test. If you reduce the significance level (e.g., from 0.05 to 0.01), the region of acceptance gets bigger. As a result, you are less likely to reject the null hypothesis. This means you are less likely to reject the null hypothesis when it is false, so you are more likely to make a Type II error. In short, the power of the test is reduced when you reduce the significance level; and vice versa. The \"true\" value of your tested parameter or effective size: The greater the difference between the \"true\" value of a parameter and the value specified in the null hypothesis, the greater the power of the test. [The business consequence] Potential loss of money after adjustment is made, because your variation didn't actually beat your control version in the long run Toolkits Usually the test plan/create variation step can be executed with company's own techpack, or using some popular tools Google optimize optimizely For hypo testing and result analysis: online resources of excel's A/B testing macro are widely available","link":"/post/AB-testing/"},{"title":"Zhenlin Wang (Criss)","text":"Welcome to my personal website! I'm Zhenlin Wang. You can call me Criss if it's easier for you. I'm currently studying for a Master of Machine Learning degree in Carnegie Mellon University. Before coming to the US, I completed my undergraduate study at National University of Singapore (NUS). My academic and practical interests lie in the intersection of data science, machine learning and software engineering, and I've been actively seeking for SDE/MLE internships recently. If you are interested to know more about me as a person, you can find some information [here]. What's in my websiteThis is a space to drop down some notes about my life. I started maintaining this website from my sophomore year in NUS. The original version used a minimal-mistake style and I've migrated into the hexo-icarus style on Aug 2022. There are several sections in this space: 1. Technical BlogsBlogs about various topics in DS/AI/ML and software engineering are written here. The mathematical/technical details are presented. I often included some discussion about the pros/cons of the methodologies outlined in these blogs. Nonetheless, I never believe that any blog post can be &quot;DONE&quot; as new perspectives on these topics can always supplement what's on the posts. Thus, I consistently update these posts whenever I learn some new knowledge about the topics discussed in these posts. Disclaimer: I try my best to give credit for all sources I made referenecs to. If you found some parts in my post that were referenced from your work without credit, please kindly contact me so I can immediately correct my mistakes and make apology. 2. Experience and skill setMy project experiences are listed here for record, many are driven by the interests to explore tech/skills I never knew. I'm eager to understand the basics of various technology toolkits and learn new stuff along the way. 3. Anime and EnlightenmentThese posts will be more of a casual type. I watch anime once in a while, especially those classical series that touched me deeply in heart. I've decided to reflect upon reviewing these series and drop them down here. Sometimes I meditate ((aka. emo)) or read. Now that I think it's a good idea to take note of them so that I won't waste time repeating myself during my meditations. Special thanksIn retrospect, I did make various references to many websites/blogs when building this version of personal space. I've got to thank @Lei Mao for the inspiration to build a website from scratch. I followed him since 2018 when I read his background story and found it so motivating. Learning from his blog styles, I built my first version of website and the journey of bloghing started afterwards. When trying to customizing the pages, I learnt so much from @iMaeGoo, @Xinyu Liu and @Pengyuan Li. I should thank them for their great tutorials on website styling.","link":"/post/Zhenlin%20Wang/"},{"title":"A fundamental course for Data Engineering","text":"OverviewIn this blog, I'll discuss about a variety of fundamental concepts in data processing. While remembering them is pointless (since the true data engineers learn by practice), I try to give a clear view of different aspects in handling the data, this includes: Data Storage Where data is stored Data ETL Extracting / Transforming / Loading Data Properties Source types and information system types Data Processing Types How different data are processed differently Data Cleansing What if the data is not well-organized Database Compliance ACID or BASE? Data Storage Data lake: a centralized repository that allows you to store structured, semistructured, and unstructured data at any scale. Single source of truth Store any type of data, regardless of structure Example: AWS Data Lake: Harness the power of purpose-built analytic services for a wide range of use cases, such as interactive analysis, data processing using Apache Spark and Apache Hadoop, data warehousing, real-time analytics, operational analytics, dashboards, and visualizations. Data warehouse: A data warehouse is a central repository of structured data from many data sources. This data is transformed, aggregated, and prepared for business reporting and analysis. ETL (Extract, Transform, Load) operations before stored into data warehouse Data is stored within the data warehouse using a schema. Data Mart: A subset of data from a data warehouse is called a data mart. Data marts only focus on one subject or functional area. A Data Warehouse might contain all relevant sources for an enterprise, but a Data Mart might store only a single department’s sources. Because data marts are generally a copy of data already contained in a data warehouse, they are often fast and simple to implement. Traditional data warehousing: pros and cons Pros Cons Fast data retrieval Costly to implement Curated data sets Maintenance can be challenging Centralized storage Security concerns Better business intelligence Hard to scale to meet demand Data Warehouse vs Data Lake Factors Data warehouse Data Lake Data Relational from transactional systems, operational databases, and line of business applications Non-relational and relational from IoT devices, websites, mobile apps, social media, and corporate applications Schema Designed prior to implementation (schema-on-write) Written at the time of analysis (schema-on-read) Price/performance Fastest query, higher cost storage Fast, low-cost storage Data quality Highly curated data that serves as the central version of the truth Any data, which may or may not be curated (e.g., raw data) Users Business analysts Data scientists, data developers, and BA Analytics Batch reporting, BI, and visualizations Machine learning, predictive analytics, data discovery, and profiling. ETL Basics Extracting data There are four key areas you must plan for. You must identify \"where\" all of the source data resides. This may be data stored on-premises by your company but can also include data that is found online. You must carefully plan \"when\" the extraction will take place due to the potential impact of the copy process on the source system. You must plan for \"where\" the data will be stored during processing. This is generally referred to as a staging location. You must plan for \"how often\" the extraction must be repeated. Transforming data This phase involves using a series of rules and algorithms to massage the data into its final form. Data cleansing also occurs during this part of the process. It can be basic or advanced: This could be replacing NULL values with a zero or replacing the word female with the letter F; Or applying business rules to the data to calculate new values. Filtering, complex join operations, aggregating rows, splitting columns Loading data The planning steps you took in the transformation phase will dictate the form the final data store must take. This could be a database, data warehouse, or data lake. Data Properties Data Source: In each of these data sources, data is stored in a specific way. Some data sources use a schema to organize content and indexes to improve performance. Others organize data in a more flexible way and are called schemaless. Schemaless data sources still use indexes to improve performance. Types of data source: Structured data: stored in a tabular format, often within a database management system (DBMS). Organized based on a relational data model Defines and standardize data elements The downside to structured data is its lack of flexibility: you must reconfigure the schema to allow for this new data, and you must account for all records that don’t have a value for this new field. Semistructured data (NoSQL) Stored in the form of elements within a file. (CSV, JSON, XML, etc) Organized based on elements and the attributes that define them. No pre-defined schemas. Semistructured data is considered Have a self-describing structure: Each element is a single instance of a thing, such as a conversation. The attributes within an element define the characteristics of that conversation. Each conversation element can track different attributes. The trade-off is with analytics. It can be more difficult to analyze semistructured data when analysts cannot predict which attributes will be present in any given data set. Unstructured data Stored in the form of files. This data doesn't conform to a predefined data model and isn't organized in a predefined manner. Can be text-heavy, photographs, audio recordings, or even videos. Need to be preprocessed to perform meaningful analysis. Types of information systems There are two main ways—known as information systems—of organizing data within a relational database. The data can be organized to focus on the storage of transactions or the process of analyzing transactions. Online transaction processing (OLTP) databases: operational databases, primary focus being on the speed of data entry These databases are characterized by a large number of insert, update, and delete operations. based on ensuring rapid data entry and updates. The effectiveness of an OLTP system is often measured by the number of transactions per second. Online analytical processing (OLAP) databases: data warehouses, primary focus being the speed of data retrieval through queries. These databases are characterized by a relatively low number of write operations and the lack of update and delete operations based on the types of queries and other analytics that will be performed using the data. The effectiveness of an OLAP system is often measured by the response time of query results. OLTP VS OLAP: Characteristic OLTP OLAP Nature Constant transactions (queries/updates) Periodiclarge updates, complex queries Examples Accounting database, online retail transactions Reporting, decision support Type Operational data Consolidated data Data retention Short-term (2-6 months) Long-term (2-5 years) Storage Gigabytes (GB) Terabytes (TB)/petabytes (PB) Users Many Few Protection Robust, constant data protection and fault tolerance Periodic protection Processing Types Categories and types: By Collection: Batch: Velocity is very predictable with batch processing. It amounts to large bursts of data transfer at scheduled intervals. Periodic: Velocity is less predictable with periodic processing. The loss of scheduled events can put a strain on systems and must be considered. Near real-time: Velocity is a huge concern with near real-time processing. These systems require data to be processed within minutes of the initial collection of the data. This can put tremendous strain on the processing and analytics systems involved. Real-time: Velocity is the paramount concern for real-time processing systems. Information cannot take minutes to process. It must be processed in seconds to be valid and maintain its usefulness. By Processing: Batch and periodic: Once the data has been collected, processing can be done in a controlled environment. There is time to plan for the appropriate resources. Near real-time and real-time: Collection of the data leads to an immediate need for processing. Depending on the complexity of the processing (cleansing, scrubbing, curation), this can slow down the velocity of the solution significantly. Plan accordingly. Data acceleration Another key characteristic of velocity on data is data acceleration, which means the rate at which large collections of data can be ingested, processed, and analyzed. Data acceleration is not constant. It comes in bursts. Take Twitter as an example. Hashtags can become hugely popular and appear hundreds of times in just seconds, or slow down to one tag an hour. That's data acceleration in action. Your system must be able to efficiently handle the peak of hundreds of tags a second and the lows of one tag an hour. Attributes of batch and stream processing Batch data processing Stream data processing Data scope over all or most of the data over data within a rolling time window, or on just the most recent data record Data size Large batches of data Individual records/ micro batches consisting of a few records Latency Minutes to hours Seconds or milliseconds Analysis Complex analytics Simple response functions, aggregates, and rolling metrics Processing big data streams There are many reasons to use streaming data solutions. In a batch processing system, processing is always asynchronous, and the collection system and processing system are often grouped together. With streaming solutions, the collection system (producer) and the processing system (consumer) are always separate. Streaming data uses what are called data producers. Each of these producers can write their data to the same endpoint, allowing multiple streams of data to be combined into a single stream for processing. Another huge advantage is the ability to preserve client ordering of data and the ability to perform parallel consumption of data. This allows multiple users to work simultaneously on the same data. Data Cleansing Curation: the action or process of selecting, organizing, and looking after the items in a collection. Data integrity: the maintenance and assurance of the accuracy and consistency of data over its entire lifecycle. Different types of integrity: Referential integrity: the process of ensuring that the constraints of table relationships are enforced. Domain integrity: the process of ensuring that the data being entered into a field matches the data type defined for that field.. Entity integrity: the process of ensuring that the values stored within a field match the constraints defined for that field. Maintaining Integrity accross steps of a data lifecycle: Creation phase: Ensure data accuracy. Mainly involves software audits/data generation audits/data Aggregation phase: Ensure the metrics computed are well-defined. Bad practice such as poor naming of metrics Storage phase: Ensure stable data are not changed and volatile data are only changed by authorized personels Access phase: System should be read-only and audited regularly for anomalies in access pattern Share pahse: The phase where veracity get truly examined Archive phase: Security of the data is the most important factor. Ensure limited access and read-only Data veracity: the degree to which data is accurate, precise, and trusted. A few best practices to help you identify data integrity issues Know what clean looks like Before you do anything else, you must come to a consensus on what clean looks like. Some businesses deem clean data to be data in its raw format with business rules applied. Some businesses deem clean data as data that has been normalized, aggregated, and had value substitutions applied to regulate all entries. These are two very different understandings of clean. Be sure to know which one you are aiming for. Know where the errors are coming from As you find errors in the data, trace them back to their likely source. This will help you to predict workloads that will have integrity issues. Know what acceptable changes look like From a purely data-centric view, entering a zero in an empty column may seem like an easy data cleansing decision to make, but beware the effects of this change. Know if the original data has value In some systems, the original data is no longer valuable once it has been transformed. However, in highly regulated data or highly volatile data, it is important that both the original data and the transformed data are maintained in the destination system. Database Schemas A data schema is the set of metadata used by the database to organize data objects and enforce integrity constraints. The schema defines attributes of the database, providing descriptions of each object and how it interacts with other objects within the database. One or more schemas can reside on the same database. Logical schemas: focus on the constraints to be applied to the data within the database. This includes the organization of tables, views, and integrity checks. Physical schemas: focus on the actual storage of data on disk or in a cloud repository. These schemas include details on the files, indices, partitioned tables, clusters, and more. Information Schemas: An information schema is a database of metadata that houses information on the data objects within a database. Given the proper permissions on the database, you can query the information schema to learn about the objects within the database. When queries are executed, this information is used to ensure the best optimization for the query. The information schema can also be used in maintenance of the database itself. Database Compliance ACID: ACID is an acronym for Atomicity, Consistency, Isolation, and Durability Atomicity: Ensures that your transactions either completely succeed or completely fail. No one statement can succeed without the others Consistency: Ensures that all transactions provide valid data to the database. If any single statement violates these checks, the whole transaction will be rolled back Isolation: Ensures that one transaction cannot interfere with another concurrent transaction Data durability: Ensures your changes actually stick. Once a transaction has successfully completed, durability ensures that the result of the transaction is permanent even in the event of a system failure. This means that all completed transactions that result in a new record or update to an existing record will be written to disk and not left in memory. Mainly to ensure veracity in a structured database The goal of an ACID-compliant database is to return the most recent version of all data and ensure that data entered into the system meets all rules and constraints that are assigned at all times. BASE: BASE is an acronym for Basically Available Soft state Eventually Consistent. Basically Available: allows for one instance to receive a change request and make that change available immediately. The system will always guarantee a response for every request. However, it is possible that the response may be a failure or stale data, if the change has not been replicated to all nodes. Soft State: In a BASE system, there are allowances for partial consistency across distributed instances. For this reason, BASE systems are considered to be in a soft state, also known as a changeable state. Eventually Consistency: The data will be eventually consistent. In other words, a change will eventually be made to every copy. However, the data will be available in whatever state it is during propagation of the change. BASE supports data integrity in non-relational databases This consistency is mostly concerned with the rapid availability of data To ensure the data is highly available, changes to data are made available immediately on the instance where the change was made. However, it may take time for that change to be replicated across the fleet of instances. ACID vs BASE: ACID BASE Strong consistency Weak consistency – stale data is OK Isolation is key Availability is key Focus on committed results Best effort results Conservative (pessimistic) availability Aggressive (optimistic) availability","link":"/post/big-data-guidelines/"},{"title":"Apache Spark: Only the simple answer","text":"OverviewIn this post, I'm just gonna discuss some fundational things I learned about big data with Apache Spark. Personally, I'm just a bit interested in this topic, and do not aim to really become a big data professional (not yet~). It does take tremendous effort to learn Spark well, not to mention the entire big data ecosystem. I'll update this post if I try out some new projects that really apply Spark and its APIs in a deep manner, but for now, let's just talk about some basics of Spark. Apache Spark vs Hadoop MapReduce MapReduce is a programming model, Spark is a processing framework Apache Spark MapReduce Processing Type Process in batches and in real-time Process in batches only Speed nearly 100x faster slower due to large scale data processing Storage store data in RAM i.e. in-memeory (easier to retrieve) Store in HDFS, longer time to retrieve Memory dependence caching (for RDD) and in-memory data storage disk-dependent Important Components of Spark Ecosystem Core components[MOST IMPORTANT]: Spark Core (Caching, RDD, DataFrames, Datasets || Transformations and Actions) Memory management Fault recovery Task dispatching Scheduling and monitoring jobs Spark SQL (Data Query) Used for structured and semi-structured data processing Usually used for in-memory processing Top level: DataFrame DSL(domain specific language) ; Spark SQl and HQL(Hive) Level 2: DataFrame API Level 3: Data Source API Base Level: CSV + JSON + JDBC(Java Database connectivity) + etc storage/query Spark Streaming (Stream data) Spark MLlib (Machine Learnig toolkits) GraphX (Graph Processing models) Langauge Support Java Python Scala R Spark Cluster Managers Standalone mode: Default choice, run in FIFO order, and each application will try to use all available nodes Apache YARN (Hadoop Integration): This is the resource manager of Hadoop, use this will help spark to connect to hdfs better Kubernetes: For deployment, scaling and management of containerized applications RDD Resilient Distributed Datasets RDDs are immutable, fault-tolerant distributed collections of objects that can be operated in parallel (split into partition and executed on different nodes of a cluster) 2 major types of operations: Transformation: map, filter, join, union, etc. Yield a new RDD containing the result Action: reduce, count, first, etc. Return a value after running a computation on RDD Works in a similar style as java Stream How Spark runs applications with the help of its architectureSTART EXECUTION In driver program Spark applications runs as independent processes (i.e. split tasks) running across different machines Spark sessions/context as the entry point of the application Driver: Record the flow of the application Resource manager/Cluster manager (DAG Scheduler at the backend) The driver program request resources from the clusters Assign task to workers, one task per partition Knows each step of the application for in-memory processing Worker node processing slave (node manager) grands the request to usage of resources from resource manager The request is called Container, within the Container, executor process is launched to apply tasks to its unit of work to the dataset in its partition and outputs a new partition dataset because iterative algorithms apply operations repeated to the data, the benefit from caching datasets across iterations END EXECUTION Results are sent back: worker node -&gt; container -&gt; manager -&gt; driver program/disk The entire execution is lazily evaluated (transformations not evaluated until an action is called) What is a Parquet file and what are its advantages Parquet is a columnar format that is supported byh several data processing systems (default data type for spark) Advisable to use if not all fields/columns in the data are used Advantages able to fetch specific columns for access consumes less space follow type-specific encoding limited I/O operations What is shuffling in Spark? When does it occur? Shuffling is the process of redistributing data across partitions that may lead to data movement across executors Occurs while joining two tables or while performing byKey operations such as GroupByKey or ReduceByKey Notes on Big Data Learning Journey (for those who truly want a Big Data job and for my future )To excel in the Big Data domain, you should master the following skills: Java &amp; Scala Understand source code for related package/API development Linux Everyone should know about shell scripts, bash and linux commands Hadoop It's a broad topic, but first of all, the ability to read whatever source code for an API is a must Hive Know how to use it, understand how the SQL is converted in base code and how to optimize the query process or MapReduce/Spark Operations Spark The core developement process (But honestly, most of the time it is still SQL) Kafka High-volumn stream data processing; Good to use when you have high concurrency Flink Faster than Spark sometimes. However, you should not discard Spark. Learn based on what you need. HBase Know your database knowledge. Understand its fundamental knowledge Zookeeper Distributation cluster data coordination services; Know how to use, better to understand the basic YARN Cluster resources management; Know how to use Sqoop, Flume, Oozie/Azkaban Know how to use Different cluster managers Spark Standalone mode by default, applications submitted to the standalone mode cluster will run in FIFO order, and each application will try to use all available nodes Apache Mesos an open sources project to manage computer clusters, and can also run Hadoop applications YARN Apache YARN is the cluster resource manager of Hadoop 2. Kubernetes an open-source system for automating deployment, scaling and management of containerized applications","link":"/post/big-data-spark/"},{"title":"Clustering: K Means and Gaussian Mixture Models","text":"OverviewIn this blog we talk about K means and GMM algorithms, the famous and intuitively useful algorithms. As we venture further into unsupervised learning/clustering problems, we will see more interesting problem formulations as well as diverse evaluation metrics. Hope we would enjoy this learning journey along the way :) K means1. Defintion Clustering: A cluster refers to a collection of data points aggregated together because of certain similarities. K-means: an iterative algorithm that tries to partition the dataset into 𝐾 pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group Kmeans gives more weight to the bigger clusters. Kmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn't work well when clusters are in different shapes such as elliptical clusters. Full procedure: Specify number of clusters . Initialize centroids by first shuffling the dataset and then randomly selecting data points for the centroids without replacement. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn't changing. Compute the sum of the squared distance between data points and all centroids. Assign each data point to the closest cluster (centroid). Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster. Expectation-Maximization: The approach kmeans follows to solve the problem is called Expectation-Maximization. The EM algorithm attempts to find maximum likelihood estimates for models with latent variables. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it). The objective function is: where for data point if it belongs to cluster ; otherwise, . Also, is the centroid of 's cluster. It's a minimization problem of two parts. We first minimize J w.r.t. and treat fixed. Then we minimize J w.r.t. and treat fixed. Technically speaking, we differentiate J w.r.t. first and update cluster assignments (E-step). Then we differentiate J w.r.t. and recompute the centroids after the cluster assignments from previous step (M-step). Therefore, E-step is: In other words, assign the data point to the closest cluster judged by its sum of squared distance from cluster's centroid. And M-step is: Which translates to recomputing the centroid of each cluster to reflect the new assignments. Standardization: Since clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it's recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income. Cold start the code may lead to Local optimum: Need to use different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance. Evaluation Method: Contrary to supervised learning where we have the ground truth to evaluate the model's performance, clustering analysis doesn't have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires as an input and doesn't learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different clusters since clusters are used in the downstream modeling. In this notebook we'll cover two metrics that may give us some intuition about : Elbow method Elbow method gives us an idea on what a good number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters' centroids. We pick at the spot where SSE starts to flatten out and forming an elbow. We'll use the geyser dataset and evaluate SSE for different values of and see where the curve might form an elbow and flatten out. Silhouette analysis Silhouette analysis can be used to determine the degree of separation between clusters. For each sample: Compute the average distance from all data points in the same cluster (). Compute the average distance from all data points in the closest cluster (). Compute the coefficient: The coefficient can take values in the interval [-1, 1]. If it is 0 –&gt; the sample is very close to the neighboring clusters. It it is 1 –&gt; the sample is far away from the neighboring clusters. It it is -1 –&gt; the sample is assigned to the wrong clusters. Therefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. We'll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely only two groups of data points. 2. Pros &amp; ConsPros Easy to interpret Relatively fast Scalable for large data sets Able to choose the positions of initial centroids in a smart way that speeds up the convergence Guarantees convergence Cons The globally optimal result may not be achieved The number of clusters must be selected beforehand k-means is limited to linear cluster boundaries: this one may be solved using Similar technique as SVM does. One possible solution is the “Spectral Clustering”: i.e Kernelized K-means below 3. Applications Not to use if it contains heavily overlapping data/full of outliers Not so well if there are many categorical fields Not so well if the clusters have a complicated geometric shapes Real-world samples Market Segmentation Document clustering Image segmentation Image compression 4. Code ImplementationSklearn package's GMM12345from sklearn.cluster import KMeanskm = KMeans(n_clusters=2, max_iter=100)km.fit(X_std)centroids = km.cluster_centers_ GMM1. Definition The Gaussian mixture model (GMM) can be regarded as an optimization of the k-means model. It is not only a commonly used in industry but also a generative model. A model composed of K single Gaussian models. These K submodels are the hidden variables of the hybrid model Single Gaussian model: A Univariate Gaussian Distribution for the data Attempts to find a mixed representation of the probability distribution of the multidimensional Gaussian model, thereby fitting a data distribution of arbitrary shape. Also uses EM algorithm1: if our observations come from a mixture model with mixture components, the marginal probability distribution of is of the form: where is the latent variable representing the mixture component for is the mixture component, and is the mixture proportion representing the probability that belongs to the -th mixture component. Let denote the probability distribution function for a normal random variable. In this scenario, we have that the conditional distribution so that the marginal distribution of is: Similarly, the joint probability of observations is therefore: This note describes the EM algorithm which aims to obtain the maximum likelihood estimates of and given a data set of observations . Likelihood expression is ; Take log, compute and simplify the expected value of the complete log-likelihood: From here, we derive the expressions for each parameter: The EM algorithm, motivated by the two observations above, proceeds as follows: Initialize the 's, 's and 's and evaluate the log-likelihood with these parameters. E-step: Evaluate the posterior probabilities using the current values of the 's and 's with equation (2) M-step: Estimate new parameters and with the current values of using equations (3), (4) and (5). Evaluate the log-likelihood with the new parameter estimates. If the loglikelihood has changed by less than some small , stop. Otherwise, go back to step 2 . The EM algorithm is sensitive to the initial values of the parameters, so care must be taken in the first step. However, assuming the initial values are \"valid\", one property of the EM algorithm is that the log-likelihood increases at every step. This invariant proves to be useful when debugging the algorithm in practice. 2. Pros &amp; ConsPros GMM is a lot more flexible in terms of cluster covariance It is a soft-clustering method, which assign sample membersips to multiple clusters. This characteristic makes it the fastest algorithm to learn mixture models Cons Slower than k-means does not work if the mixture is not really a gaussian distribution It is very sensitive to the initial values which will condition greatly its performance. GMM may converge to a local minimum, which would be a sub-optimal solution. When having insufficient points per mixture, the algorithm diverges and finds solutions with infinite likelihood unless we regularize the covariances between the data points artificially. 3. Application perform GMM when you know that the data points are mixtures of a gaussian distribution if you think that your model is having some hidden, not observable parameters, then you can try to use GMM. 4. Simple codeSklearn package's GMM1234from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components = 3) gmm.fit(X_principal)gmm.fit_predict(X_principal)","link":"/post/clustering-1/"},{"title":"Clustering: Hierachial, BIRCH and Spectral","text":"Hierachial Clustering1. Definition 2 Main approaches Agglomerative : This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive : This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. Agglomerative Clustering Initially each data point is considered as an individual cluster. At each iteration, the most similar clusters merge with other clusters until 1/ K clusters are formed. No need to specify number of clusters, performance In sklearn, if we specify the number of clusters, performance can be improved Procedure Compute the proximity matrix Let each data point be a cluster Repeat: Merge two closest clusters and update the proximity matrix until 1/ K cluster remains Divisive Clustering Opposite of agglomerative clustering. We start with one giant cluster including all data points. Then data points are separated into different clusters. Similarity score: Basically the proximity between two clusters Distance calculation Euclidean Distance Squared Euclidean Distance Manhattan Distance Maximum Distance: Mahalanobis Distance: where is Covariance matrix For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used. For details, see Distance metrics &amp; Evaluation method[Unsupervised Learning/0. Distance metrics and Evaluation Methods/Distance_Metrics_Evaluation_Methods.ipynb] Distance references Complete-linkage: The maximum distance between elements of each cluster Single-linkage: The minimum distance between elements of each cluster Average linkage: The mean distance between elements of each cluster Ward’s linkage: Minimizes the variance of the clusters being merged. Least increase in total variance around cluster centroids is aimed. 2. Pros &amp; ConsPros Do not have to specify the number of clusters beforehand It is easy to implement and interpretable with the help of dendrograms Always generates the same clusters (Stability) Cons Exponential runtime for larger datasets 3. Application Text grouping: However, it is a highly complex task due the high-dimensionality of data. Social network analysis Outlier detection 4. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.cluster import AgglomerativeClustering from sklearn.preprocessing import StandardScaler, normalizefrom sklearn.decomposition import PCAfrom sklearn.metrics import silhouette_scoreimport scipy.cluster.hierarchy as shcraw_df = pd.read_csv('CC GENERAL.csv')raw_df = raw_df.drop('CUST_ID', axis = 1) raw_df.fillna(method ='ffill', inplace = True) # Standardize datascaler = StandardScaler() scaled_df = scaler.fit_transform(raw_df) # Normalizing the Data normalized_df = normalize(scaled_df) # Converting the numpy array into a pandas DataFrame normalized_df = pd.DataFrame(normalized_df) # Reducing the dimensions of the data pca = PCA(n_components = 2) X_principal = pca.fit_transform(normalized_df) X_principal = pd.DataFrame(X_principal) X_principal.columns = ['P1', 'P2'] plt.figure(figsize =(6, 6)) plt.title('Visualising the data') Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) # Determine the optimal number of clusters using [Silhouette Score]silhouette_scores = []for n_cluster in range(2, 8): silhouette_scores.append( silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) # Plotting a bar graph to compare the results k = [2, 3, 4, 5, 6,7] plt.bar(k, silhouette_scores) plt.xlabel('Number of clusters', fontsize = 10) plt.ylabel('Silhouette Score', fontsize = 10) plt.show() agg = AgglomerativeClustering(n_clusters=3)agg.fit(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = AgglomerativeClustering(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) plt.show() BIRCH Clustering1. Definition Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) Rationale: Existing data clustering methods do not adequately address the problem of processing large datasets with a limited amount of resources (i.e. memory and cpu cycles). In consequence, as the dataset size increases, they scale poorly in terms of running time, and result quality. Main logic of BIRCH: Deals with large datasets by first generating a more compact summary that retains as much distribution information as possible, and then clustering the data summary instead of the original dataset Metric attributes Definition: values can be represented by explicit Euclidean coordinates (no categorical variables). BIRCH can only deal with metric attributes Clustering Features BIRCH summarize the information contained in dense regions as Clustering Feature (CF); where = # of data points in a cluster, = linear sum of data; = square sum of data; CF additivity theorem: ; CF Tree Clustering Feature tree structure is similar to the balanced B+ tree A very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster. Each non-leaf node contains at most entries. Each leaf node contains at most entries, and each entry is a CF Threshold for leaf entry: all sample points in this CF must be in the radius In a hyper-sphere less than T. Insertion Algo: (Insert a new CF/Point entry in to the tree) Starting from the root, recursively traverse down the tree by choosing the node that has shortest Euclidean distance to the inserted entry; Upon reaching a leaf node, find the shorest distance CF and see if it can include the new CF/Point into the cluster without radius threshold violation;If can: do not create a new leaf, but update all the CF triplets on the path, the insertion ends;If cannot: go to 3; If the number of CF nodes of the current leaf node is less than the threshold , create a new CF node, put in a new sample and the new CF node into this leaf node, update all CF triplets on the path, and insertion Ends.Otherwise, go to 4 If the leaf node has &gt; L entires after addition, then split the leaf node by choosing the 2 entries that are farthest apart and redistribute CF based on distance to each of the 2 entries; Modify the path to leaf: Since the leaf node is updated, we need to update the entire path from root to leaf; In the event of split, we need to insert a nonleaf entry into the parent node, and if parent node has &gt; nodes, then we need to split again; do so until it reaches the root Complete procedure Phase 1: The algorithm starts with an initial threshold value (ideally start from low), scans the data, and inserts points into the tree. If it runs out of memory before it finishes scanning the data, it increases the threshold value, and rebuilds a new, smaller CF-tree, by re-inserting the leaf entries of the old CF-tree into the new CF-tree. After all the old leaf entries have been re-inserted, the scanning of the data and insertion into the new CF-tree is resumed from the point at which it was interrupted. (Optional) Filter the CF Tree created in the first step to remove some abnormal CF nodes. (Optional) Use other clustering algorithms such as K-Means to cluster all CF tuples to get a better CF Tree. Phase 2: Given that certain clustering algorithms perform best when the number of objects is within a certain range, we can group crowded subclusters into larger ones resulting in an overall smaller CF-tree. Phase 3: Almost any clustering algorithm can be adapted to categorize Clustering Features instead of data points. For instance, we could use KMEANS to categorize our data, all the while deriving the benefits from BIRCH Additional passes over the data to correct inaccuracies caused by the fact that the clustering algorithm is applied to a coarse summary of the data. The complexity of the algorithm is 2. Pros &amp; ConsPros Save memory, all samples are on disk, CF Tree only stores CF nodes and corresponding pointers. The clustering speed is fast, and it only takes one scan of the training set to build the CF Tree, and the addition, deletion, and modification of the CF Tree are very fast. Noise points can be identified, and preliminary classification pre-processing can be performed on the data set. Cons There is need to specify number of clusters; The clustering result may be different from the real category distribution. Does not perform well on non-convex dataset distribution Apart from number of clusters we have to specify two more parameters; Birch doesn’t perform well on high dimensional data (if there are &gt;20 features, you’d better use something else). 3. Applications If the dimension of the data features is very large, such as greater than 20, BIRCH is not suitable. At this time, Mini Batch K-Means performs better. 4. Code implementation12345678910111213141516import numpy as npfrom matplotlib import pyplot as pltimport seaborn as snssns.set()from sklearn.datasets import make_blobsfrom sklearn.cluster import BirchX, clusters = make_blobs(n_samples=450, centers=6, cluster_std=0.70, random_state=0)plt.scatter(X[:,0], X[:,1], alpha=0.7, edgecolors='b')# Predict and visualizebrc = Birch(branching_factor=50, n_clusters=None, threshold=1.5)brc.fit(X)labels = brc.predict(X)plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow', alpha=0.7, edgecolors='b') Spectral Clustering1. Definition In spectral clustering, data points are treated as nodes of a graph. Thus, spectral clustering is a graph partitioning problem. The nodes are mapped to a low-dimensional space that can be easily segregated to form clusters. No assumption is made about the shape/form of the clusters. The goal of spectral clustering is to cluster data that is connected but not necessarily compact or clustered within convex boundaries. In general, spectral clustering is a generalized version of k-means: it does not assume a circular shape, but apply different affinity functions in its similarity matrix Procedures Project data into matrix Define an Affinity matrix A , using a Gaussian Kernel K or an Adjacency matrix Construct the Graph Laplacian from A (i.e. decide on a normalization) Solve the Eigenvalue problem Select k eigenvectors corresponding to the k lowest (or highest) eigenvalues to define a k-dimensional subspace Form clusters in this subspace using k-means Similarity Graph We first create an undirected graph G = (V, E) with vertex set V = {v1, v2, …, vn} = 1, 2, …, n observations in the data. -neighbourhood Graph: : Each point is connected to all the points which lie in it’s -radius. If all the distances between any two points are similar in scale then typically the weights of the edges (i.e. the distance between the two points) are not stored since they do not provide any additional information. Hence, the graph built is an undirected and unweighted graph. K-Nearest Neighbours: : For two vertices and , an edge is directed from to only if is among the k-nearest neighbours of u. The graph is a weighted and directed graph because it is not always the case that for each u having v as one of the k-nearest neighbours, it will be the same case for v having u among its k-nearest neighbours. To make this graph undirected, one of the following approaches are followed: Direct an edge from u to v and from v to u if either v is among the k-nearest neighbours of u OR u is among the k-nearest neighbours of v. Direct an edge from u to v and from v to u if v is among the k-nearest neighbours of u AND u is among the k-nearest neighbours of v. Fully-Connected Graph: Each point is connected with an undirected edge-weighted by the distance between the two points to every other point. Since this approach is used to model the local neighbourhood relationships thus typically the Gaussian similarity metric is used to calculate the distance: Thus, when we create an adjacency matrix for any of these graphs, when the points are close and if the points are far apart.Consider the following graph with nodes 1 to 4, weights (or similarity) wij and its adjacency matrix: Adjacency Matrix Adjacency Matrix Affinity metric determines how close, or similar, two points our in our space. We will use a Gaussian Kernel and not the standard Euclidean metric. Given 2 data points (projected in ), we define an Affinity that is positive, symmetric, and depends on the Euclidian distance between the data points We might provide a hard cut off threshold , so that if when the points are close in , and if the points , are far apart. Close data points are in the same cluster. Data points in different clusters are far away. But data points in the same cluster may also be far away–even farther away than points in different clusters. Our goal then is to transform the space so that when 2 points , are close, they are always in same cluster, and when they are far apart, they are in different clusters. Generally we use the Gaussian Kernel K directly, or we form the Graph Laplacian . Degree Matrix The degree matrix of a graph is the matrix defined by where of a vertex is the number of edges that terminate at Graph Laplacian The whole purpose of computing the Graph Laplacian was to find eigenvalues and eigenvectors for it, in order to embed the data points into a low-dimensional space. Just another matrix representation of a graph. It can be computed as: Simple Laplacian where is the Adjacency matrix and is the Degree Matrix Normalized Laplacian Generalized Laplacian Relaxed Laplacian Ng, Jordan, &amp; Weiss Laplacian , where The Cluster Eigenspace Problem To identify good clusters, Laplacian should be approximately a block-diagonal, with each block defining a cluster. If we have 3 major clusters (C1, C2, C3), we would expect - We also expect that the 3 lowest eigenvalues &amp; eigenvectors of each correspond to a different cluster. - For K clusters, compute the first K eigen vectors. . Stack the vectors vertically to form the matrix with eigen vecttors as columns. Represent every node as the corresponding row of this new matrix, these rows form the feature vector of the nodes. Use Kmeans to cluster these points into k clusters 2. Pros &amp; ConsPros Clusters not assumed to be any certain shape/distribution, in contrast to e.g. k-means. This means the spectral clustering algorithm can perform well with a wide variety of shapes of data. Works quite well when relations are approximately transitive (like similarity) Do not necessarily need the actual data set, just similarity/distance matrix, or even just Laplacian Because of this, we can cluster one dimensional data as a result of this; other algos that can do this are k-medoids and heirarchical clustering. Cons Need to choose the number of clusters k, although there is a heuristic to help choose Can be costly to compute, although there are algorithms and frameworks to help 3. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import SpectralClustering from sklearn.preprocessing import StandardScaler, normalize from sklearn.decomposition import PCA from sklearn.metrics import silhouette_scoreraw_df = pd.read_csv('CC GENERAL.csv')raw_df = raw_df.drop('CUST_ID', axis = 1) raw_df.fillna(method ='ffill', inplace = True) # Preprocessing the data to make it visualizable # Scaling the Data scaler = StandardScaler() X_scaled = scaler.fit_transform(raw_df) # Normalizing the Data X_normalized = normalize(X_scaled) # Converting the numpy array into a pandas DataFrame X_normalized = pd.DataFrame(X_normalized) # Reducing the dimensions of the data pca = PCA(n_components = 2) X_principal = pca.fit_transform(X_normalized) X_principal = pd.DataFrame(X_principal) X_principal.columns = ['P1', 'P2'] ## Affinity matrix with Gaussian Kernel## affinity = \"rbf\"# Building the clustering model spectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf') # Training the model and Storing the predicted cluster labels labels_rbf = spectral_model_rbf.fit_predict(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = SpectralClustering(n_clusters = 2, affinity ='rbf') .fit_predict(X_principal), cmap =plt.cm.winter) plt.show() ## Affinity matrix with Eucledean Distance## affinity = ‘nearest_neighbors’# Building the clustering model spectral_model_nn = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') # Training the model and Storing the predicted cluster labels labels_nn = spectral_model_nn.fit_predict(X_principal)# Visualizing the clustering plt.scatter(X_principal['P1'], X_principal['P2'], c = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') .fit_predict(X_principal), cmap =plt.cm.winter) plt.show() ### Evaluate performance# List of different values of affinity affinity = ['rbf', 'nearest-neighbours'] # List of Silhouette Scores s_scores = [] # Evaluating the performance s_scores.append(silhouette_score(raw_df, labels_rbf)) s_scores.append(silhouette_score(raw_df, labels_nn)) # Plotting a Bar Graph to compare the models plt.bar(affinity, s_scores) plt.xlabel('Affinity') plt.ylabel('Silhouette Score') plt.title('Comparison of different Clustering Models') plt.show() print(s_scores)","link":"/post/clustering-2/"},{"title":"Clustering: DBSCAN","text":"DBSCAN Introduction Density-based spatial clustering of applications with noise (DBSCAN) Summary: DBSCAN is a density-based clustering method that discovers clusters of nonspherical shape. Main Concept: Locate regions of high density that are separated from one another by regions of low density. It also marks as outliers the points that are in low-density regions. Implicit assumptions about the method: Densities across all the clusters are the same. Cluster sizes or standard deviations are the same. Density of region: Mainly defined by 2 parameters Density at a point P: Number of points within a circle of Radius from point P. Dense Region: For each point in the cluster, a circle with radius contains at least minimum number of points () The Epsilon neighborhood of a point P in the database D is defined as The function is usually defined by Euclidean Distance 3 classification of points: Core point: if the point has Border point: if the point has but it lies in the neighborhood of another Core point. Noise: any data point that is neither Core nor Border point Density Reachable/Density Connected/Directly Density Reachable Directly Density Reachable: Data-point is directly density reachable from a point if is a core point is in the epsilon neighborhood of Density Reachable: Data-point is density reachable from a point if For a chain of points , , is directly density reachable from . Density reachable is transitive in nature but, just like direct density reachable, it is not symmetric Density Connected: Data-point is density connected to a point if with respect to and there is a point such that, both and are density reachable from w.r.t. to and Procedure Starts with an arbitrary point which has not been visited and its neighborhood information is retrieved from the parameter. If this point contains neighborhood points, cluster formation starts.Otherwise the point is labeled as noise.- This point can be later found within the neighborhood of a different point and, thus can be made a part of the cluster. If a point is found to be a core point then the points within the neighborhood is also part of the cluster. So all the points found within neighborhood are added, along with their own neighborhood, if they are also core points. Continue the steps above (1-3) until the density-connected cluster is completely found. The process restarts with a new point which can be a part of a new cluster or labeled as noise. 2. Pros &amp; ConsPros Identifies randomly shaped clusters doesn’t necessitate to know the number of clusters in the data previously (as opposed to K-means) Handles noise Cons If the database has data points that form clusters of varying density, then DBSCAN fails to cluster the data points well, since the clustering depends on ϵ and MinPts parameter, they cannot be chosen separately for all clusters May Overcome this issue by running additional rounds over large clusters If the data and features are not so well understood by a domain expert then, setting up and could be tricky Computational complexity — when the dimensionality is high, it takes 3. Code Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltimport numpy as npfrom scipy import statsfrom sklearn.cluster import DBSCANfrom sklearn.metrics import silhouette_score# To choose the best combination of the algorithm parameters I will first create a matrix of investigated combinations.from itertools import productmall_data = pd.read_csv('Mall_Customers.csv')X_numerics = mall_data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']] # subset with numeric variables onlyeps_values = np.arange(8,12.75,0.25) # eps values to be investigatedmin_samples = np.arange(3,10) # min_samples values to be investigatedDBSCAN_params = list(product(eps_values, min_samples))no_of_clusters = []sil_score = []for p in DBSCAN_params: DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_numerics) no_of_clusters.append(len(np.unique(DBS_clustering.labels_))) sil_score.append(silhouette_score(X_numerics, DBS_clustering.labels_))tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples']) tmp['No_of_clusters'] = no_of_clusterspivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')fig, ax = plt.subplots(figsize=(12,6))sns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)ax.set_title('Number of clusters')plt.show()tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples']) tmp['Sil_score'] = sil_scorepivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')fig, ax = plt.subplots(figsize=(18,6))sns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)plt.show()DBS_clustering = DBSCAN(eps=12.5, min_samples=4).fit(X_numerics)DBSCAN_clustered = X_numerics.copy()DBSCAN_clustered.loc[:,'Cluster'] = DBS_clustering.labels_ # append labels to pointsDBSCAN_clust_sizes = DBSCAN_clustered.groupby('Cluster').size().to_frame()DBSCAN_clust_sizes.columns = [\"DBSCAN_size\"]display(DBSCAN_clust_sizes)outliers = DBSCAN_clustered[DBSCAN_clustered['Cluster']==-1]fig2, (axes) = plt.subplots(1,2,figsize=(12,5))sns.scatterplot('Annual Income (k$)', 'Spending Score (1-100)', data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1], hue='Cluster', ax=axes[0], palette='Set1', legend='full', s=45)sns.scatterplot('Age', 'Spending Score (1-100)', data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1], hue='Cluster', palette='Set1', ax=axes[1], legend='full', s=45)axes[0].scatter(outliers['Annual Income (k$)'], outliers['Spending Score (1-100)'], s=5, label='outliers', c=\"k\")axes[1].scatter(outliers['Age'], outliers['Spending Score (1-100)'], s=5, label='outliers', c=\"k\")axes[0].legend()axes[1].legend()plt.setp(axes[0].get_legend().get_texts(), fontsize='10')plt.setp(axes[1].get_legend().get_texts(), fontsize='10')plt.show()","link":"/post/clustering-3/"},{"title":"Clustering: Affinity Propagation","text":"Affinity Propagation Introduction Developed recently (2007), a centroid based clustering algorithm similar to k Means or K medoids Affinity propagation finds \"exemplars\" i.e. members of the input set that are representative of clusters. It uses a graph based approach to let points 'vote' on their preferred 'exemplar'. The end result is a set of cluster 'exemplars' from which we derive clusters by essentially doing what K-Means does and assigning each point to the cluster of it's nearest exemplar. We need to calculate the following matrices: Here we must specify to notation: = row, = column Similarity matrix Responsibility matrix Availability matrix Criterion matrix Similarity matrix Rationale: information about the similarity between any instances, for an element i we look for another element j for which is the highest (least negative). Hence the diagonal values are all set to the most negative to exclude the case where i find i itself Barring those on the diagonal, every cell in the similarity matrix is calculated by the negative sum of the squares differences between participants. Note that the diagonal values will not be just 0: It is Responsibility matrix Rationale: quantifies how well-suited element k is, to be an exemplar for element i , taking into account the nearest contender k’ to be an exemplar for i. We initialize R matrix with zeros. Then calculate every cell in the responsibility matrix using the following formula: Interpretation: R_{i,k} can be thought of as relative similarity between i and k. It quantifies how similar is i to k, compared to some k’, taking into account the availability of k’. The responsibility of k towards i will decrease as the availability of some other k’ to i increases. Availability matrix Rationale: It quantifies how appropriate is it for i to choose k as its exemplar, taking into account the support from other elements that k should an exemplar. The Availability formula for different instances is The Self-Availability is Interpretation of the formulas Availability is self-responsibility of k plus the positive responsibilities of k towards elements other than i. We include only positive responsibilities as an exemplar should be positively responsible/explain at least for some data points well, regardless of how poorly it explains other data points. If self-responsibility is negative, it means that k is more suitable to belong to another exemplar, rather than being an exemplar. The maximum value of is 0. reflects accumulated evidence that point k is suitable to be an exemplar, based on the positive responsibilities of k towards other elements. and matrices are iteratively updated. This procedure may be terminated after a fixed number of iterations, after changes in the values obtained fall below a threshold, or after the values stay constant for some number of iterations. Criterion Matrix Criterion matrix is calculated after the updating is terminated. Criterion matrix is the sum of and : An element i will be assigned to an exemplar k which is not only highly responsible but also highly available to i. The highest criterion value of each row is designated as the exemplar. Rows that share the same exemplar are in the same cluster. Sample run Data Similarity Matrix Responsibility Matrix (First round) Availability Matrix (First round) Criterion Matrix 2. Pros &amp; ConsPros Does not need to specify the cluster number Allows for non-metric dissimilarities (i.e. we can have dissimilarities that don't obey the triangle inequality, or aren't symmetric) Providebetter stability over runs Cons Similar issue as K-means: susceptible to outliers Affinity Propagation tends to be very slow. In practice running it on large datasets is essentially impossible without a carefully crafted and optimized implementation 3. Code Implementation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from sklearn.cluster import AffinityPropagationfrom sklearn import metricsfrom sklearn.datasets import make_blobs# ############################################################################## Generate sample datacenters = [[1, 1], [-1, -1], [1, -1]]X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)# ############################################################################## Compute Affinity Propagationaf = AffinityPropagation(preference=-50).fit(X)cluster_centers_indices = af.cluster_centers_indices_labels = af.labels_n_clusters_ = len(cluster_centers_indices)print('Estimated number of clusters: %d' % n_clusters_)print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels, metric='sqeuclidean'))# ############################################################################## Plot resultimport matplotlib.pyplot as pltfrom itertools import cycleplt.close('all')plt.figure(1)plt.clf()colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')for k, col in zip(range(n_clusters_), colors): class_members = labels == k cluster_center = X[cluster_centers_indices[k]] plt.plot(X[class_members, 0], X[class_members, 1], col + '.') plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14) for x in X[class_members]: plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)plt.title('Estimated number of clusters: %d' % n_clusters_)plt.show()","link":"/post/clustering-4/"},{"title":"Clustering: Apriori","text":"Association RuleAssociation rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. For example, we may want to find 1-1 product category assocaition rule: product cateogry 1 -&gt; product category 2 This is often used for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. Because we don't have initial associations in our data, it is an unsupervised learning problem for marketing activities such as, e.g., promotional pricing or product placements. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. [Wikipedia] Evaluation Metrics1 Support : % of transactions where items in X AND Y are bought together Property of down-ward closure which means that all sub sets of a frequent set (support &gt; min. support threshold) are also frequent Cons: Items that occur very infrequently in the data set are pruned although they would still produce interesting and potentially valuable rules. Confidence : % of transactions amongst all customers who bought Y given that they have bought X While support is used to prune the search space and only leave potentially interesting rules, confidence is used in a second step to filter rules that exceed a min. confidence threshold Cons: sensitive to the frequency of the consequent (Y) in the data set. Caused by the way confidence is calculated, Ys with higher support will automatically produce higher confidence values even if they exists no association between the items. Lift An association rule X -&gt; Y is only useful if the lift value &gt; 1 Want to consider also the presence of Y being bought independently without knowledge about X Largely solves to problem of confidence threshold: sensitive to the frequency of the consequent (Y) Conviction : How poor can the association be. A directed measure monotone in confidence and lift. Leverage : difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically independent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells. Cons: suffer from the rare item problem. Apriori PropertyAll subsets of a frequent itemset must be frequent (Apriori propertry). If an itemset is infrequent, all its supersets will be infrequent. Applying the apriori property, we get the following algorithm. Algorithm Generating Support Value for Itemsets containing one items (One Itemset) With a pre-defined support threshold, identify itemsets worth exploring With the shortlisted One Itemset that are above the support threshold, generate Itemsets containing two items (Two Itemsets) With the same pre-definited support threshold, identify associations in Two Itemsets that are worth exploring With the shortlisted Two Itemsets, association rule is generated between the two items Confidence value is generated for each association rule With a pre-defined confidence threshold, association rules are being shortlisted With shortlisted association rules, the lift values are computed for each of them Only association rules with lift value &gt; 1 is considered as meaningful associations","link":"/post/clustering-5/"},{"title":"An overview of Big Data Analytics","text":"OverviewData analysis is the process of compiling, processing, and analyzing data so that you can use it to make decisions. Let's start with the well-known 5 V's and proceed down to more concepts in Data analytics. The 5 V's Volume – data storage: When businesses have more data than they are able to process and analyze, they have a volume problem. There are three broad classifications of data source types: Structured data (10%):: Organized and stored in the form of values that are grouped into rows and columns of a table. Semistructured data (10%):: Often stored in a series of key-value pairs that are grouped into elements within a file. Unstructured data (80%):: Not structured in a consistent way. Some data may have structure similar to semi-structured data but others may only contain metadata. Velocity - data processing: When businesses need rapid insights from the data they are collecting, but the systems in place simply cannot meet the need, there's a velocity problem. 2 types of processing: Batch processing (Large burst of data): For initial insights and real-time feedback Stream Processing (Tiny burst of data): For deep insights using complex analysis Variety - data structure and types When your business becomes overwhelmed by the sheer number of data sources to analyze and you cannot find systems to perform the analytics, you know you have a variety problem. Veracity - data cleasing and transformation When you have data that is ungoverned, coming from numerous, dissimilar systems and cannot curate the data in meaningful ways, you know you have a veracity problem Value - data insight and business intelligence When you have massive volumes of data used to support a few golden insights, you may be missing the value of your data Data Analytics Definitions Information analyticsInformation analytics is the process of analyzing information to find the value contained within it. This term is often synonymous with data analytics. Operational analyticsOperational analytics is a form of analytics that is used specifically to retrieve, analyze, and report on data for IT operations. This data includes system logs, security logs, complex IT infrastructure events and processes, user transactions, and even security threats. 5 forms of analysis: Descriptive (Data Mining): Human Judgement Insight - Requires highest amount of human effort and interpretation - Focus on \"Whatdunit\" Diagnostic Human Judgement Insight - Used to compare historic data with other data to find dependencies and patterns that lead to answers - Focus on \"Whydunit\" Predictive Human Judgement Insight - Focus on Future prediction - Uses the results of descriptive and diagnostics analysis to predict future events and trends - Accuracy highly dependent on the quality of data and stability of environment setup Prescriptive Human Judgement Insight Decision Action - Focus on Solution finding - Used to prescribe actions to taken given the data provided - Requires input from all other forms of analytics, combined with rules and contraints-based optimization to make relevant suggestion. - This part is likely to be automated via machine learning Cognitive Human Judgement Insight Decision Action - Focus on recommended actions (not to be confused with \\\"solution\\\" to problems) - Try to mimic what the human brain does in problem solving - Generates hypothesis from the existing data, connections and contraints. Answers are provided in the form of recommendation and a confidence ranking Analytic services and velocity Batch analytics: Typically involves querying large amounts of “cold” data. Batch analytics are implemented on large data sets to produce a large number of analytical results on a regular schedule. Interactive analytics: Typically involves running complex queries across complex data sets at high speeds. This type of analytics is interactive in that it allows a user to query and see results right away. Batch analytics are generally run in the background, providing analytics in the form of scheduled report deliveries. Stream analytics: Requires ingesting a sequence of data and incrementally updating metrics, reports, and summary statistics in response to each arriving data record. This method is best suited for real-time monitoring and response functions. Streaming data processing requires two layers: a storage layer and a processing layer. The storage layer needs to support record ordering and strong consistency to enable fast, inexpensive, and re-playable reads and writes of large streams of data. The processing layer is responsible for consuming data from the storage layer, running computations on that data, and then notifying the storage layer to delete data that is no longer needed. Best practices for writing reports Gather the data, facts, action items, and conclusions. Identify problems and formats Identify the audience, expectations they have, and the proper method of delivery. Identify the visualization styles and report style that will best fit the needs of the audience. Create the reports and dashboards.","link":"/post/data-analytics/"},{"title":"The Data Mining Triology: II. Cleaning","text":"OverviewVery often, the data loaded into your notebooks are not entirely usable. There might be missing values, noisy data points, duplicates and outliers. Sometimes, data needs to be scaled up and down. Encoding and dimensionality reductions can be performed to make data cleaner and easier to operate on. Here we discuss about some essential ways to clean up the data Basic CleaningThe first step involves Detecting and handling missing or noisy data; Removal of outliers Minimizing duplication and computed biases within the data Missing Data Missing data is the entries with empty input or Null input. It can be handled in following ways: Ignore the Tuple Note: Suitable only when the dataset is quite large and multiple values are missing within a tuple 123456df2 = df[[column for column in df if df[column].count() / len(df) &gt;= 0.3]] # Drops columns with &gt;70% of rows missing value;print(\"List of dropped columns:\", end=\" \")for c in df.columns: if c not in df2.columns: print(c, end=\", \") # list out the dropped columns Fill the Missing Values using Manual imputation (via inspection &amp; domain knowledge) Mean value imputation Most Probable Value (Mode) imputation Noisy Data Noisy data is meaningless data that can't be interpreted by machines. It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways : Binning Method: [Sorted data in order to smooth it] [The whole data divided into segments of equal size] [Various methods are performed to complete the task. Each segmented is handled separately.] Regression: Fitting data to a regression function: ML Regression Algorithm can be used for smoothing of data. Interpolate using the regression. Clustering: Groups the similar data in a cluster and apply unsupervised learning. Detect and Remove Outliers: Detect Outliers (Some simple methods outlined below) 1. Using Boxplot12import seaborn as snssns.boxplot(...) 2. Using Scatterplot12345%matplotlib inlinefrom matplotlib import pyplot as pltfig, ax = plt.subplots(figsize=(16,8))ax.scatter(...)plot.show() 3. Using z score12345from scipy import statsimport numpy as npz = np.abs(stats.zscore(boston_df))threshold = 3print(np.where(z &gt; threshold)) 4. Using interquartile range (IQR) score1234Q1 = boston_df_o1.quantile(0.25)Q3 = boston_df_o1.quantile(0.75)IQR = Q3 - Q1print(boston_df_o1 &lt; (Q1 - 1.5 * IQR)) |(boston_df_o1 &gt; (Q3 + 1.5 * IQR)) Remove Outliers (Fixed value or interval methods) 5. Using column specific value threshold1boston_df_o = boston_df_o[(z &lt; 3).all(axis=1)] 6. Using value range (IQR in this case)1boston_df_out = boston_df_o1[~((boston_df_o1 &lt; (Q1 - 1.5 * IQR)) |(boston_df_o1 &gt; (Q3 + 1.5 * IQR))).any(axis=1)] Remove Duplicates Sometimes, there may exist duplicate data entries. Most of the time, this is undesirable. You may want to remove those entries (after carefully examine the problem setups) 123 s1_dup = s1_trans[s1_trans.duplicated()] # Identify Duplicatesprint(s1_dup.shape)s1_trans.drop_duplicates(subset = None, keep = 'first', inplace = True) # Remove Duplicates Transforming dataWe transform datasets in some situations to : Convert the raw data into a specified format according to the need of the model. Remove redundancy within the data (not duplicates, but unnecessary bytes that occupy the storage for no meaning) Efficiently organize the data Here we just present the method using sci-kit laern's preprocessing module. Data Conversion Normalization (Basically Data rescaling/mean removal):It is done in order to scale the data values in a specified range (-1.0 to 1.0 or 0.0 to 1.0) Attribute Selection (Usually for Aggregation purpose):In this strategy, new attributes are constructed from the given set of attributes to help the mining process. Discretization: (IMPT!!!)This is done to replace the raw values of numeric attribute by interval levels or conceptual levels. Using classes/ranges/bands mapping (given or need to design) Using Top-down approaches: Entropy-based Discretization Concept Hierarchy Generation:Here attributes are converted from level to higher level in hierarchy. For example, the attribute \"city\" can be converted to \"country\" in some scenarios. Encode Data:Machine learning algorithms cannot work with categorical data directly, categorical data must be converted to number. Label Encoding One hot encoding Dummy variable trap Label encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them. A One hot encoding is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1. The Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated in simple term one variable can be predicted from the others. By using pandas get_dummies function we can do all above three step in line of code. We will this fuction to get dummy variable for sex, children,smoker,region features. By setting drop_first =True function will remove dummy variable trap by droping one variable and original variable.The pandas makes our life easy. Advanced: Box-Cox transformationA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn't normal, applying a Box-Cox means that you are able to run a broader number of tests. All that we need to perform this transformation is to find lambda value and apply the rule shown below to your variable. 123## The trick of Box-Cox transformation is to find lambda value, however in practice this is quite affordable. The following function returns the transformed variable, lambda value,confidence interval. See the sample code below:from scipy.stats import boxcoxy_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05) box-cox Data Scaling / Standardizing / Mean RemovalNote: Don't use all of them, only use some selectively when you need to (which is usually the case after EDA)!!! Rescaling Data: scaled between the given range12data_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))data_scaled = data_scaler.fit_transform(input_data) Mean Removal: standardize input_data into mean = 0 and std = 1123data_standardized = preprocessing.scale(input_data)data_standardized.mean(axis = 0)data_standardized.std(axis = 0) Normalizing Data: values of a feature vector are adjusted so that they sum up to 11data_normalized = preprocessing.normalize(input_data, norm = 'l1') Binarizing Data: convert a numerical feature vector into a Boolean vector1data_binarized = preprocessing.Binarizer(threshold=1.4).transform(input_data) Label Encoding: changing the word labels into numbers 1234567891011### Encode:label_encoder = preprocessing.LabelEncoder()input_classes = ['suzuki', 'ford', 'suzuki', 'toyota', 'ford', 'bmw']label_encoder.fit(input_classes) #Generate Mappingprint(\"\\nClass mapping:\")for i, item in enumerate(label_encoder.classes_): print(item, '--&gt;', i)labels = ['toyota', 'ford', 'suzuki']encoded_labels = label_encoder.transform(labels) # Actual Encoding### Decode:decoded_labels = label_encoder.inverse_transform(encoded_labels) # Actual Decoding Warning: it assumes higher the categorical value, better the category (solved by one hot encoding) One Hot Encoding (often used together with argmax function): Dummy Variable Encoding123456789101112&gt;&gt;&gt; enc = OneHotEncoder(handle_unknown='ignore')&gt;&gt;&gt; X = [['Male', 1], ['Female', 3], ['Female', 2]]&gt;&gt;&gt; enc.fit(X)OneHotEncoder(handle_unknown='ignore')&gt;&gt;&gt; enc.categories_[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]&gt;&gt;&gt; enc.transform([['Female', 1], ['Male', 4]]).toarray()array([[1., 0., 1., 0., 0.], [0., 1., 0., 0., 0.]])&gt;&gt;&gt; enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])array([['Male', 1], [None, 2]], dtype=object) Scaler Comparison and choices MinMaxScaler Definition: Add or substract a constant. Then multiply or divide by another constant. MinMaxScaler subtracts the mimimum value in the column and then divides by the difference between the original maximum and original minimum. Preprocessing Type: Scale Range: 0 to 1 default, can override Mean: varies Distribution Characteristics: Bounded When Use: Use first unless have theoretical reason to need stronger scalers. Notes: Preserves the shape of the original distribution. Doesn't reduce the importance of outliers. Least disruptive to the information in the original data. Default range for MinMaxScaler is 0 to 1. RobustScaler Definition: RobustScaler standardizes a feature by removing the median and dividing each feature by the interquartile range. Preprocessing Type: Standardize Range: varies Mean: varies Distribution Characteristics: Unbounded When Use: Use if have outliers and don't want them to have much influence. Notes: Outliers have less influence than with MinMaxScaler. Range is larger than MinMaxScaler or Standard Scaler. StandardScaler Definition: StandardScaler standardizes a feature by removing the mean and dividing each value by the standard deviation. Preprocessing Type: Standardize Range: varies Mean: 0 Distribution Characteristics: Unbounded, Unit variance When Use: When need to transform a feature so it is close to normally distributed. Notes: Results in a distribution with a standard deviation equal to 1 (and variance equal to 1). If you have outliers in your feature (column), normalizing your data will scale most ofthe data to a small interval. Normalizer Definition: An observation (row) is normalized by applying 12 (Euclidian) normalization. If each element were squared and summed, the total would equal 1. Could also specify 11 (Manhatten) normalization. Preprocessing Type: Normalize Range: varies Mean: 0 Distribution Characteristics: Unit norm When Use: Rarely Notes: Normalizes each sample observation (row), not the feature (column)! Reduce DataThe aim of data reduction is to fit the data size to the question/make model more efficient. Usually there are 4 major methods as outlined below: 1. Data Cube Aggregation:Aggregation operation is applied to data for the construction of the data cube. The cube stores multidimensional aggregated information Ensures a smallest representation which is enough for the Task Base cuboid: individual entity of interest (e.g customers) Apex cuboid: total of all branches (e.g total sales for all item types) 2. Attribute Subset Selection:The highly relevant attributes should be used, rest all can be discarded.Significance Level and p-value of the attribute comparison: The attribute having p-value greater than significance level can be discarded. 3. Numerosity Reduction:This enable to store the model of data instead of whole data, for example: Regression Models. Parametric MethodRegressionlog-linear models Non-parametri Methodhistograms (for supervised learning binning)clustering (for unsupervised learning binning)sampling (best is simple random sampling without replacement)data cube aggregation (move from lowest level to highest level, data reduces when moving up the cube) 4. Dimensionality Reduction: This reduce the size of data by encoding mechanisms. lossy vs lossless:If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction Methods: Wavelet transforms: decompose a signal based on special bases (or basis functions), which have certain mathematical properties; Works well for image description PCA (Identify the components contributing to the most of the variances in the data) ICA (identify independent components that extract individual signals from a mixture) ConclusionThe above methods really focused a lot on numerical data and basic variables. There are tones of details I neglected (like errors in entries, how to detect them and how to fix them). Also, there are special treatments on words or images (like regex processing and image transformations). These are widely applied in the field of natural language processing and computer visions. We encourage interested readers to explore these ideas by reading the regex tutorials (I’ve also written a blog on it) and CV tutorials.","link":"/post/data-cleaning/"},{"title":"The Data Mining Triology: I. Preparation","text":"OverviewIn this Data Mining triology, I'm going to present the following critical steps each scientist should perform when handling the data: Data Preparation Load and integrate data sources Data Cleaning Prepocessing the data to make them meaningful and usable Exploratory Data Analysis Analyze the data and understand its pattern. Make corresponding adjustments to data along the way In this blog, let's talk about the first one – Data Preparation. Sources of dataThere are indeed a great variety of data sources since the age of machine learning started. While we may come across a wide variety of data types (image, video, text, sheet, signal… you name it), we often can get these data from some popular website: Google Datasets Search Pros: Wide coverage, can find whatever dataset you want Cons: Some datasets are not actually accessible, and the website does not indicate that at all! Government Datasets Pros: Most of them are publically available, which is good Most of these data are well preprocessed, so you don't have to worry about it Cons: Sometimes, the size of the dataset is not large enough for meaningful projects Governments may not release the most up-to-date datasets. Hence the effectiveness of prediction models may be questionable Kaggle Datasets Pros: Really easy to get the data: via command line. This often preferred by professional engineers. See tutorial on kaggle api Aside from the datasets themselves, you can often find a bunch of enthusiasts on machine learning in Kaggle and excellent tutorials on the datasets you found. Cons: It takes a bit of practice to get along with Kaggle. Passion and drive are the key to success in Kaggle UCI Machine Learning Repository Pros: One of the most widely used repository for machine learning datasets Very Often, the datasets are related to academic/industrial research projects, so it is extremely helpful to researchers Cons: As a well-aged repo, the datasetes there certainly have been studied extensively. So it may not be so useful for new breakthroughs (but still, it should be very helpful for beginners) Code for loading the dataThe most common format for machine learning data is CSV files, and we are using python 3.x here for actual code.This step should mark the start of your notebook (after np/pd/sklearn/plt). 123import pandas as pdtrain = pd.read_csv('../input/train.csv')test = pd.read_csv('../input/test.csv') Note that this is too widely abused that people forget about other ways to load data: Load CSV with Python Standard Library123456import csvimport numpy as npraw_data = open(\"your filename here\", 'rt')reader = csv.reader(raw_data, delimiter=',')x = list(reader)data = np.array(x).astype('float') Load CSV with Numpy123import numpyraw_data = open(\"your filename here\", 'rt')reader = numpy.loadtxt(raw_data, delimiter=\",\") Load CSV with URL12345from numpy import loadtxtfrom urllib.request import urlopenurl = 'URL to a dataset'raw_data = urlopen(url)dataset = loadtxt(raw_data, delimiter=\",\") Now be cautioned that this is just for CSV files. There are a lot of other data formats, and Google is always your best friend in finding methods to load datasets.{: .notice–info .notice–x-large} That was input, how about output?Converting data from python objects into byte streams is known as Pickling or Serialization. This allows your own data to be passed around efficiently. Very often, they are stored as .pkl or .json files. Python Pickle and JSONThe following table is inspired by this tutorial Python Pickle JSON Definition Python Pickle is the process of converting python objects (list, dict, tuples, etc.) into byte streams which can be saved to disks or can be transferred over the network. The byte streams saved on the file contains the necessary information to reconstruct the original python object. The process of converting byte streams back to python objects is called de-serialization. JSON stands for JavaScript Object Notation. Data Stored can be loaded without having the need to recreate the data again. Storage format Binary serialization format Simple text serialization format, human-readable Storage Versatility Not only data entries, but classes and methods can be serialized and de-serialized JSON is limited to certain python objects, and it cannot serialize every python object, such as classes and functions Language dependency Very reliant on the language (Python specific) and versions (2.x pickle files may not be compatible in 3.x env) JSON is supported by almost all programming languages. Speed Slower serialization and de-serialization in pickle Lightweights, much faster than pickle Security There is always security risks with pickle files JSON is generally secure With the above table in mind, one can choose their outputs accordingly. ConclusionLoading data is merely the first step and people can quickly learn to apply them. However, I/O choices does matter, and one should be cautious about them. Now, lets step into the second step in data mining: cleaning data.","link":"/post/data-preparation/"},{"title":"Database: Intro","text":"DefinitionA database is a collection of information that is usually stored in computer systems. The information are usually data of various forms. The main function of database is for the managers of these information to better store and manipulate the data efficiently. A database can be either relational or non-relational. Relational databaseIn a relational database, each row in the table is a record with a \"unique ID\" called the key. The columns of the table hold attribute of the data, and each record usually has a value for each attribute, making it easy to establish the relationships among data points. Non-relational databaseA non-relational database is a database that does not use the tabular schema of rows and columns found in most traditional database systems. Instead, non-relational databases use a storage model that is optimized for the specific requirements of the type of data being stored. For example, data may be stored as simple key/value pairs, as JSON documents, or as a graph consisting of edges and vertices.1 Note: The 3 points above are for interview preparation. They are succint and clearly explains each part. You can directly use them or shorten them in interview. 4. DBMS (Database management system)I assume that most of you have some prior knowledge about SQL. Otherwise, you are welcome to review my posts on SQL. So I'm not gonna explain what DBMS is and some examples of DBMS.To be updated…","link":"/post/database-introduction/"},{"title":"Dimension Reduction: Life savers","text":"OverviewHigh dimensional data modeling has always been a popular topic of discussion. Many research and work are done in this field simply because we have limited computational power. Even if quantum technology can greatly boost this power in near future, we will still face the curse of unlimited flow of data and features. Thus, it's actually extremely important that we reduce the amount of data input in a model. In this blog, we explore several well-known tools for dimensionality reduction, namly Linear Discriminant Analysis (LDA), Principle Component Analysis (PCA) and Nonnegative Matrix Factorization (NMF). LDA1. Definition Can be either a predictive modeling algorithm for multi-class classification or a dimensionality reduction technique A Generative Learning Algorithm based on labeled data Assumes Gaussian Distribution for ; Each attribute has the same variance (Mean removal/Feature Engineering with Log/Root functions/Box-Cox transformation needed) The class calculated from the discrimination function as having the largest value will be the output classification () LDA creates a new axis based on Maximize the distance between means Minimize the variations within each categories Procedure For Dimensionality Reduction (reduced-rank LDA) Compute the d-dimensional mean vectors for the different classes from the dataset. Compute the scatter matrices (in-between-class and within-class scatter matrix). Compute the eigenvectors (e1,e2,…,ed) and corresponding eigenvalues () for the scatter matrices. Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix W (where every column represents an eigenvector). Use this d×k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: Y=XW (where X is a n×d-dimensional matrix representing the n samples, and y are the transformed n×k-dimensional samples in the new subspace). For classification (Essentially, LDA classifies the sphered data to the closest class mean.) Perform eigen-decomposition on the pooled covariance matrix Spheres the data: to produce an identity covariance matrix in the transformed space Obtain group means in the transformed space: Classify according to : where is the group's prior probability Extensions to LDA Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables). Flexible Discriminant Analysis (FDA): Where non-linear combinations of inputs is used such as splines. Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA. 2. Pros &amp; ConsPros Need Less Data Simple prototype classifier: Distance to the class mean is used, it's simple to interpret. The decision boundary is linear: It's simple to implement and the classification is robust. Cons Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired. In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired. Support for more complex prototype classification is desired. 3. Application Bankruptcy prediction Facial recognition 4. Code implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from sklearn.datasets import load_wineimport pandas as pdimport numpy as npnp.set_printoptions(precision=4)from matplotlib import pyplot as pltimport seaborn as snssns.set()from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_split# Loading Datawine = load_wine()X = pd.DataFrame(wine.data, columns=wine.feature_names)y = pd.Categorical.from_codes(wine.target, wine.target_names)# Merge X and y (Training set)df = X.join(pd.Series(y, name='class'))## The Simply Way: using sklearnfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminantAnalysis()X_lda = lda.fit_transform(X, y)## The hard way: Understand the logic here# Compute means for each classclass_feature_means = pd.DataFrame(columns=wine.target_names)for c, rows in df.groupby('class'): class_feature_means[c] = rows.mean()class_feature_means# Compute the Within Class Scatter Matrix using the Mean Vectorwithin_class_scatter_matrix = np.zeros((13,13))for c, rows in df.groupby('class'): rows = rows.drop(['class'], axis=1) s = np.zeros((13,13))for index, row in rows.iterrows(): x, mc = row.values.reshape(13,1), class_feature_means[c].values.reshape(13,1) s += (x - mc).dot((x - mc).T) within_class_scatter_matrix += s# Compute the Between Class Scatter Matrix:feature_means = df.mean()between_class_scatter_matrix = np.zeros((13,13))for c in class_feature_means: n = len(df.loc[df['class'] == c].index) mc, m = class_feature_means[c].values.reshape(13,1), feature_means.values.reshape(13,1) between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)# Compute the Eigenvalues &amp; Eigenvectors, then sort accordinglyeigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]pairs = sorted(pairs, key=lambda x: x[0], reverse=True)# Print the Explained Varianceeigen_value_sums = sum(eigen_values)print('Explained Variance')for i, pair in enumerate(pairs): print('Eigenvector {}: {}'.format(i, (pair[0]/eigen_value_sums).real))# Identify the Principle Eigenvalues (here k = 2); Compute the new feature space X_ldaw_matrix = np.hstack((pairs[0][1].reshape(13,1), pairs[1][1].reshape(13,1))).realX_lda = np.array(X.dot(w_matrix))le = LabelEncoder()y = le.fit_transform(df['class']) # Here the y is just the encoded label set# Visualizeplt.xlabel('LD1')plt.ylabel('LD2')plt.scatter( X_lda[:,0], X_lda[:,1], c=y, cmap='rainbow', alpha=0.7, edgecolors='b') PCA1. Definition A linear model Basic intuition: projecting data onto its orthogonal feature subspace It is a technique for feature extraction — it combines our input variables in a specific way, then we can drop the \"least important\" variables while still retaining the most valuable parts of all of the variables (high variance, independent, few number) Each of the \"new\" variables after PCA are all independent of one another (due to the linear model assumption) PCA effectively minimizes error orthogonal to the model itself It can only be applied to datasets which are linearly separable Complete procedure Standardize the data. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix (by performing eigendecomposition), or perform Singular Value Decomposition (SVD) [SVD preferred due to efficiency and automatic eigenvalue sorting] Find the hyperplane that accounts for most of the variation using SVD (that hyperplane represents 1st componet); This basically means it tries to minimize the points’ distance to the hyperplane/maximize the distance from the projected point to the origin Find the orthogonal subspace, get from the subspace a best hyperplane with largest variation using SVD again (2nd component which is clearly independent of 1st component) Repeat to find all the components Use the eigenvalues to determine the proportion of variation that each component account for; construct the new system using the principle (top k) components; plot the samples using the projections on components as coordinates We should keep the component if it contributes substantially to the variation It eventually cluster samples that are highly correlated; It is possible to restore all the samples if each component correspond to one distinct variable (Note that # of PC &lt; # of Samples) Warning: scaling and centering data is very Important!!! Kernel PCA an extension of PCA into non-linear dataset by project dataset into a higher dimensional feature space using the kernel trick (recall SVM) Some popular kernels are Gaussian/RBF Polynomial Hyperbolic tangent: Note that the kernel matrix still need to be normalized for PCA to use kernel PCA so kernel PCA will have difficulties if we have lots of data points. Robust PCA an extension of PCA to deal with sparsity in the matrix It factorizes a matrix into the sum of 2 matrices, , where is the original matrix, is the low-rank (with lots of redundant information) matrix and is a sparse matrix (In the case of corrupted data, often captures the corrupted entries) Application: Latent semantic indexing =&gt; captures all common words while captures all key words that best identify each document. The minimization is over subject to . Minimizing L1-norm results in sparse values, while minimizing nuclear norm (sometimes also use Frobenious norm ) leads to sparse singular values (hence low rank) 2. Pros &amp; ConsPros Removes Correlated Features Improves Algorithm Performance Reduces Overfitting Cons Independent variables become less interpretable Data standardization is must before PCA Information Loss (if PCs chosen are not sufficient) 3. Application When interpretability is not an issue, use pca When the dimension is too large or you want to identify features that are independent, use pca 4. ComparisonPCA vs LDA Not as good as LDA in clustering/classification effect, yet idea for Factor analysis PCA projects the entire dataset onto a different feature (sub)space, and LDA tries to determine a suitable feature (sub)space in order to distinguish between patterns that belong to different classes PCA vs ICA In PCA the basis you want to find is the one that best explains the variability of your data; In ICA the basis you want to find is the one in which each vector is an independent component of your data (which usually has mixed signals/mixed features) 5. Code implementation12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns; sns.set(style='white')%matplotlib inline%config InlineBackend.figure_format = 'retina'from sklearn import decompositionfrom sklearn import datasetsfrom mpl_toolkits.mplot3d import Axes3Ddigits = datasets.load_digits()X = digits.datay = digits.target# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))plt.figure(figsize=(16, 6))for i in range(10): plt.subplot(2, 5, i + 1) plt.imshow(X[i,:].reshape([8,8]), cmap='gray') pca = decomposition.PCA(n_components=2)X_reduced = pca.fit_transform(X)print('Projecting %d-dimensional data to 2D' % X.shape[1])plt.figure(figsize=(12,10))plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, edgecolor='none', alpha=0.7, s=40, cmap=plt.cm.get_cmap('nipy_spectral', 10))plt.colorbar()plt.title('MNIST. PCA projection')pca = decomposition.PCA().fit(X)plt.figure(figsize=(10,7))plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)plt.xlabel('Number of components')plt.ylabel('Total explained variance')plt.xlim(0, 63)plt.yticks(np.arange(0, 1.1, 0.1))plt.axvline(21, c='b')plt.axhline(0.9, c='r')plt.show() NMF1. Definition It automatically extracts sparse and meaningful (easily interpretable) features from a set of nonnegative data vectors. We basically factorize into 2 smaller matrices non-negative and such that and ( low-rank approximate factorizations) Interpretation of and : Basically, we can interpret to be a weighted sum of some components, where each row in is a component, and each row in contains the weights of each component Idea of the algo: Formalize an objective function and iteratively optimize it A local minima is sufficient for the solution Objective function to minimize : Frobenius norm: w.r.t. s.t. Generalized Kullback-Leibler divergence: Choices of Optimization technique used: Coordinate descent (alternative: gradient descent which fix and optimize , then fix and optimize until tolerance is met) Multiplicative Update , Method to choose the optimal factorisation rank, : General guideline: Trial and error, Estimation using SVD based of the decay of the singular values Insights from experts Tricks Initialization: uses SVD to compute a rough estimate of the matrices and . If the non-negativity condition did not exist, taking the top k singular values and their corresponding vectors would construct the best rank k estimate, measured by the frobenius norm. Since and must be non-negative, we must slightly modify the vectors we use. Regularization: Since the represents weights of a component, it may produces weights that are too high/low. The classical way is to use or regularization losses 2. Application NMF is suited for tasks where the underlying factors can be interpreted as non-negative Image processing Topic Modeling Text mining Hyperspectral unmixing 3. Code Implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135from operator import itemgetterfrom concurrent.futures import ProcessPoolExecutor from time import timeimport os import gensimimport pandas as pdimport itertoolsimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefrom nltk.stem import WordNetLemmatizerfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.decomposition import NMFfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoderlemmatizer = WordNetLemmatizer()data_path = 'matrix_factorization_arxiv_query_result.json'articles_df = pd.read_json(data_path)articles_df.head()def stem(text): return lemmatizer.lemmatize(text)def map_parallel(f, iterable, **kwargs): with ProcessPoolExecutor() as pool: result = pool.map(f, iterable, **kwargs) return resultdef retrieve_articles(start, chunksize=1000): return arxiv.query( search_query=search_query, start=start, max_results=chunksize )def vectorize_text(examples_df, vectorized_column='summary', vectorizer=CountVectorizer): vectorizer = vectorizer(min_df=2) features = vectorizer.fit_transform(examples_df[vectorized_column]) le = LabelEncoder() ohe = OneHotEncoder() labels = le.fit_transform(valid_example_categories).reshape(-1, 1) labels_ohe = ohe.fit_transform(labels).todense() vectorized_data = { 'features': features, 'labels': labels, 'labels_onehot' : labels_ohe } return vectorized_data, (vectorizer, ohe, le)def extract_keywords(text): \"\"\" Use gensim's textrank-based approach \"\"\" return gensim.summarization.keywords( text=stem(text), lemmatize=True )def filter_out_small_categories(df, categories, threshold=200): class_counts = categories.value_counts() too_small_classes = class_counts[class_counts &lt; threshold].index too_small_classes valid_example_indices = ~categories.isin(too_small_classes) valid_examples = df[valid_example_indices] valid_example_categories = categories[valid_example_indices] return valid_examples, valid_example_categoriesdef print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): message = \"Topic #%d: \" % (topic_idx + 1) message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]) print(message) print()categories = articles_df['arxiv_primary_category'].apply(itemgetter('term'))main_categories = categories.apply(lambda s: s.split('.')[0].split('-')[0])main_categories_counts = main_categories.value_counts(ascending=True)main_categories_counts.plot.barh()plt.show()main_categories_counts[main_categories_counts &gt; 200].plot.barh()plt.show()categories.value_counts(ascending=True)[-10:].plot.barh()plt.show()articles_df['summary_keywords'] = [extract_keywords(i) for i in articles_df['summary']]article_keyword_lengths = articles_df['summary_keywords'].apply(lambda kws: len(kws.split('\\n')))valid_examples, valid_example_categories = filter_out_small_categories(articles_df, main_categories)vectorized_data, (vectorizer, ohe, le) = vectorize_text( valid_examples, vectorized_column='summary_keywords', vectorizer=TfidfVectorizer)x_train, x_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split( vectorized_data['features'], vectorized_data['labels_onehot'], vectorized_data['labels'], stratify=vectorized_data['labels'], test_size=0.2, random_state=0)nmf = NMF(n_components=5, solver='mu', beta_loss='kullback-leibler')topics = nmf.fit_transform(x_train)n_top_words = 10tfidf_feature_names = vectorizer.get_feature_names()print_top_words(nmf, tfidf_feature_names, n_top_words)dominant_topics = topics.argmax(axis=1) + 1categories = le.inverse_transform(y_train_labels[:,0])pd.crosstab(dominant_topics, categories)","link":"/post/dimentionality-reduction/"},{"title":"Stats in ML: Dirichlet Distribution","text":"DirichletBefore university, I've never heard of Dirichlet distribution. It seemed to me like a loner in the family of statistics when I first saw it. But soon I discovered that this is a huge misunderstanding. Dirichlet is way too important, too usful in the field of machine learning and statistical learning theories to be ignored by any interested scholars. I know deep down in my heart that I must dedicate one whole blog for it to stress its significance. As we walk along the path, we shall see why it is so great (in the past, present and future). Let's begin with a general definition of it. Formal definitionThe Dirichlet distribution Dir() is a family of continuous multivariate probability distributions parameterized by a vector of positive reals. It is a multivariate generalisation of the Beta distribution. Dirichlet distributions are commonly used as prior distributions (e.g. for categorical and multinomial distributions) in Bayesian statistics. Conjugate Prior and its usage.In Bayesian probability theory, we have Posterior , Prior and Likelihood related via the Bayes' theorem: If the posterior distribution and the prior distribution are from the same probability distribution family, then the prior and posterior are called conjugate distributions, and the prior is the conjugate prior for the likelihood function . Note that in many algorithm, we want to find the value of that maximizes the posterior (Maximum a posteriori). If the prior is some weird distribution, we may not get an analytical form for the posterior. Consequently, more complicated optimization strategies like interior point method may need to be applied, which can be computationally expensive. If both the prior and posterior have the same algebraic form, applying bayes rule to find is much easier. ExpressionAnalogous to multinomial distribution to binomial distribution, Dirichlet is the multinomial version for the beta distribution. Dirichlet distribution is a family of continuous probability distribution for a discrete probability distribution for categories , where for and , denoted by parameters . Formally, we denote . The expression is then where is some constant normalizer. Think of as the probability density associated with θ which is used for a multinomial distribution, given that our Dirichlet distribution has parameter . Moments explicit expressions To see that Dirichlet distribution is the conjugate prior for multinomial distribution, consider prior , and likelihood , where is the sample/result representing success out of trials for each object . Therefore, . We can intrepret this as \"given prior Dirichlet distribution (with param ) of probability vector for a total of objects and an observation vector , the posterior belief of the is a new Dirichlet Distribution with param ()\". Note the key points here are: Distributino has 2 parameters: the scale (or concentration) , and the base measure . A Dirichlet with small concentration favors extreme distributions, but this prior belief is very weak and is easily overwritten by data. It shall be seen as a generalization of Beta: Beta is a distribution over binomials (in an interval ); Dirichlet is a distribution over Multinomials (in the so-called simplex ). If we want to marginalize the parameters out (often used in ML models for parameter optimization) we can use the following formula: If we want to make prediction via conditional pdf of new data given previous data, we can use the following formula instead: Side noteThe above section gives a comprehensive view of dirichlet distribution. However, a more widely applied technique is Dirichlet Process. It is similar to Gaussian Process, but uses Dirichlet as conjugate prior instead on problems with multinomial likelihood (e.g. Latent Dirichlet Allocation). We've discussed this idea in the topic modeling blog. Interested readers can go that that blog for details. References Dirichlet distribution Lei Mao's Blog","link":"/post/dirichlet/"},{"title":"Ensemble Models: Overview","text":"OverviewAn important techinque in machine learning is ensemble models. It includes some very popular techniques like bootstraping and boosting. In the upcoming blogs, I will outline these models in detail, and give comparisons when necessary. The mathematical proofs are omitted for simplicity. However, I highly recommend interested readers to take a look at the theoretical foundations of these models to gain great intuitions about the ideas behind ensemble models. Intro An ensemble model is a composite model which combines a series of low performing or weak classifiers with the aim of creating a strong classifier. Here, individual classifiers vote and final prediction label returned that performs majority voting. Now, these individual classifiers are combined according to some specific criterion to create an ensemble model. These ensemble models offer greater accuracy than individual or base classifiers. These models can parallelize by allocating each base learner to different mechanisms. So, we can say that ensemble learning methods are meta-algorithms that combine several machine learning algorithms into a single predictive model to increase performance. Ensemble models are created according to some specific criterion as stated below: Bagging - They can be created to decrease model variance using bagging approach. Boosting - They can be created to decrease model bias using a boosting approach. Stacking - They can be created to improve model predictions using stacking approach. It can be depicted with the help of following diagram. Ensemble Machine Learning 1. Bagging Bagging stands for bootstrap aggregation. It combines multiple learners in a way to reduce the variance of estimates. For example, random forest trains N Decision Trees where we will train N different trees on different random subsets of the data and perform voting for final prediction. Bagging ensembles methods are Random Forest and Extra Trees. 2. Boosting Boosting algorithms are a set of the weak classifiers to create a strong classifier. Strong classifiers offer error rate close to 0. Boosting algorithm can track the model who failed the accurate prediction. Boosting algorithms are less affected by the overfitting problem. The following three algorithms have gained massive popularity in data science competitions AdaBoost (Adaptive Boosting) Gradient Tree Boosting (GBM) XGBoost We will discuss AdaBoost in this kernel and GBM and XGBoost in future posts. 3. Stacking Stacking (or stacked generalization) is an ensemble learning technique that combines multiple base classification models predictions into a new data set. This new data are treated as the input data for another classifier. This classifier employed to solve this problem. Stacking is often referred to as blending. Blending (average) ensemble model: Fits the base learners to the training data and then, at test time, average the predictions generated by all the base learners. Use VotingClassifier from sklearn that: fit all the base learners on the training data at test time, use all base learners to predict test data and then take the average of all predictions. Stacked ensemble model: Fits the base learners to the training data. Next, use those trained base learners to generate predictions (meta-features) used by the meta-learner (assuming we have only one layer of base learners). There are few different ways of training stacked ensemble model: Fitting the base learners to all training data and then generate predictions using the same training data it was used to fit those learners. This method is more prune to overfitting because the meta learner will give more weights to the base learner who memorized the training data better, i.e. meta-learner won't generate well and would overfit. Split the training data into 2 to 3 different parts that will be used for training, validation, and generate predictions. It's a suboptimal method because held out sets usually have higher variance and different splits give different results as well as learning algorithms would have fewer data to train. Use k-folds cross validation where we split the data into k-folds. We fit the base learners to the (k - 1) folds and use the fitted models to generate predictions of the held out fold. We repeat the process until we generate the predictions for all the k-folds. When done, refit the base learners to the full training data. This method is more reliable and will give models that memorize the data less weight. Therefore, it generalizes better on future data. 4. How are base-learners classified Base-learners are classified into two types. On the basis of the arrangement of base learners, ensemble methods can be divided into two groups. Parallel ensemble: base learners are generated in parallel for example - Random Forest. Sequential ensemble: base learners are generated sequentially for example AdaBoost. On the basis of the type of base learners, ensemble methods can be divided into two groups. Homogenous ensemble: uses the same type of base learner in each iteration. Heterogeneous ensemble: uses the different type of base learner in each iteration. Bagging vs Boosting1. Selecting the best technique- Bagging or Boosting Depends on the data, the simulation and the circumstances. Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability. If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model. By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting. In fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting. 2. Similarities between Bagging and Boosting Both are ensemble methods to get N learners from 1 learner. Both generate several training data sets by random sampling. Both make the final decision by averaging the N learners (or taking the majority of them i.e Majority Voting). Both are good at reducing variance and provide higher stability. 3. Differences between Bagging and Boosting Bagging is the simplest way of combining predictions that belong to the same type while Boosting is a way of combining predictions that belong to the different types. Bagging aims to decrease variance, not bias while Boosting aims to decrease bias, not variance. In Baggiing each model receives equal weight whereas in Boosting models are weighted according to their performance. In Bagging each model is built independently whereas in Boosting new models are influenced by performance of previously built models. In Bagging different training data subsets are randomly drawn with replacement from the entire training dataset. In Boosting every new subsets contains the elements that were misclassified by previous models. Bagging tries to solve over-fitting problem while Boosting tries to reduce bias. If the classifier is unstable (high variance), then we should apply Bagging. If the classifier is stable and simple (high bias) then we should apply Boosting. Bagging is extended to Random forest model while Boosting is extended to Gradient boosting.","link":"/post/ensemble-0/"},{"title":"Ensemble Models: Bagging Techniques","text":"OverviewWe have learnt about what bagging is in Ensemble Models: Overview, to recap, bagging is: In bagging (Bootstrap Aggregating), a set of weak learners are combined to create a strong learner that obtains better performance than a single one. Bagging helps to decrease the model’s variance. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.In this blog, we will use random forest as an example to illustrate how bagging worksBagging works as follows: Multiple subsets are created from the original dataset, selecting observations with replacement. A base model (weak model) is created on each of these subsets. The models run in parallel and are independent of each other. The final predictions are determined by combining the predictions from all the models. Next let's consider random forest, a model that fully utilized the idea of bagging in its procedure. Random Forest1. Definition A random forest consists of multiple random decision trees. Two types of randomnesses are built into the trees. First, each tree is built on a random sample from the original data. Second, at each tree node, a subset of features are randomly selected to generate the best split. (Key difference from Bagging algorithms) An ensemble model that is widely applied (as it can be parallelized) Designed to solve the overfitting issue in Decision Tree. The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias. Procedure Execute until every last combination is exhausted Bootstrapping: Create a bootstraped dataset: randomly select samples from the dataset until it reaches the same size as the original sample (we're allowed to pick the same sample more than once); Decision Tree Construction: Create a decision tree using the bootstraped dataset, but only use a random subset of variables/features at each step (i.e each decision node selection); Bagging: defined as bootstrapping the data plus using the aggregation to make a decision given a new instance, run through all the decision trees (the enitre random forest) and obtain the sum of votes for y = 1 and y = 0; (this step is called “aggregation”) decide on result of aggregation using the result with higher vote; Choose the most accurate random forest: Measure Accuracy based on the Out-of-Bag samples (CV) compute the Out-of-Bag Error as the # of samples which Bagging classifies wrongly Choice of number of variable used per step affects accuracy (optimized during CV): Usually choose the square root of the # of total variables and try a few settings above and below that value The low correlation between models is the key WARNING: RF is often considered as Bagging model while it is not always true, see this link 2. Pros &amp; ConsPros The power of handle large data sets with higher dimensionality (as each tree select much less features in its construction) The model outputs importance of variable, which can be a very handy feature (rf.feature_importance_) Balancing errors in data sets where classes are imbalanced. It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing. Using the out of bag error estimate for selection the most accurate random forest removes the need for a set aside test set. Cons It has very poor interpretability Does not work well for extrapolation to predict for data that is outside of the bounds of your original training data Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds. 3. Simple ImplementationThis is a template inspired by the Kaggle notebooks. I shall thank those writers whose code I borrowed from. Also note that here an aws s3 connection is made, which automatically makes the process parallelized. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierRSEED = 50# Load in datadf = pd.read_csv('https://s3.amazonaws.com/projects-rf/clean_data.csv')# Full dataset: https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system# Extract the labelslabels = np.array(df.pop('label'))# 30% examples in test datatrain, test, train_labels, test_labels = train_test_split(df, labels, stratify = labels, test_size = 0.3, random_state = RSEED)# Imputation of missing valuestrain = train.fillna(train.mean())test = test.fillna(test.mean())# Features for feature importancesfeatures = list(train.columns)# Create the model with 100 treesmodel = RandomForestClassifier(n_estimators=100, random_state=RSEED, max_features = 'sqrt', n_jobs=-1, verbose = 1)# Fit on training datamodel.fit(train, train_labels)n_nodes = []max_depths = []# Stats about the trees in random forestfor ind_tree in model.estimators_: n_nodes.append(ind_tree.tree_.node_count) max_depths.append(ind_tree.tree_.max_depth) print(f'Average number of nodes {int(np.mean(n_nodes))}')print(f'Average maximum depth {int(np.mean(max_depths))}')# Training predictions (to demonstrate overfitting)train_rf_predictions = model.predict(train)train_rf_probs = model.predict_proba(train)[:, 1]# Testing predictions (to determine performance)rf_predictions = model.predict(test)rf_probs = model.predict_proba(test)[:, 1]from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curveimport matplotlib.pyplot as plt# Plot formattingplt.style.use('fivethirtyeight')plt.rcParams['font.size'] = 18def evaluate_model(predictions, probs, train_predictions, train_probs): \"\"\"Compare machine learning model to baseline performance. Computes statistics and shows ROC curve.\"\"\" baseline = {} baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))]) baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))]) baseline['roc'] = 0.5 results = {} results['recall'] = recall_score(test_labels, predictions) results['precision'] = precision_score(test_labels, predictions) results['roc'] = roc_auc_score(test_labels, probs) train_results = {} train_results['recall'] = recall_score(train_labels, train_predictions) train_results['precision'] = precision_score(train_labels, train_predictions) train_results['roc'] = roc_auc_score(train_labels, train_probs) for metric in ['recall', 'precision', 'roc']: print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}') # Calculate false positive rates and true positive rates base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))]) model_fpr, model_tpr, _ = roc_curve(test_labels, probs) plt.figure(figsize = (8, 6)) plt.rcParams['font.size'] = 16 # Plot both curves plt.plot(base_fpr, base_tpr, 'b', label = 'baseline') plt.plot(model_fpr, model_tpr, 'r', label = 'model') plt.legend(); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves'); plt.show();evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)","link":"/post/ensemble-1/"},{"title":"Ensemble Models: Boosting Techniques","text":"Overview Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model. Let’s understand the way boosting works in the below steps. A subset is created from the original dataset. Initially, all data points are given equal weights. A base model is created on this subset. This model is used to make predictions on the whole dataset. Errors are calculated using the actual values and predicted values. The observations which are incorrectly predicted, are given higher weights. (Here, the three misclassified blue-plus points will be given higher weights) Another model is created and predictions are made on the dataset. (This model tries to correct the errors from the previous model) Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble. We will discuss 3 major boosting models: AdaBoost, Gradient Boost and XGBoost. AdaBoost1. Definition AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Stump: a tree with only 1 node and 2 leaves; Generally stumps does not perform as good as forest does; The AdaBoost uses the forest of stumps AdaBoost should meet two conditions: The classifier should be trained interactively on various weighed training examples. In each iteration, it tries to provide an excellent fit for these examples by minimizing training error. Complete Procedure Assign each sample with a weight (initially set to equal weight) each row in Dataframe has a equal weight Use the feature selection in decision node method to choose the first stump; Measure how well a stump classifies the samples using: where is the weight of and is the set of misclassified datapoints Determine the vote significance for the stump using Laplace smoothing for the vote significance: in case Total Error = 1 or 0, the formula will return error, we add a small value in the formula Modify the weight of samples so that next stump will take the errors that current stump made into account: 6.1 Run each sample down the stump 6.2 Compute new weight using: Formula: = New Sample Weight = Current Sample weight. = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and = place holder for 1 if stump correctly classified, -1 if misclassified. 6.3 Normalize the new weights With the new sample weight we can either: Use Weighted Gini Index to construct the next stump (Best feature for split) Use a new set of sample derived from the previous sample: pick until number of samples reach the size of original set construct an interval-selection scheme using the sum of new sample weight as cutoff value if a number falls in i-th interval between (0,1), choose i-th sample; - e.g (0-0.07:1; 0.07-0.14:2; 0.14-0.60:3;0.60-0.67:4; etc) randomly generate a number x between 0 and 1 pick the sample according to the scheme (note that the same sample can be repeatly picked) Repeat Step 1 to 7 until the entire forest is built 2. Pros and ConsPros Achieves higher performance than bagging when hyper-parameters tuned properly. Can be used for classification and regression equally well. Easily handles mixed data types. Can use \"robust\" loss functions that make the model resistant to outliers. AdaBoost is easy to implement. We can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. Cons Difficult and time-consuming to properly tune hyper-parameters. Cannot be parallelized like bagging (bad scalability when vast amounts of data). More risk of overfitting compared to bagging. AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. Slower as compared to XGBoost 3. Comparison with Random Forest Random Forest VS AdaBoost (Bagging vs Boosting) Random Forest uses full grown trees while Adaboost uses stumps (one root node with two leafs) In a Random Forest all the trees have similar amount of say, while in Adaboost some trees have more say than the other. In a random forest the order of the tree does not matter, while in Adaboost the order is important (especially since each tree is built by taking the error of the previous error). 4. Sample Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import pandas as pdimport numpy as npfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.model_selection import train_test_split,GridSearchCV, StratifiedShuffleSplitfrom sklearn.metrics import accuracy_scorefrom sklearn.preprocessing import StandardScalertrain = pd.read_pickle(\"train.pkl\")X = train.drop(['Survived'], axis = 1)y = train[\"Survived\"]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state=0)# Feature Scaling## We will be using standardscaler to transformst_scale = StandardScaler()## transforming \"train_x\"X_train = st_scale.fit_transform(X_train)## transforming \"test_x\"X_test = st_scale.transform(X_test)adaBoost = AdaBoostClassifier(base_estimator=None, learning_rate=1.0, n_estimators=100)adaBoost.fit(X_train, y_train)y_pred = adaBoost.predict(X_test)accuracy_score(y_test, y_pred)n_estimators = [100,140,145,150,160, 170,175,180,185];cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)learning_r = [0.1,1,0.01,0.5]parameters = {'n_estimators':n_estimators, 'learning_rate':learning_r }grid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree. ), param_grid=parameters, cv=cv, n_jobs = -1)grid.fit(X,y) print (grid.best_score_)print (grid.best_params_)print (grid.best_estimator_)adaBoost_grid = grid.best_estimator_adaBoost_grid.score(X,y) GBM (Gradient Boosting) Gradient Boosting trains many models in a gradual, additive and sequential manner (sequential + homogeneous). Major Motivation: allows one to optimise a user specified cost function, instead of a loss function that usually offers less control and does not essentially correspond with real world applications. Main logic: utilizes the gradient descent to pinpoint the challenges in the learners' predictions used previously. The previous error is highlighted, and, by combining one weak learner to the next learner, the error is reduced significantly over time. Procedure: For Regression Start by Compute the average of the , this is our 'initial prediction' for every sample Then compute the ; : Pseudo Residual at i-th sample : True value of i-th sample : Estimated value of i-th sample (here in first iteration) Construct a new decision tree (fixed size) with the goal of predicting the residuals (a DT of , not the true value!!!) If in a leaf, # of leaves &lt; # of samples, then put the of samples that fall into same category into the same leaf; Then take average of all values on that leaf as output values; Compute the new predicted value () : Newly Estimated value of i-th sample : learning rate, usually between 0 ~ 0.1 : Estimated pseudo residual values (deduced from the decision tree) Compute the new of each sample = Construct the new tree with the new pseudo residual in step 5: Repeat step 2, 3 Compute the new predicted value (here is deduced from a new DT): Compute the new pseudo residual of each sample Loop through the process UNTIL: adding additional trees does not significantly reduce the size of the pseudo residuals For Classification Set the initial prediction for every sample using ( is th probability of a sample being classified as 1) : (same for every sample) : log(odds) prediction for i-th sample, initially the same value, but value for each sample will change upon future iterations Using logistic function for classification: ; Decide on the classification: if &gt; threshold, then \"Yes\"; else \"No\"; here the threshold may not be 0.5 (AUC and ROC to decide on the value); Compute Build a DT using the pseudo residual Transformation of the pseudo residual to obtain the output values on each leaf: e.g: if a leaf has (0.3, -0.7), then the leaf output value Compute the new prediction : log(odds) prediction for i-th sample in new iteration : learning rate, usually between 0 ~ 0.1 Compute the new Probability Compute the new predicted value for each sample; Compute the new pseudo residual for each sample; Build the new tree; Loop until the pseudo residual does not change significantly; Early Stopping: Early Stopping performs model optimisation by monitoring the model’s performance on a separate test data set and stopping the training procedure once the performance on the test data stops improving beyond a certain number of iterations. It avoids overfitting by attempting to automatically select the inflection point where performance on the test dataset starts to decrease while performance on the training dataset continues to improve as the model starts to overfit. In the context of gbm, early stopping can be based either on an out of bag sample set (\"OOB\") or cross- validation (\"cv\"). 1. Pros &amp; ConsPros Robust against bias/outliers GBM can be used to solve almost all objective function that we can write gradient out, some of which RF cannot resolve Able to reduce bias and remove some extreme variances Cons More sensitive to overfitting if the data is noisy GBDT training generally takes longer because of the fact that trees are built sequentially Prone to overfitting, but can be overcame by parameter optimization 2. AdaBoost vs GBM Both AdaBoost and Gradient Boosting build weak learners in a sequential fashion. Originally, AdaBoost was designed in such a way that at every step the sample distribution was adapted to put more weight on misclassified samples and less weight on correctly classified samples. The final prediction is a weighted average of all the weak learners, where more weight is placed on stronger learners. Later, it was discovered that AdaBoost can also be expressed as in terms of the more general framework of additive models with a particular loss function (the exponential loss). So, the main differences between AdaBoost and GBM are as follows: The main difference therefore is that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function (Exponential loss function). Hence, gradient boosting is much more flexible. AdaBoost can be interepted from a much more intuitive perspective and can be implemented without the reference to gradients by reweighting the training samples based on classifications from previous learners. In Adaboost, shortcomings are identified by high-weight data points while in Gradient Boosting, shortcomings of existing weak learners are identified by gradients. Adaboost is more about 'voting weights' and Gradient boosting is more about 'adding gradient optimization'. Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances. AdaBoost use simple stumps as learners, while the fixed size trees of GBM are usually of maximum leaf number between 8 and 32; Adaboost corrects its previous errors by tuning the weights for every incorrect observation in every iteration, but gradient boosting aims at fitting a new predictor in the residual errors committed by the preceding predictor. 3. Random Forest vs GBM GBMs are harder to tune than RF. There are typically three parameters: number of trees, depth of trees and learning rate, and each tree built is generally shallow. RF is harder to overfit than GBM. RF runs in parallel while GBM runs in sequence 4. Application A great application of GBM is anomaly detection in supervised learning settings where data is often highly unbalanced such as DNA sequences, credit card transactions or cybersecurity. 5. Sample Code implementation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auctrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")train.set_index(\"PassengerId\", inplace=True)test.set_index(\"PassengerId\", inplace=True)# generate training target set (y_train)y_train = train[\"Survived\"]# delete column \"Survived\" from train settrain.drop(labels=\"Survived\", axis=1, inplace=True)train_test = train.append(test)# delete columns that are not used as features for training and predictioncolumns_to_drop = [\"Name\", \"Age\", \"SibSp\", \"Ticket\", \"Cabin\", \"Parch\", \"Embarked\"]train_test.drop(labels=columns_to_drop, axis=1, inplace=True)# convert objects to numbers by pandas.get_dummiestrain_test_dummies = pd.get_dummies(train_test, columns=[\"Sex\"])train_test_dummies.fillna(value=0.0, inplace=True)# generate feature sets (X)X_train = train_test_dummies.values[0:891]X_test = train_test_dummies.values[891:]scaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)X_test_scale = scaler.transform(X_test)X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0)learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]for learning_rate in learning_rates: gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0) gb.fit(X_train_sub, y_train_sub) print(\"Learning rate: \", learning_rate) print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub))) print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub))) print() gb = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 2, random_state = 0)gb.fit(X_train_sub, y_train_sub)predictions = gb.predict(X_validation_sub)print(\"Confusion Matrix:\")print(confusion_matrix(y_validation_sub, predictions))print()print(\"Classification Report\")print(classification_report(y_validation_sub, predictions))y_scores_gb = gb.decision_function(X_validation_sub)fpr_gb, tpr_gb, _ = roc_curve(y_validation_sub, y_scores_gb)roc_auc_gb = auc(fpr_gb, tpr_gb) XGBoost An optimized GBM Evolution of XGBoost from Decision Tree Procedure: For Regression Set initial value (by default is 0.5 [for both regression and classification]) Build the first XGBoost Tree (a unique type of regression tree): Start with a root containing all the residuals ; Compute similarity score where is the Regularization parameter. Make a decision on spliting condition: For each consecutive samples, compute the mean k of 2 input as the threshold for decision node; then split by the condition feature_value &lt; k Decide the best thresold for spliting: Adopt the threshold that gives the largest Gain For example: we have points {}: Step 1: set the first threshold be Step 2: now left node has {x1}, right node has {x2, x3}, we have and Step 3: Compute Step 4: Compute the second threshold and new Gain using this thresold Step 5: Since gain of threshold 1 is greater than that of threshold 2, we use as the spliting threshold If the leaf after spliting has &gt; 1 residual, consider whether to split again (based on the residuals in the leaf); continue until it reaches the max_depth (default is 6) or no more spliting is possible Notes on A larger leads to greater likelihood of prunning as the are lower; The reduce the prediction’s sensitivity to nodes with low # of observations Prune the Tree From bottom branch up, decide on whether to prune the node/branch : The threshold to determine if a Gain is large enough to be kept if then prune (remove the branch); Note that setting does not turn off prunnig!!! If we prune every branch until it reaches the root, then remove the tree; Compute Compute New prediction : Learning rate, default value = 0.3 : Output value of in each residual tree Compute the new residuals for all samples, build the next tree and prune the tree; Repeat the process just like Gradient Boost does; As more trees are built, the Gains will decease; We stop until the Gain &lt; terminating value For Classification Set initial value (by default is 0.5) Build the first XGBoost Tree: Start with a root containing all the residuals Compute similarity score where is the Regularization parameter. Repeat the same procedure as the regression does; Compute all the Gains Warning of Cover: defined for the minimum number of residuals in each leaf (by default is 1) in Regression: = # of Residuals in the leaf (always &gt;= 1) in Classification: = (not necessarily &gt;= 1), hence some leafs violating the Cover threshold will be removed. Here Cover needs to be carefully chosen (like 0, 0.1, etc) Prune the tree: same procedure as Regression case Compute Compute New prediction : Learning rate, default value = 0.3 : Output value of in each residual tree (here is the first tree) Convert into using logistic regression: Compute the new residuals for all samples, build the next tree and prune the tree; Repeat the process just like Gradient Boost does; As more trees are built, the Gains will decease; We stop until the Gain &lt; terminating value; Prediction: ; 1. Advantage of XGBoost Parallelized Tree Building Unlike GBM, XGBoost is able to build the sequential tree using a parallelized implementation This is possible due to the interchangeable nature of loops used for building base learners: the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation. Tree Pruning using depth-first approach The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses 'max_depth' parameter as specified instead of criterion first, and starts pruning trees backward. (This 'depth-first' approach improves computational performance significantly.) Cache awareness and out-of-core computing allocating internal buffers in each thread to store gradient statistics. Further enhancements such as 'out-of-core' computing optimize available disk space while handling big data-frames that do not fit into memory. Regularization It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting. Efficient Handling of missing data XGboost decides at training time whether missing values go into the right or left node. It chooses which to minimise loss. If there are no missing values at training time, it defaults to sending any new missings to the right node. In-built cross-validation capability The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run. LightGBM A follow-up (and competitor) from XGBoost Generally the same as GBM, except that a lot of optimizations are done, see this page to view all of them Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. It is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. 1. Advantages of Light GBM Faster training speed and higher efficiency: Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. Lower memory usage: Replaces continuous values to discrete bins which result in lower memory usage. Better accuracy than any other boosting algorithm: It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter. Compatibility with Large Datasets: It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. Parallel learning supported 2. Code sample: XGBoost vs LightGBM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#importing standard libraries import numpy as np import pandas as pd from pandas import Series, DataFrame #import lightgbm and xgboost import lightgbm as lgb import xgboost as xgb #loading our training dataset 'adult.csv' with name 'data' using pandas data=pd.read_csv('adult.csv',header=None) #Assigning names to the columns data.columns=['age','workclass','fnlwgt','education','education-num','marital_Status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','Income'] #glimpse of the dataset data.head() # Label Encoding our target variable from sklearn.preprocessing import LabelEncoder,OneHotEncoderl=LabelEncoder() l.fit(data.Income) l.classes_ data.Income=Series(l.transform(data.Income)) #label encoding our target variable data.Income.value_counts() #One Hot Encoding of the Categorical features one_hot_workclass=pd.get_dummies(data.workclass) one_hot_education=pd.get_dummies(data.education) one_hot_marital_Status=pd.get_dummies(data.marital_Status) one_hot_occupation=pd.get_dummies(data.occupation)one_hot_relationship=pd.get_dummies(data.relationship) one_hot_race=pd.get_dummies(data.race) one_hot_sex=pd.get_dummies(data.sex) one_hot_native_country=pd.get_dummies(data.native_country) #removing categorical features data.drop(['workclass','education','marital_Status','occupation','relationship','race','sex','native_country'],axis=1,inplace=True) #Merging one hot encoded features with our dataset 'data' data=pd.concat([data,one_hot_workclass,one_hot_education,one_hot_marital_Status,one_hot_occupation,one_hot_relationship,one_hot_race,one_hot_sex,one_hot_native_country],axis=1) #removing dulpicate columns _,i = np.unique(data.columns, return_index=True) data=data.iloc[:, i] #Here our target variable is 'Income' with values as 1 or 0. #Separating our data into features dataset x and our target dataset y x=data.drop('Income',axis=1) y=data.Income #Imputing missing values in our target variable y.fillna(y.mode()[0],inplace=True) #Now splitting our dataset into test and train from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3)#The data is stored in a DMatrix object #label is used to define our outcome variabledtrain=xgb.DMatrix(x_train,label=y_train)dtest=xgb.DMatrix(x_test)#setting parameters for xgboostparameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}#training our model num_round=50from datetime import datetime start = datetime.now() xg=xgb.train(parameters,dtrain,num_round) stop = datetime.now()#Execution time of the model execution_time_xgb = stop-start print(f'execution_time_xgb: {execution_time_xgb}')#datetime.timedelta( , , ) representation $\\implies$ (days , seconds , microseconds) #now predicting our model on test set ypred=xg.predict(dtest) display(ypred)#Converting probabilities into 1 or 0 for i in range(0, 9769): if ypred[i] &gt;= .5: ypred[i] = 1 else: ypred[i]=0 #calculating accuracy of our model from sklearn.metrics import accuracy_score accuracy_xgb = accuracy_score(y_test,ypred) print(f'accuracy_xgb: {accuracy_xgb}')train_data=lgb.Dataset(x_train,label=y_train)#setting parameters for lightgbmparam = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}param['metric'] = ['auc', 'binary_logloss']#Here we have set max_depth in xgb and LightGBM to 7 to have a fair comparison between the two.#training our model using light gbmnum_round=50start=datetime.now()lgbm=lgb.train(param,train_data,num_round)stop=datetime.now()#Execution time of the modelexecution_time_lgbm = stop-startprint(f'execution_time_lgbm: {execution_time_lgbm}')#predicting on test setypred2=lgbm.predict(x_test)display(ypred2[0:5]) # showing first 5 predictions#converting probabilities into 0 or 1for i in range(0,9769): if ypred2[i]&gt;=.5: # setting threshold to .5 ypred2[i]=1 else: ypred2[i]=0 #calculating accuracyaccuracy_lgbm = accuracy_score(ypred2,y_test)print(f'accuracy_lgbm: {accuracy_lgbm}')display(y_test.value_counts())from sklearn.metrics import roc_auc_score#calculating roc_auc_score for xgboostauc_xgb = roc_auc_score(y_test,ypred)print(f'auc_xgb: {auc_xgb}')#calculating roc_auc_score for light gbm. auc_lgbm = roc_auc_score(y_test,ypred2)print(f'auc_lgbm: {auc_lgbm}')comparison_dict = {'accuracy score':(accuracy_lgbm, accuracy_xgb),'auc score':(auc_lgbm,auc_xgb),'execution time':(execution_time_lgbm, execution_time_xgb)}#Creating a dataframe ‘comparison_df’ for comparing the performance of Lightgbm and xgb. comparison_df = DataFrame(comparison_dict) comparison_df.index= ['LightGBM','xgboost'] display(comparison_df) 3. General Pros and cons of boostingPros Achieves higher performance than bagging when hyper-parameters tuned properly. Can be used for classification and regression equally well. Easily handles mixed data types. Can use “robust” loss functions that make the model resistant to outliers. Cons Difficult and time consuming to properly tune hyper-parameters. Cannot be parallelized like bagging (bad scalability when huge amounts of data). More risk of overfitting compared to bagging. ConclusionHere we end the discussion about ensemble models. It was a fun and challenging topic. While most users of these model won't need to understand every nitty-gritty of these models, these profound theories laid significant foundations for future research on supervised ensemble learning models (and even meta-learning). In the next month, I'll share some posts about unsupervised learning. This is even large a topic, and I expect the content to be even deeper. Good luck, me and everyone!","link":"/post/ensemble-2/"},{"title":"The Data Mining Triology: III. Analysis","text":"OverviewFinally we have come to the last part in fundamental data mining. This is where people's analytical power shine through. However, we also highlight some cautions engineers should practice in exploratory analysis. While data analysis is fascinating, I feel that building models based on the analysis to facilitate business decisions is even more exciting. This heavily relies on machine learning models and artificial intelligence toolkits. I've also written (and will write more in the future) blogs on these topics. Word of reminder: these models require some level of statistical and mathematical foundations, so it really depends on one's interests in developing these models. A general view of the datasetOne can always use an easy trick: YourDataFrameName.describe() to show the details about your data entries. This gives very good view of properties of your data. A sample output looks like 12345678910111213&gt;&gt;&gt; df.describe(include='all') categorical numeric objectcount 3 3.0 3unique 3 NaN 3top f NaN afreq 1 NaN 1mean NaN 2.0 NaNstd NaN 1.0 NaNmin NaN 1.0 NaN25% NaN 1.5 NaN50% NaN 2.0 NaN75% NaN 2.5 NaNmax NaN 3.0 NaN Next let's look into numerical data's pattern first. Numerical data distributions1. Generate comprehensive view for the numericals in Data Set1234list(set(df.dtypes.tolist()))df_num = df.select_dtypes(include = ['float64', 'int64']) # select only numerical datadisplay(df_num.head()) # output the first 5 entriesdf_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8) # Give the comprehensive views of all the distribution (in histogram) of the numerical values; Key steps:(i) From the graphs, find which features have similar distributions;(ii) Document the discovery for further investigation; 2. Correlation (correlation is affected by outliers)Find the strongly correlated values with the output. Call this list of values golden_features_list. 123df_num_corr = df_num.corr()['SalePrice'][:-1] # -1 because the latest row is SalePricegolden_features_list = df_num_corr[abs(df_num_corr) &gt; 0.5].sort_values(ascending=False)print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list)) 3. Correlation (outliers removal) Plot the numerical features and see which ones have very few or explainable outliers Remove the outliers from these features and see which one can have a good correlation without their outliers 1234for i in range(0, len(df_num.columns), 5):sns.pairplot(data=df_num, x_vars=df_num.columns[i:i+5], y_vars=['SalePrice']) Key steps:(i) Spot any clear outliers, document. Think of outlier's plausibility. Think of whether to remove it &amp; document;(ii) Spot any clearly linear/non-linear relationships, document;(iii) Spot any distribution with a lot of 0's: do Correlation (0 Removal); 4. Correlation (0 Removal)Removing all 0's in some columns and generate golden_features_list again, see if any new features are added. 123456789101112131415import operatorindividual_features_df = []for i in range(0, len(df_num.columns) - 1): # -1 because the last column is SalePrice tmpDf = df_num[[df_num.columns[i], 'SalePrice']] tmpDf = tmpDf[tmpDf[df_num.columns[i]] != 0] individual_features_df.append(tmpDf)all_correlations = {feature.columns[0]: feature.corr()['SalePrice'][0] for feature in individual_features_df}all_correlations = sorted(all_correlations.items(), key=operator.itemgetter(1))for (key, value) in all_correlations: print(\"{:&gt;15}: {:&gt;15}\".format(key, value)) golden_features_list = [key for key, value in all_correlations if abs(value) &gt;= 0.5]print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list)) Finally, we can give conclusion with respect to the Numerical data distribution analysis. Feature to feature (Categorical to Categorical) Correlation Analysis: Heat map for features: Steps of Analysis for the Heatmap: First of all, remove all simple correlations (easy to explain &amp; not that relevant) Next, identify the relationships that are pertinent to the questoin/task Lastly, conclude the features that are similar to be easily combined/ need further investigation/ clearly helpful to the task Document the analysis; 1234567corr = df_num.drop('SalePrice', axis=1).corr() # We already examined SalePrice correlationsplt.figure(figsize=(12, 10))sns.heatmap(corr[(corr &gt;= 0.5) | (corr &lt;= -0.4)], cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1, annot=True, annot_kws={\"size\": 8}, square=True)# this generates the heatmap that display highly related features;# Note that this map only displays bidirectional relationships;# We cannot conclude much if there are relationships among feature set of size &gt;= 3; Q –&gt; QQ to Q stands for \"Quantitative to Quantitative relationship\", which is often found in a pure numeric dataset. For qualitative relationships, tricks like counting and sorting can also be used to transform the qualitative data into numeric ones for Q to Q analysis. Extract strongly correlated quantitative features123features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]features_to_analyse.append('SalePrice')display(features_to_analyse) plot the distribution:1234fig, ax = plt.subplots(round(len(features_to_analyse) / 3), 3, figsize = (18, 12))for i, ax in enumerate(fig.axes): if i &lt; len(features_to_analyse) - 1: sns.regplot(x=features_to_analyse[i],y='SalePrice', data=df[features_to_analyse], ax=ax) Analysis of the distribution: Since Linear Regression is give, we focus on analyzing the spread of the data in each graph C –&gt; Q (Categorical to Quantitative relationship)C to Q stands for \"Categorical to Quantitative relationship\". This is different from qualitative or quantitive relationships, as we cannot compare the degree of desired attributes based on category number themselves. Thus we should see how these attributes are manipulated to make the relationship interpretable. Extract Categorical features123categorical_features = [a for a in quantitative_features_list[:-1] + df.columns.tolist() if (a not in quantitative_features_list[:-1]) or (a not in df.columns.tolist())]df_categ = df[categorical_features]df_not_num = df_categ.select_dtypes(include = ['O']) # include the non-numerical features Apply Boxplot123plt.figure(figsize = (12, 6))ax = sns.boxplot(x='SaleCondition', y='SalePrice', data=df_categ) # can replace \"SaleCondition\" with other features Apply Distribution plot123456fig, axes = plt.subplots(round(len(df_not_num.columns) / 3), 3, figsize=(12, 30))for i, ax in enumerate(fig.axes): if i &lt; len(df_not_num.columns): ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45) sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)fig.tight_layout() Through these plots, we can see that some categories are predominant for some features such as Utilities, Heating, GarageCond, Functional… These features may not be relevant for our predictive model. ConclusionThe methods above cover a wide range of tools being applied in data analytics. There are definitely many more directions in EDA, and I'll update my discovery every time I find some interesting things.","link":"/post/exploratory-data-analysis/"},{"title":"Feature Selection &amp; Model Selections","text":"OverviewRunning machine learning models have become much easier in recent years. The prevalence of tutorials and model packages makes it much more convenient for people to apply various theoretically complex algorithms on their datasets and thrive. So to excel in the field of data science, one cannot simple KNOW how to use models, but also appreciate each model's significance and select proper models wisely. That's where feature selections and model selections come in. Both turn out to be challenging and extremely useful in the same time. In light of this, I want to take down the notes I learned through practice and tutorials some key aspects of these two things. Feature Selection Benefits It enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret. It improves the accuracy of a model if the right subset is chosen. It reduces Overfitting Methods Here we discuss about some widely used methods for feature selections. To facilitate the demo code, we require the following packages to be applied and data being tuned: 123456789101112131415161718from sklearn.datasets import load_bostonimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport statsmodels.api as smfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lassofrom sklearn.feature_selection import RFE%matplotlib inline#Loading the datasetx = load_boston()df = pd.DataFrame(x.data, columns = x.feature_names)df[\"MEDV\"] = x.targetX = df.drop(\"MEDV\",1) #Feature Matrixy = df[\"MEDV\"] #Target Variable Filter Methods No mining algorithm included Uses the exact assessment criterion which includes distance, information, dependency, and consistency. The filter method uses the principal criteria of ranking technique and uses the rank ordering method for variable selection. Generally used as a dasta preprocessing step Several main filter methods based on the variable attributes: filter methods Wrapper Methods workflow: filter methods - Use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset Computationally expensive 3 Types: Forward Selection: An iterative method Start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model. Backward Elimination: An iterative method Start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features. E.g. If the p-value is above 0.05 then we remove the feature, else we keep it. 12345678910111213141516171819202122# Adding constant column of ones, mandatory for sm.OLS modelX_1 = sm.add_constant(X)# Fitting sm.OLS modelmodel = sm.OLS(y,X_1).fit()display(model.pvalues)# Backward Eliminationcols = list(X.columns)pmax = 1while (len(cols)&gt;0): p= [] X_1 = X[cols] X_1 = sm.add_constant(X_1) model = sm.OLS(y,X_1).fit() p = pd.Series(model.pvalues.values[1:],index = cols) pmax = max(p) feature_with_p_max = p.idxmax() if(pmax&gt;0.05): cols.remove(feature_with_p_max) else: breakselected_features_BE = colsprint(selected_features_BE) Recursive Feature elimination: A greedy optimization algorithm It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination 1234567891011model = LinearRegression()#Initializing RFE modelrfe = RFE(model, 7)#Transforming data using RFEX_rfe = rfe.fit_transform(X,y) #Fitting the data to modelmodel.fit(X_rfe,y)print(rfe.support_)print(rfe.ranking_)&gt;&gt;&gt; [False False False True True True False True True False True False True]&gt;&gt;&gt; [2 4 3 1 1 1 7 1 1 5 1 6 1] Here we took LinearRegression model with 7 features and RFE gave feature ranking as above, but the selection of number '7' was random. Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to 13. We then take the one for which the accuracy is highest. 1234567891011121314151617181920#no of featuresnof_list=np.arange(1,13) high_score=0#Variable to store the optimum featuresnof=0 score_list =[]for n in range(len(nof_list)): X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0) model = LinearRegression() rfe = RFE(model,nof_list[n]) X_train_rfe = rfe.fit_transform(X_train,y_train) X_test_rfe = rfe.transform(X_test) model.fit(X_train_rfe,y_train) score = model.score(X_test_rfe,y_test) score_list.append(score) if(score&gt;high_score): high_score = score nof = nof_list[n]print(\"Optimum number of features: %d\" %nof)print(\"Score with %d features: %f\" % (nof, high_score)) As seen from above code, the optimum number of features is 10. We now feed 10 as number of features to RFE and get the final set of features given by RFE method 1234567891011cols = list(X.columns)model = LinearRegression()#Initializing RFE modelrfe = RFE(model, 10) #Transforming data using RFEX_rfe = rfe.fit_transform(X,y) #Fitting the data to modelmodel.fit(X_rfe,y) temp = pd.Series(rfe.support_,index = cols)selected_features_rfe = temp[temp==True].indexprint(selected_features_rfe) (*) Bidirectional Elimination: A combination of Forward Selection &amp; Backword Elimination Self-defined Methods There are many interesting methods that can be directly applied in experimentations. However, one method that caught my eyes is the Boruta method: Boruta Method (Using shadow features and random forest) The main reason I liked this is because its application on Random Forest and XGBoost models. It generally works well with well structured data and relatively smaller datasets. In the hindsight, it is still relatively slower as compared to some simpler selection criterion, and it does not handle multicollinearity immediately. checkout this python tutorial for more details Embedded Methods It combines the qualities of filter and wrapper methods. It's implemented by algorithms that have their own built-in feature selection methods Workflow Embedded Method Workflow Here in the demo code we will do feature selection using Lasso regularization. If the feature is irrelevant, lasso penalizes it's coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken. 1234567891011121314reg = LassoCV()reg.fit(X, y)print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)print(\"Best score using built-in LassoCV: %f\" % reg.score(X,y))&gt;&gt;&gt; Best alpha using built-in LassoCV: 0.724820&gt;&gt;&gt; Best score using built-in LassoCV: 0.702444coef = pd.Series(reg.coef_, index = X.columns)print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" + str(sum(coef == 0)) + \" variables\")&gt;&gt;&gt; Lasso picked 10 variables and eliminated the other 3 variablesimp_coef = coef.sort_values()import matplotlibmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)imp_coef.plot(kind = \"barh\")plt.title(\"Feature importance using Lasso Model\") Filter vs Wrapper Now let us make a comparison between filter methods and wrapper methods, the two most commonly used ways in feature selection. Characteristics Filter Method Wrapper Methods Measure of feature relevance correlation with dependent variable actually training a model on a subset of feature Speed Much faster Slower due to model training Performance Evaluation statistical methods for evaluation Model results cross validation Quality of feature set selected May be suboptimal Guaranteed to output optimal/near-optimal feature set Overfitting ? Less likely Much more prone to Model SelectionHere we must clarify one important conceptual misunderstanding: Note: Classical Model selection mainly focuses on performing metrics evaluations through different models, tuning the model parameter and variating the training datasets. The choice of model in the end is often manual. Hence, it differs from the automated model selection procedure where the final selection of model is also done automatically. The latter is often known as AutoML, and has gained quick wide popularity in recent years. We now think about what are the main strategies to improve model performance: Use a more complicated/more flexible model Use a less complicated/less flexible model Tuning hyperparameters Gather more training samples Gather more data to add features to each sampleClearly, the first 4 are model selection strategies, and the last one is feature selection. When we make these adjustments, we must keep in mind the The Bias-variance trade-off: bias: Usually the case where the model underfits, i.e. it does not have enough model flexibility to suitably account for all the features in the data variance: Usually the case where the model overfits, i.e. so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution For high-bias models, the performance of the model on the validation set is similar to the performance on the training set. For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set. We can easily visualize this via the learning curve Plot 1: The curve to find the best amount of train set size (too low –&gt; high variance; too high –&gt; high bias) In the meantime, we observe from the validation curve below that model complexity/hyperparameter choices affect the model performances as well Plot 2: The curve to find the best hyperparameters For more details on metrics evaluation and hyperparameter tuning with feedback from validation sets, interested readers can read my blogs on these topics as well.","link":"/post/feature-and-model-selections/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it’s the task of minimizing the cost/loss function parameterized by the model’s parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don’t have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it’s negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step. Now let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent’s variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent%20copy%202/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it’s the task of minimizing the cost/loss function parameterized by the model’s parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don’t have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it’s negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step. Now let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent’s variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent%20copy%203/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it’s the task of minimizing the cost/loss function parameterized by the model’s parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don’t have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it’s negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step. Now let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent’s variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent%20copy/"},{"title":"Gradient Descent Algorithm and Its Variants!","text":"Overview of Gradient DescentOptimization refers to the task of minimizing/maximizing an objective function parameterized by . In machine/deep learning terminology, it’s the task of minimizing the cost/loss function parameterized by the model’s parameters . Optimization algorithms (in case of minimization) have one of the following goals: Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems. There are three kinds of optimization algorithms: Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression. Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates. Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate . Therefore, we follow the direction of the slope downhill until we reach a local minimum. In this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent. Let’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight and bias . Initialize weight and bias to any random numbers. Pick a value for the learning rate . The learning rate determines how big the step would be on each iteration. If is very small, it would take long time to converge and become computationally expensive. IF is large, it may fail to converge and overshoot the minimum. Therefore, plot the cost function against different values of and pick the value of that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (Figure 1). Figure 2 The most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3. Make sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (Figure 2). Figure 2 Scale the data to have and . Below is the formula for scaling each example: On each iteration, take the partial derivative of the cost function w.r.t each parameter (gradient): The update equations are: For the sake of illustration, assume we don’t have bias. If the slope of the current values of , this means that we are to the right of optimal . Therefore, the update will be negative, and will start getting close to the optimal values of . However, if it’s negative, the update will be positive and will increase the current values of to converge to the optimal values of (Figure 3): Figure 3 Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change. In addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step. Now let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step). Batch Gradient DescentBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: manual gradient update123for i in range(num_epochs): grad = compute_gradient(data, params) params = params - learning_rate * grad The main advantages: We can use fixed learning rate during training without worrying about learning rate decay. It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. It has unbiased estimate of gradients. The more the examples, the lower the standard error. The main disadvantages: Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets. Each step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update. Mini-Batch Gradient DescentInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of examples: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch. manual gradient update minibatch12345for i in range(num_epochs): np.random.shuffle(data) for batch in radom_minibatches(data, batch_size=32): grad = compute_gradient(batch, params) params = params - learning_rate * grad The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2. The main advantages: Faster than Batch version because it goes through a lot less examples than Batch (all examples). Randomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning. With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur. The main disadvantages: It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges. Due to the noise, the learning steps have more oscillations (see figure 5) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum. Figure 4 With large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size , we get the Batch Gradient Descent. Stochastic Gradient DescentInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example . Therefore, learning happens on every example: Shuffle the training dataset to avoid pre-existing order of examples. Partition the training dataset into examples. manual gradient update stochastic12345for i in range(num_epochs): np.random.shuffle(data) for example in data: grad = compute_gradient(example, params) params = params - learning_rate * grad It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD: It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time. We can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step. Below is a graph that shows the gradient descent’s variants and their direction towards the minimum: Figure 5 As the figure above shows, SGD direction is very noisy compared to mini-batch. Areas for advancementBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch: Gradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if: Second derivative = 0 the curvature is linear. Therefore, the step size = the learning rate . Second derivative &gt; 0 the curvature is going upward. Therefore, the step size &lt; the learning rate and may lead to divergence. Second derivative &lt; 0 the curvature is going downward. Therefore, the step size &gt; the learning rate . As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function. The norm of the gradient is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8). In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive. As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets. All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.","link":"/post/gradient-descent/"},{"title":"An overview of Hidden markov model and its algorithms","text":"OverviewStochastic Process is a critical piece of knowledge in statistical learning. It's also an important piece in the increasing popular reinforcement learning field. I feel like I might apply its algorithms or do research work on it in the future. Hence, I create this blog to introduce an important concept, hidden markov model (HMM), and some useful algorithms and their intuitions for HMM. Markov ChainThe HMM is based on augmenting the Markov Chain(MC), which is a type of a random process about a set of states. The transition from one state to another depends on certain probabilities , which we define as transition probability. The nice property of an MC is that the transition probability of only depends on the previous state , i.e. . We call it the Markov Assumption. This property allows us to produce an each transition graph like this[1] [1]:https://web.stanford.edu/~jurafsky/slp3/A.pdf HMM DefinitionA Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don't normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. A hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. In HMM, we use observations to describe observed events with values denoted by , and states to describe hidden events with values denoted by . Note that Markov Assumption still holds. In addition, we have output independence . In HMM, we try to solve the following problems: Problem 1 (Likelihood): Given an HMM with transition probabilities and observation probabilities and an observation sequence , determine the likelihood . Problem 2 (Decoding): Given an observation sequence and an HMM , discover the best hidden state sequence . Problem 3 (Learning): Given an observation sequence and the set of states in the HMM, learn the HMM parameters and . Algorithms1. The Forward Algorithm - Likelihood solverThe forward algorithm is a dynamic programming method that computes the observation probability by summing over the probabilities of all possible hidden state paths that could generate the observation sequence, but it does so efficiently by implicitly folding each of these paths into a single forward trellis, which computes the probability of being in state after seeing the first observations, given the parameteres , i.e. From above, we can quickly derive the result by: first initialize . Recrusively apply the above expression (1) for . Compute 2. The Viterbi Algorithm - Decoding solverLike forwarding algorithm, Viterbi is also DP that makes uses of a dynamic programming Viterbi trellis. The idea is to process the observation sequence left to right, filling out the trellis. Each cell of the trellis, , represents the probability that the HMM is in state after seeing the first observations and passing through the most probable state sequence , given the parameters . The value of each cell is computed by recursively taking the most probable path that could lead us to this cell. Here, a major difference from (1) is that we now take the most probable of the extensions of the paths that lead to the current cell. In addition to the max value , we shall also keep track of the solution This is called backpointers. We need this value because while the forward algorithm needs to produce an observation likelihood, the Viterbi algorithm must produce a probability and also the most likely state sequence. We compute this best state sequence by keeping track of the path of hidden states that led to each state, and Viterbi backtrace then at the end backtracing the best path to the beginning (the Viterbi backtrace). Finally, we can compute the optimal score and path We use a demo code to illustrate the process. The code for forwarding algorithm and Forward-Backward Algorithm can be implmented in a similar fashion. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import numpy as npimport pandas as pd# Define the problem setupobs_map = {'Cold':0, 'Hot':1}obs = np.array([1,1,0,1,0,0,1,0,1,1,0,0,0,1])inv_obs_map = dict((v,k) for k, v in obs_map.items())obs_seq = [inv_obs_map[v] for v in list(obs)]print(\"Simulated Observations:\\n\",pd.DataFrame(np.column_stack([obs, obs_seq]),columns=['Obs_code', 'Obs_seq']) )pi = [0.6,0.4] # initial probabilities vectorstates = ['Cold', 'Hot']hidden_states = ['Snow', 'Rain', 'Sunshine']pi = [0, 0.2, 0.8]state_space = pd.Series(pi, index=hidden_states, name='states')a_df = pd.DataFrame(columns=hidden_states, index=hidden_states)a_df.loc[hidden_states[0]] = [0.3, 0.3, 0.4]a_df.loc[hidden_states[1]] = [0.1, 0.45, 0.45]a_df.loc[hidden_states[2]] = [0.2, 0.3, 0.5]print(\"\\n HMM matrix:\\n\", a_df)a = a_df.valuesobservable_states = statesb_df = pd.DataFrame(columns=observable_states, index=hidden_states)b_df.loc[hidden_states[0]] = [1,0]b_df.loc[hidden_states[1]] = [0.8,0.2]b_df.loc[hidden_states[2]] = [0.3,0.7]print(\"\\n Observable layer matrix:\\n\",b_df)b = b_df.values# Apply the Viterbi Algorithm based on http://www.blackarbs.com/blog/introduction-hidden-markov-models-python-networkx-sklearn/2/9/2017def viterbi(pi, a, b, obs): nStates = np.shape(b)[0] T = np.shape(obs)[0] # init blank path path = path = np.zeros(T,dtype=int) # delta --&gt; highest probability of any path that reaches state i delta = np.zeros((nStates, T)) # phi --&gt; argmax by time step for each state phi = np.zeros((nStates, T)) # init delta and phi delta[:, 0] = pi * b[:, obs[0]] phi[:, 0] = 0 print('\\nStart Walk Forward\\n') # the forward algorithm extension for t in range(1, T): for s in range(nStates): delta[s, t] = np.max(delta[:, t-1] * a[:, s]) * b[s, obs[t]] phi[s, t] = np.argmax(delta[:, t-1] * a[:, s]) print('s={s} and t={t}: phi[{s}, {t}] = {phi}'.format(s=s, t=t, phi=phi[s, t])) # find optimal path print('-'*50) print('Start Backtrace\\n') path[T-1] = np.argmax(delta[:, T-1]) for t in range(T-2, -1, -1): path[t] = phi[path[t+1], [t+1]] print('path[{}] = {}'.format(t, path[t])) return path, delta, phi# Run the algopath, delta, phi = viterbi(pi, a, b, obs)state_map = {0:'Snow', 1:'Rain', 2:'Sunshine'}state_path = [state_map[v] for v in path]pd.DataFrame().assign(Observation=obs_seq).assign(Best_Path=state_path) 3. Forward-backward algorithm - Learning solverThe standard algorithm for HMM training is the forward-backward, or Baum-Welch Welch algorithm, a special case of the Expectation-Maximization (EM) algorithm. The algorithm will let us train both the transition probabilities and the emission probabilities of the HMM. EM is an iterative algorithm, computing an initial estimate for the probabilities, then using those estimates to computing a better estimate, and so on, iteratively improving the probabilities that it learns. For a real HMM, we only get observations, and cannot compute which observation is from which state directly from an observation sequence since we don't know which path of states was taken through the machine for a given input. What's more, we don't even know when is a hidden state present. The Baum-Welch algorithm solves this by iteratively estimating the number of times a state occurs. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities to derive better and better probabilities. And we're going to do this by computing the forward probability for an observation and then dividing that probability mass among all the different paths that contributed to this forward probability. First, we define backward probability , the probability of seeing the observations from time to the end, given that we are in state at time (and given the automaton ): We can actually think of it as a reverse of the forwarding probability , and the computation is just the \"reverse\" of that for , so now the computation of : First initialize . Recrusively apply . Compute Now, we start the actual work. To estimate the , we may adopt a simple maximum likelihood estimation How do we compute the expected number of transitions from state to state ? Here's the intuition. Assume we had some estimate of the probability that a given transition was taken at a particular point in time in the observation sequence. If we knew this probability for each particular time , we could sum over all times to estimate the total count for the transition . We can then compute probability of being in state at time and state at time , given the observation sequence and of course the model via bayes' rule: since we can easily compute we get Similarly, by considering expected number of times in state and observing symbol . Define the probability of being in state at time as , compute We are ready to compute . For the numerator, we sum for all time steps in which the observation is the symbol that we are interested in. For the denominator, we sum over all time steps . The result is the percentage of the times that we were in state and saw symbol (the notation means “sum over all for which the observation at time was Using (3), (4), we can apply EM algorithm easily, as demonstrated below: Initialize iterate until Convergence: E-step: $$\\xi_t(i, j) = \\frac{\\alpha_t(i) \\widehat{P}_{ij} \\widehat{P^H}j\\left(o{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\; \\forall t, i \\; \\text{and} j $$ M-step: This concludes the blog, thank you!","link":"/post/hidden-markov-models/"},{"title":"Hyperparameter Tuning","text":"OverviewHyperparameter tuning is a large field of study, just like any subjects under the topic of machine learning. In fact, I really need to thank this topic for bringing me into the field of Bayesian Optimization and Bandit, as well as the future sequential decision-making models I researched on. In this blog, I'll present some classical and popular methods for hyperparameter tuning. Before that, let us make crystal clear what is hyperparameter and why is it so important. Hyperparameter vs parameter Parameters: The variables inside the model that are gradually updated and estimated via data training. For example, the coefficients in a regression model, or the weights of a deep neural network. hyperparameters: The input variables to a model that stay the same during the training. They are often used in the algorithm to actually help estimate the model parameters. For example, the error rate specified in a statistical learning model like Support Vector Machine or the learning rate in deep neural networks. From the description above, we see that we should choose our hyperparameters wisely so that the parameters which helps our model stand out from the rest models using the same algorithm. Unfortunately, these inputs to the models are often in a continuous space, and we will rarely be able to explore every possible value. That's why numerous tuning methods were devised to help us obtain good hyperparameters that improve our models' performances. Automated hyperparameter tuningAlthough the job can be via manual selection. This is a very tedious process. Instead, many algorithms surfaced to help us overcome this difficulty. 1. Random SearchIn each iteration, we choose from the input space (for hyperparameters) random combination of hyperparameters to run the model. After our budget for iterations runs out, we then compare the performance of model with each combination and select the best one. 2. Grid SearchAs compared to random search, we choose from the input space a set of hyperparameters combinations evenly (sometimes with additional greedy exploration). We then choose from the observed models the best one. This avoid unintential negligence of certain regions in the input space. Unfortunately, the two methods above require the “boundedness” assumption for the input domain. The following methods allow for a general open set for the input spaces. 3. Bayesian OptimizationIn general, a sequential design strategy for global extrema computation of black-box functions that does not assume any functional form. It can be applied in hyperparameter optimization as well. Strategy Description: Prior: a function that is applied on existing observations. The prior captures the bellief about the behaviour of the function. Posterior: the distribution function over the objective function after the priors are evaluated on the observations. Acquisition function: the functions to determine the next query point based on the optimization objective Methods to define the prior/posterior distributions: Gaussian Process Finding the values of p(y|x) where y is the function to be minimized (e.g., validation loss) and x is the value of hyperparameter More expensive Usually executed with few observations Assume Gaussian distribution initially Generate new points (expected value and variance) with in the support Tree of Parzen Estimator Less expensive construct 2 distributions for 'high' and 'low' points, and then finds the location tht maximizes the expected improvement of 'low' points models P(x|y) and P(y) The drawback is the lack of interaction between hyperparameters. Acquisition Functions: Mainly trade-off exploitation and exploration so as to minimize the number of function queries Exploitation means sampling where the surrogate model predicts a high objective Exploration means sampling at locations where the prediction uncertainty is high. Several types of functions: Probability of improvement Expected improvement Upper Confidience Bound Thompson Sampling Entropy Search Methods Optimization of functions: Mainly through discretization using Newton's Method such as lbfgs In general, BO is widely applied in hyperparameter optimization, and is often ideal when the function evaluation is very costly, as its convergence rate is often much better as compared to other methods. 4. HyperbandHyperband is a variation of random search, but with the decision-making models from bandit algorithms to help find the best time allocation for each of the configurations. The method is theoretically sound, and has great variants ASHA (Asynchronous Hyperband) and BOHB (Bayesian Optimization with Hyperband) This also aroused my interests in bandit problems. You may read the research paper here. Genetic Algorithm1. Definition A search heuristic that is inspired by Charles Darwin’s theory of natural evolution. This algorithm reflects the process of natural selection where the fittest individuals are selected for reproduction in order to produce offspring of the next generation. 5 phases: Genetic Algorithm Procedure Initial population stochastic process: the individuals' genes are usually initialized at random. Fitness function Determines the ability (Fitness score) of an individual to compete with other individuals) Selection Error: A significant selection error means low fitness. Those individuals with greater fitness have a higher probability of being selected for recombination. Rank-based fitness assignment Most used method for fitness assignment Formula: Here is a constant called selective pressure, and its value is fixed between 1 and 2. Higher selective pressure values make the fittest individuals have more probability of recombination The is the rank of individual Selection Selection operator selects the individuals according to their fitness level The number of selected individuals is 2, being the population size. Roulette wheel Most used selection method A stochastic sampling with replacement The roulette is turned, and the individuals are selected at random. The corresponding individual is selected for recombination Crossover Every time, picks two individuals at random and combines their features to get four offsprings for the new population until the new population has the same size as the old one. The crossover operator recombines the selected individuals to generate a new population (dropping the selected individuals, the output population size remains constant.) illustration: Crossover Illustration Mutation The crossover operator can generate offsprings that are very similar to the parents. This might cause a new generation with low diversity. The mutation operator solves this problem by changing the value of some features in the offsprings at random. To decide if a feature is mutated, we generate a random number between 0 and 1. If this number is lower than a value called the mutation rate, that variable is flipped. The mutation rate is usually chosen to be 1/m, where m is the number of features. With that value, we mutate one feature of each individual (statistically). 2. Pros &amp; ConsPros Genetic algorithms can manage data sets with many features. They don't need specific knowledge about the problem under study. These algorithms can be easily parallelized in computer clusters. Cons Genetic Algorithms might be costly in computational terms since the evaluation of each individual requires the training of a model. These algorithms can take a long time to converge since they have a stochastic nature. 3. Application If the space to be searched is not so well understood and relatively unstructured (e.g. non-convex, undifferentiable, etc), and if an effective GA representation of that space can be developed, GA is good for usage They’re best for problems where there is a clear way to evaluate fitness. If the base algorithm's computation is expensive, it is not advisable to use this method It is rather rare to use, if you want, checkout this notebook using deap as the package for GA Some tools to use1. Scikit learn Random Search Grid Search 2. HyperOpt Random Search Tree of Parzen Estimators (TPE) Adaptive TPE 3. Optuna Most BO algorithms contained Pruning feature which automatically stops the unpromising trails in the early stages of training 4. Ray Tune ASHA, BOHB Distributed asynchronous automatically Very Scalable Supports Tensorboard and MLflow. Supports a variety of frameworks such sklearn, xgboost, Tensorflow, pytorch, etc. ConclusionFor engineers, it is really matter of choices based on the nature of your code/project. However, for researchers, what optimization strategy you choose could directly affect the theoretical performance of the algorithm. Hence it is worth reading more into the topic of Bayesian Optimization and Sequential decision-making problems. I will also update my posts on BO/Bandit later.","link":"/post/hyperparam-tuning/"},{"title":"manual","text":"","link":"/post/manual/"},{"title":"Neural Network Applied: Optimizer Selection","text":"BackgroundAs one starts to use Neural Networks in their data models, he will inevitably encounter code of form like this: One might be quickly puzzled by the 3 terms optimizer, adam and sparse_categorical_crossentropy here. The first 2 are part of this blog's focus, which is about the optimization strategy applied in a Neural Network execution, and the sparse_categorical_crossentropy is a loss function used to help with the optimization. To understand the relevance of optimizer, one must first understand how an NN is trained. During the training of an NN, the weights of each neuron keeps getting updated so that the loss can be minimized. However, randomly updating the weights is not really feasible as there are hundreds of thousands of weights. Hence our smart scientists came up with a backward propagation (BP) algorithm for updating the weights. One may learn more about BP here. Behind BP we now require the optimizer to facilitate the updating of weights in each iteration. Right below we discuss a few most commonly used optimizers: Gradient DescentGradient Descent is the most basic optimization strategy which is based on the first order derivative of a loss function. The first order derivative serves as a guide on the direction to modify the weight so as to minimize the loss function. We've discussed its variant in details in an earlier post. To refresh our memory and make this blog more coherent, let's quickly recap here. Analytic form: Characteristics of Gradient Descent include: It's used heavily in linear regression and classification algorithms. Easy computation and implementatoin (Pros) May trap at local minima (Cons) Weights are changed only after calculating gradient on the whole dataset. So, if the dataset is too large then the convergence may take very long time (Cons) Requires large memory to calculate gradient on the whole dataset (Cons) Stochastic Gradient DescentGradient Descent has the problem of calculate gradient on the whole dataset in each itearation for weight update. Here Stochastic Gradient Descent aims to resolve this issue by processing data in random batches. As the model parameters are frequently updated parameters have high variance and fluctuations in loss functions at different intensities. Analytic form: Characteristics of SGD include: The learning rate needs to be updated in each iteartion to aviod over-fitting Faster convergence rate and less memory used (Pros) High variance in model parameters. (Cons) May continue to run even when global minima is achieved. (Cons) To reduce the variance we further have the mini-batch Gradient Descent which divides the data into mutiple batches and updates the model parameters after every batch (vs 1 data entry per update in SGD). In general, Gradient Descent method has the challenge of Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge. Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate. May get trapped at local minima. MomentumMomentum was invented for reducing high variance in SGD and softens the convergence. It takes advantage of information from previous directions via a formula Analytic form: Characteristics of Momentum include: The momentum term is usually set to 0.9 or a similar value. Faster Convergence and smaller variance (pros) Less Oscilliation &amp; more smooth shifting of direction (pros) AdagradOften, the learning rate of the optimizer is a constant. However, one may expect the optimizer to explore faster at the start and slower at the end to quickly converge to an optimum. Hence the learning rate may subject to change as iteration goes. Adagrad aims to achieve such effect. If we use low learning rates for parameters associated with most frequently occurring features, and high learning rates for parameters associated with infrequent features. We can get a good model. Analytic form:where and Here is a smoothing term that avoids division by zero (usually on the order of 1e−8) Characteristics of Adagrad include: Learning rate changes for each training parameter. Don't need to manually tune the learning rate. (pros) Able to train and performs well on sparse data. (pros) Computationally expensive as a need to calculate the second order derivative. (cons) Learning rate is monotone decreasing as iteration increases. (cons) AdaDeltaIt is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size . In this optimizer, exponentially moving average is used rather than the sum of all the gradients. By the idea above, we reducing the window size of from to : . Analytic form: Characteristics of AdaDelta include: Learning rate does not decay necessarily (pros) More computationally expensive as expectation is involved (cons) RMSPropThe RMSProp algorithm full form is called Root Mean Square Prop, which is an adaptive learning rate optimization algorithm proposed by Geoff Hinton. RMSProp is another strategy that tries to resolve Adagrad's radically diminishing learning rates problem by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient. While Adagrad accumulates all previous gradient squares, RMSprop just calculates the corresponding average value, so it can eliminate the problem of quickly dropping of learning rate of the Adagrad. By the idea above, we replace the with an expectation formula: Analytic form: Conclusion for the dynamic learning rate optimizer: Good for sparse data Be careful of the diminishing speed of learning rate More expensive computationally in general AdamAdam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don't want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaGrad, Adam also keeps an exponentially decaying average of past gradients M(t). In summary, Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. Note that although the name momentum looks fancy, the terms we need to consider are just first and second order momentum, which are essentially mean and variance of the gradients. Afterwards, we can consider 2 terms and as follows Hence the formula is as follows: These 2 terms are used to approximate the first and second moments, that is: Although we have above, theorem suggests that we can use the observed and to approximate and directly. After bias correction, we derive the terms Here and have really good default values of 0.9 and 0.999 respectively. Finally the update formula is just Code implementationTo Be Updated","link":"/post/nn-optimizers/"},{"title":"Recommender Systems: I. Content-Based Filtering And Collaborative Filtering","text":"OverviewThe rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user. They are used to predict the rating or preference that a user would give to an item. Almost every major tech company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. Moreover, companies like Netflix and Spotify depend highly on the effectiveness of their recommendation engines for their business and success. Traditional recommender system modelsThere are basically three types of traditional recommender systems, let's use the example of movie recommendation (e.g. Netflix): Demographic Filtering: They offer generalized recommendations to every user, based on movie popularity and/or genre. The System recommends the same movies to users with similar demographic features. Since each user is different , this approach is considered to be too simple. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience. Content Based Filtering: They suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it. Collaborative Filtering: This system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like its content-based counterparts. In later blogs, we will talk about more recent models for recommender systems, including factorization machines and deep learning based models. Content Based Filtering1. Definition Use additional information about users and/or items. Example: User features: age, the sex, the job or any other personal information Exmaple: Item features: the category, the main actors, the duration or other characteristics for the movies. Main idea: given the set of features (both User and Item), apply a method to identify the model that explain the observed user-item interactions. Content Flow Little concern about \"Cold Start\": new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities One key tool used: Term Frequency-Inverse Document Frequency (TF-IDF): TF: the frequency of a word in a document IDF: the inverse of the document frequency among the whole corpus of documents. log: log function is taken to dampen the effect of high frequency word (0 vs 100 0 vs 2 (log100)) Note that normalization is needed before we apply TF-IDF because the initial feature map are all 1's and 0's, but the log function will remove all these differentiation. In the end the TF score will just be 1/0 2. Limitation They are not good at capturing inter-dependence or complex behaviours. For example: A user may prefer gaming + tv the most while a pure tv is not really his favourate. 3. Code Sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as np # linear algebraimport pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.pairwise import linear_kernelbooks = pd.read_csv('goodread/books.csv', encoding = \"ISO-8859-1\")ratings = pd.read_csv('goodread/ratings.csv', encoding = \"ISO-8859-1\")book_tags = pd.read_csv('goodread/book_tags.csv', encoding = \"ISO-8859-1\")tags = pd.read_csv('goodread/tags.csv')tags_join_DF = pd.merge(book_tags, tags, left_on='tag_id', right_on='tag_id', how='inner')to_read = pd.read_csv('goodread/to_read.csv')tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix = tf.fit_transform(books['authors'])cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)titles = books['title']indices = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of book authorsdef authors_recommendations(title): idx = indices[title] sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]books_with_tags = pd.merge(books, tags_join_DF, left_on='book_id', right_on='goodreads_book_id', how='inner')tf1 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix1 = tf1.fit_transform(books_with_tags['tag_name'].head(10000))cosine_sim1 = linear_kernel(tfidf_matrix1, tfidf_matrix1)# Build a 1-dimensional array with book titlestitles1 = books['title']indices1 = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of books tagsdef tags_recommendations(title): idx = indices1[title] sim_scores = list(enumerate(cosine_sim1[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]temp_df = books_with_tags.groupby('book_id')['tag_name'].apply(' '.join).reset_index()books = pd.merge(books, temp_df, left_on='book_id', right_on='book_id', how='inner')books['corpus'] = (pd.Series(books[['authors', 'tag_name']] .fillna('') .values.tolist() ).str.join(' '))tf_corpus = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')tfidf_matrix_corpus = tf_corpus.fit_transform(books['corpus'])cosine_sim_corpus = linear_kernel(tfidf_matrix_corpus, tfidf_matrix_corpus)# Build a 1-dimensional array with book titlestitles = books['title']indices = pd.Series(books.index, index=books['title'])# Function that get book recommendations based on the cosine similarity score of books tagsdef corpus_recommendations(title): idx = indices1[title] sim_scores = list(enumerate(cosine_sim_corpus[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:21] book_indices = [i[0] for i in sim_scores] return titles.iloc[book_indices]corpus_recommendations(\"The Hobbit\")corpus_recommendations(\"Twilight (Twilight, #1)\") Collaborative Filtering1. Definition Based solely on the past interactions recorded between users and items in order to produce new recommendations Main idea: Past user-item interactions are sufficient to detect similar users and/or similar items and make predictions based on these estimated proximities. Every user and item is described by a feature vector or embedding. It creates embedding for both users and items on its own. It embeds both users and items in the same embedding space. 2 Major Types: Memory Based Users and items are represented directly by their past interactions (large sparce vector) Recommendations are done following nearest neighbour information No latent model is assumed Theoretically a low bias but a high variance. Usualy recommend those items with high rating for a user : : the rating given to by user : users similar to / items similar to : respective ratings : similarity score for -th item/user similar to / (deduced using the similarity metircs shown below) Similarity Metrics: Cosine Similarity Dot Product Euclidean distance Pearson Similarity: Limitations: Don't scale easily KNN algorithm has a complexity of O(ndk) Users may easily fall into a \"information confinement area\" which only give too precise/general information Overcome Limitation: Use Approximate nearest neighbour (ANN) or take advantage of sparse matrix 2 Types: User-User: Identify users with the most similar \"interactions profile\" (nearest neighbours) in order to suggest items that are the most popular among these neighbours (and that are \"new\" to our user). We consider that two users are similar if they have interacted with a lot of common items in the same way (similar rating, similar time hovering…). Prevents overfitting As, in general, every user have only interacted with a few items, it makes the method pretty sensitive to any recorded interactions (high variance) Only based on interactions recorded for users similar to our user of interest, we obtain more personalized results (low bias) Item-Item: Find items similar to the ones the user already \"positively\" interacted with Two items are considered to be similar if most of the users that have interacted with both of them did it in a similar way. A lot of users have interacted with an item, the neighbourhood search is far less sensitive to single interactions (lower variance) Interactions coming from every kind of users are then considered in the recommendation, making the method less personalised (more biased) VS User-User: Less personalized, but more robust Model Based New reprensentations of users and items are build based on a model (small dense vectors) The model \"derives\" the relevant features of the user-item interactions Recommendations are done following the model information May contain interpretability issue Theoretically a higher bias but a lower variance 3 Types: Clustering Simple KNN/ANN will do on these metrices Matrix Factorization Main assumption: There exists a very low dimensional latent space of features in which we can represent both users and items and such that the interaction between a user and an item can be obtained by computing the dot product of corresponding dense vectors in that space. Generate the factor matrices as feature matrices for users and items. Idea: : Interaction matrix of ratings, usually sparse : User matrix : Item matrix : the dimension of the latent space Advanced Factorization methods: SVD: Not so well due to the sparsity of matrix : is item matrix; is user matrix WMF (Weighted Matrix Factorization) Weight applied to rated/non-rated entries Similar to NMF but also consider non-rated ones by associating a weight to each entry NMF: Uses only the observed or rated one Performs well with sparse matrices where indicates the -th item rated by -th user Minimizing the objective function Most common: Weighted Alternating Least Squares Formula: Regularized minimization of “rating reconstruction error” Optimization process via Gradient Descent (Reduce runtime by batch running) Instead of solving for and together, we alternate between the above two equations. Fixing and solving for Fixing and solving for This algorithm gives us an approximated result (two equations are not convex at the same time can't reach a global minimum local minimum close to the global minimum) For a fixed set of users and items, new interactions recorded over time bring new information and make the system more and more effective. Solution to \"Cold Start\" problem: Heuristics to generate embeddings for fresh items Recommending random items to new users/recommend new item to random users Recommending popular items to new usres/recommend new items to most active users Recomeending a set of various items to new users or a new item to a set of various users Use a non collaborative method for early life of the user/item Projection in WALS (given current optimal and ) 2. Pros &amp; ConsPros Require no information about users or items (more versatile) Cons Cold Start problem: Impossible to recommend anything to new users or to recommend a new item to any users Many users or items have too few interactions to be efficiently handled. 3. Comparison with Content Based Method Content based methods suffer far less from the cold start problem than collaborative approaches CB is much more constrained (because representation of users and/or items are given) CB tends to have the highest bias but the lowest variance 4. Code samples123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Import librariesimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlineimport warningswarnings.filterwarnings('ignore')import osfrom textwrap import wrap# Read the input training datainput_data_file_movie = \"movie.csv\"input_data_file_rating = \"rating.csv\"movie_data_all = pd.read_csv(input_data_file_movie)rating_data_all = pd.read_csv(input_data_file_rating)# Keep only required columnsmovie_data_all = movie_data_all.drop(['genres'], axis=1)rating_data_all = rating_data_all.drop(['timestamp'], axis=1)# Pick all ratings#num_ratings = 2000000rating_data = rating_data_all.iloc[:, :]movie_rating_merged_data = movie_data.merge(rating_data, on='movieId', how='inner')movie_rating_merged_pivot = pd.pivot_table(movie_rating_merged_data, index=['title'], columns=['userId'], values=['rating'], dropna=False, fill_value=0 )# Create a matrix R, such that, R(i,j) = 1 iff User j has selected a rating for Movie i. R(i,j) = 0 otherwise.R = np.ones(Y.shape)no_rating_idx = np.where(Y == 0.0)# Assign n_m (number of movies), n_u (number of users) and n_f (number of features)n_u = Y.shape[1]n_m = Y.shape[0]n_f = 2 # Because we want to cluster movies into 2 genres# Setting random seed to reproduce results laternp.random.seed(7)Initial_X = np.random.rand(n_m, n_f)Initial_Theta = np.random.rand(n_u, n_f)# Cost Functiondef collabFilterCostFunction(X, Theta, Y, R, reg_lambda): cost = 0 error = (np.dot(X, Theta.T) - Y) * R error_sq = np.power(error, 2) cost = np.sum(np.sum(error_sq)) / 2 cost = cost + ((reg_lambda/2) * ( np.sum(np.sum((np.power(X, 2)))) + np.sum(np.sum((np.power(Theta, 2)))))) return cost# Gradient Descentdef collabFilterGradientDescent(X, Theta, Y, R, alpha, reg_lambda, num_iters): cost_history = np.zeros([num_iters, 1]) for i in range(num_iters): error = (np.dot(X, Theta.T) - Y) * R X_grad = np.dot(error, Theta) + reg_lambda * X Theta_grad = np.dot(error.T, X) + reg_lambda * Theta X = X - alpha * X_grad Theta = Theta - alpha * Theta_grad cost_history[i] = collabFilterCostFunction(X, Theta, Y, R, reg_lambda) return X, Theta, cost_history# Tune hyperparametersalpha = 0.0001num_iters = 100000reg_lambda = 1# Perform gradient descent to find optimal parametersX, Theta = Initial_X, Initial_ThetaX, Theta, cost_history = collabFilterGradientDescent(X, Theta, Y, R, alpha, reg_lambda, num_iters)cost = collabFilterCostFunction(X, Theta, Y, R, reg_lambda)print(\"Final cost =\", cost)user_idx = np.random.randint(n_u)pred_rating = []print(\"Original rating of an user:\\n\", Y.iloc[:,user_idx].sort_values(ascending=False))predicted_ratings = np.dot(X, Theta.T)predicted_ratings = sorted(zip(predicted_ratings[:,user_idx], Y.index), reverse=True)print(\"\\nPredicted rating of the same user:\")_ = [print(rating, movie) for rating, movie in predicted_ratings]","link":"/post/recommender-1/"},{"title":"Recommender Systems: II. Factorization Machine","text":"Factorization Machine1. Definition In essence, a generalized matrix factorization method Field: A type/column in the original dataset Feature: A value in the Field (Nike is a feature, Brand is a field) Movitation: Traditional regression methods cannot handle sparse matrix very well (too much waste in computation time on null values) FM solves the problem of considering pairwise feature interactions (linear time complexity). It allows us to train, based on reliable information (latent features) from every pairwise combination of features in the model. Main logic: Instead of using field as column, each feature has a column. So the columns are basically one-hot-encoding for each value in the field and the row is user id each row covers all the information a user has log-loss function to minimize: w_i: feature parameter vector (to be optimized) x_i: feature vector (column, given) v_i: latent vector of predefined low dimension k (to be optimized) The idea here is that except for individual feature, it consider the combination of 2 features (hence a degree = 2) as a factor Extension: Field-aware FM For each feature, the parameter vector is no longer unique A feature may interact with other features with different fields. Hence we differentiate the parameter vector for a feature based on the field of its interacting feature E.g: Gaming is an activity, Make is a gender; The parameter vector for Male may also be a if it is interacting with a brand like Nike Important note on numerical features Numerical features either need to be discretized (transformed to categorical features by breaking the entire range of a particular numerical feature into smaller ranges and label encoding each range separately). Another possibility is to add a dummy field which is the same as feature value will be numeric feature for that particular row (For example a feature with value 45.3 can be transformed to 1:1:45.3). However, the dummy fields may not be informative because they are merely duplicates of features. 2. Code Sample Note that the code below will faill because we haven’t installed the xlearn package (too tedious) Refer to the code to get an inspiration Only apply the code if you have the need to use FM or FFM in your model Note that usually DL method works better for the FM-integrated recommender 123456789101112131415import pandas as pdimport xlearn as xltrain = pd.read_csv('loan prediction/train_u6lujuX_CVtuZ9i.csv')import warningswarnings.filterwarnings('ignore')cols = ['Education','ApplicantIncome','Loan_Status','Credit_History']train_sub = train[cols]train_sub['Credit_History'].fillna(0, inplace = True)dict_ls = {'Y':1, 'N':0}train_sub['Loan_Status'].replace(dict_ls, inplace = True)## train-test splitfrom sklearn.model_selection import train_test_splitX_train, X_test = train_test_split(train_sub, test_size = 0.3, random_state = 5) Next, we need to convert the dataset to libffm format which is necessary for xLearn to fit the model. Following function does the job of converting dataset in standard dataframe format to libffm format. df = Dataframe to be converted to ffm format Type = 'Train' / 'Test'/ 'Valid' Numerics = list of all numeric fields Categories = list of all categorical fields Features = list of all features except the Label and Id 12345678910111213141516171819202122232425262728293031323334353637383940414243def convert_to_ffm(df,type,numerics,categories,features): currentcode = len(numerics) catdict = {} catcodes = {} # Flagging categorical and numerical fields for x in numerics: catdict[x] = 0 for x in categories: catdict[x] = 1 nrows = df.shape[0] ncolumns = len(features) with open(str(type) + \"_ffm.txt\", \"w\") as text_file: # Looping over rows to convert each row to libffm format for n, r in enumerate(range(nrows)): datastring = \"\" datarow = df.iloc[r].to_dict() datastring += str(int(datarow['Loan_Status'])) # Set Target Variable here # For numerical fields, we are creating a dummy field here for i, x in enumerate(catdict.keys()): if(catdict[x]==0): datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x]) else: # For a new field appearing in a training example if(x not in catcodes): catcodes[x] = {} currentcode +=1 catcodes[x][datarow[x]] = currentcode #encoding the feature # For already encoded fields elif(datarow[x] not in catcodes[x]): currentcode +=1 catcodes[x][datarow[x]] = currentcode #encoding the feature code = catcodes[x][datarow[x]] datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\" datastring += '\\n' text_file.write(datastring) the xLearn library can handle csv as well as libsvm format for implementation of FMs while we necessarily need to convert it to libffm format for using FFM. Once we have the dataset in libffm format, we could train the model using the xLearn library. xLearn can automatically performs early stopping using the validation/test logloss and we can also declare another metric and monitor on the validation set for each iteration of the stochastic gradient descent. 1234567891011121314151617181920212223ffm_model = xl.create_ffm()ffm_model.setTrain(\"train_ffm.txt\")param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'}# Start to train# The trained model will be stored in model.outffm_model.fit(param, './model.out')# The library also allows us to use cross-validation using the cv() function:ffm_model.cv(param)# Prediction taskffm_model.setTest(\"test_ffm.txt\") # Test dataffm_model.setSigmoid() # Convert output to 0-1# Start to predict# The output result will be stored in output.txtffm_model.predict(\"./model.out\", \"./output.txt\")","link":"/post/recommender-2/"},{"title":"Recommender Systems: III. Deep-learning Methods","text":"A brief introThere are a wide variety of DL tools used for recommendation systems, we will outline a few below. We cite various information from the paper Deep Learning based Recommender System: A Survey and New Perspectives. You may find more details from that paper. Multilayer Perceptron (MLP) is a feed-forward neural network with multiple (one or more) hidden layers between the input layer and output layer. Here, the perceptron can employ arbitrary activation function and does not necessarily represent strictly binary classier. MLPs can be intrepreted as stacked layers of nonlinear transformations, learning hierarchical feature representations. MLPs are also known to be universal approximators. Autoencoder (AE) is an unsupervised model aempting to reconstruct its input data in the output layer. In general, the bottleneck layer (the middle-most layer) is used as a salient feature representation of the input data. ere are many variants of autoencoders such as denoising autoencoder, marginalized denoisingautoencoder, sparse autoencoder, contractive autoencoder and variational autoencoder (VAE). Convolutional Neural Network (CNN) is a special kind of feedforward neural network with convolution layers and pooling operations. It can capture the global and local features and significantly enhancing the eciency and accuracy. It performs well in processing data with grid-like topology. Recurrent Neural Network (RNN) is suitable for modelling sequential data. Unlike feedforward neural network, there are loops and memories in RNN to remember former computations. Variants such as Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) network are oen deployed in practice to overcome the vanishing gradient problem. Restricted Boltzmann Machine (RBM) is a two layer neural network consisting of a visible layer and a hidden layer. It can be easily stacked to a deep net. Restricted here means that there are no intra-layer communications in visible layer or hidden layer. Neural Autoregressive Distribution Estimation (NADE) is an unsupervised neural network built atop autoregressive model and feedforward neural networks. It is a tractable and efficient estimator for modelling data distribution and densities. Adversarial Networks (AN) is a generative neural network which consists of a discriminator and a generator. The two neural networks are trained simultaneously by competing with each other in a minimax game framework. Attentional Models (AM) are dierentiable neural architectures that operate based on soft content addressing over an input sequence (or image). Attention mechanism is typically ubiquitous and was incepted in Computer Vision and Natural Language Processing domains. However, it has also been an emerging trend in deep recommender system research. Deep Reinforcement Learning (DRL) . Reinforcement learning operates on a trial-and-error paradigm. The whole framework mainly consists of the following components: agents, environments, states, actions and rewards. The combination between deep neural networks and reinforcement learning formulate DRL which have achieved human-level performance across multiple domains such as games and selfdriving cars. Deep neural networks enable the agent to get knowledge from raw data and derive efficient representations without handcrafted features and domain heuristics. Pros and consPros Nonlinear Transformation: Contrary to linear models, deep neural networks is capable of modelling the non-linearity in data with nonlinear activations such as relu, sigmoid, tanh, etc. This property makes it possible to capture the complex and intricate user item interaction patterns. The linear assumption, acting as the basis of many traditional recommenders, is oversimplified and will greatly limit their modelling expressiveness. It is well-established that neural networks are able to approximate any continuous function with an arbitrary precision by varying the activation choices and combinations. Representation Learning: Deep neural networks is efficacious in learning the underlying explanatory factors and useful representations from input data. In general, a large amount of descriptive information about items and users is available in real-world applications. Making use of this information provides a way to advance our understanding of items and users, thus, resulting in a better recommender. As such, it is a natural choice to apply deep neural networks to representation learning in recommendation models. The advantages of using deep neural networks to assist representation learning are in two-folds: it reduces the efforts in hand-craft feature design. Feature engineering is a labor intensive work, deep neural networks enable automatically feature learning from raw data in unsupervised or supervised approach; it enables recommendation models to include heterogeneous content information such as text, images, audio and even video. Deep learning networks have made breakthroughs in multimedia data processing and shown potentials in representations learning from various sources. Sequence Modelling: Deep neural networks have shown promising results on a number of sequential modelling tasks such as machine translation, natural language understanding, speech recognition, chatbots, and many others. RNN and CNN play critical roles in these tasks. RNN achives this with internal memory states while CNN achieves this with filters sliding along with time. Both of them are widely applicable flexible in mining sequential structure in data. Modelling sequential signals is an important topic for mining the temporal dynamics of user behaviour and item evolution. For example, next-item/basket prediction and session based recommendation are typical applications. As such, deep neural networks become a perfect fit for this sequential pattern mining task Flexibility： Deep learning techniques possess high flexibility, especially with the advent of many popular deep learning frameworks. Cons Interpretability: Despite its success, deep learning is well-known to behave as black boxes, and providing explainable predictions seem to be a really challenging task. A common argument against deep neural networks is that the hidden weights and activations are generally non-interpretable, limiting explainability. However, this concern has generally been eased with the advent of neural attention models and have paved the world for deep neural models that enjoy improved interpretability. While interpreting individual neurons still pose a challenge for neural models (not only in recommender systems), present state-of-the-art models are already capable of some extent of interpretability, enablingexplainable recommendation. We discuss this issue in more detail in the open issues section. Data Requirement: A second possible limitation is that deep learning is known to be data-hungry, in the sense that it requires sufficient data in order to fully support its rich parameterization. However, as compared with other domains (such as language or vision) in which labeled data is scarce, it is relatively easy to garner a significant amount of data within the context of recommender systems research. Million/billion scale datasets are commonplace not only in industry but also released as academic datasets. Extensive Hyperparameter Tuning: A third well-established argument against deep learning is the need for extensive hyperparameter tuning. However, we note that hyperparameter tuning is not an exclusive problem of deep learning but machine learning in general (e.g., regularization factors and learning rate similarly have to be tuned for traditional matrix factorization etc) Granted, deep learning may introduce additional hyperparameters in some cases.","link":"/post/recommender-3/"},{"title":"A Regex Tutorial","text":"IntroductionRecently I've been working on software developement projects and learning some NLP algorithms. Then regex came to my attention as a powerful string processing tool. It is so useful that I have to utilize the techniques almost everyday in my learning and work. Nonetheless, I think I might have a chance of leaving it to rot after some time. To maintain good memory of the syntax, I decided to create this blog, to both teach my future self and all of you interested readers. Without further a do, let's begin. Regex Full name: regular expression Create/search for a specific pattern in a string Very useful for text editing/file searching/phrase grouping/etc terminalogies raw string: a raw string in python is just string prefixed with 'r', it tells python not to handle back slashes in any special way MetaCharacters: symbols in the search pattern to create variated pattern Useful website: [1] Regex101 In the following, I'll outline all keywords used in the expression 1. Special sequences with backslash \\d: digit(0-9) \\D: Not a Digit(0-9) \\w: Word character (a-z, A-Z, 0-9 _) \\W: Not a word character \\s: Whitespace(space, tab, blank line) \\b: Word boundary \\B: Not a word boundary \\A: Start of string, alternative representation: ^ \\Z: End of string, alternative representation: $ \\g&lt;id&gt;: Matches a previously defined group 2. MetaCharacters: \\: Escape special characters .: Matches any character $: Matches end of string ^: Matches start of string []: Matches characters in brackets [^ ]: Matches characters not in brackets |: Either or (): Group 3. Quantifiers: *: 0 or more +: 1 or more ?: 0 or 1 {m}: Exactly m times {n,}: Min n times {m,n}: From m to n times, as many as possible {m,n}?: From m to n times, as few as possible Demonstrate the idea of raw string12345print('\\tTab')print(r'\\tTab')# this gives the following output&gt;&gt;&gt; Tab&gt;&gt;&gt; \\tTab Let's try out some regex usage by initializing the following strings 12345678910111213141516171819202122232425262728293031323334353637383940414243import retext_to_search = '''abcdefghijklmnopqurtuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890Ha HaHaMetaCharacters (Need to be escaped):. ^ $ * + ? { } [ ] \\ | ( )coreyms.com321-555-4321123.555.1234123*555*1234800-555-1234900-555-1234somebody123@gamil.comtest@test.comsome@qq.eduMr. SchaferMr SmithMs DavisMrs. RobinsonMr. TFirstLastFirst.LastFirst LastFirst..LastFirst...LastFirst....Last'''urls = '''https://www.google.comhttp://baidu.com'''sentence = 'Start a sentence and then bring it to an end' Next let's apply 2 functions test and test2 that we use to apply regex on text_to_search and sentence respectively: 123456789101112131415def test(s): pattern = re.compile(s) matches = pattern.finditer(text_to_search) for match in matches: print(match) # output: span(x, y) -&gt; the location [x,y] in the string, match -&gt; matched string print(\"\")def test2(s): pattern = re.compile(s) matches = pattern.finditer(sentence) for match in matches: print(match) # output: span(x, y) -&gt; the location [x,y] in the string, match -&gt; matched string print(\"\") Then we can try the following code and play around with it 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# use of \\test(r'ms\\.com')test(r'\\\\')print(\"===========================\")# use of \\dtest(r\"\\d\")print(\"===========================\")# use of \\wtest(r\"\\w\")print(\"===========================\")# use of ^test2(r'^Start')test2(r'Start^a') # no matchtest2(r'^a') # no matchprint(\"===========================\")# use of $test2(r'end$') # no matchtest2(r'$Start') # no matchtest2(r'a$') # no matchprint(\"===========================\")# use word boundary usagetest(r'\\bHa')test(r'\\BHa')test(r'\\bHa\\b')print(\"===========================\")# use of .# warning: . does not takes \\n into accounttest(r'.')test(r'.*')print(\"===========================\")# usage of group ()test(r'\\d\\d\\d.\\d\\d\\d.\\d\\d\\d\\d')test(r'(\\d*).\\d\\d\\d.\\d\\d\\d\\d')print(\"===========================\")# use of |test(r'First( |\\.)Last')print(\"===========================\")# limit the value in pattern []test(r'([6-9]\\d*).(\\d*).(\\d*)')test(r'[a-zA-Z]')test(r'[^a-zA-Z]') # take NOTtest(r'First[ \\.]Last')print(\"===========================\")# limit the size in pattern using {}test(r'(\\d{3}).\\d{3}.\\d{4}')test(r'\\w{4,8}')test(r'\\w{4,8}?')print(\"===========================\")# use of ?test(r'First\\.?Last') # has 1 or 0 .test(r'First\\.?\\sLast')test(r'555-?')print(\"===========================\")# use of +test(r'First\\.+Last')print(\"===========================\")# use of *test(r'First\\.*Last')print(\"===========================\")# Now let's try a combination of the above methodstest(r'\\d{3}[-.]\\d{3}[-.]\\d{4}') SubstitutionTo substitute existing characters in a string, we need to specify a pattern directly, and we then use the .sub method. 123pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)')subbed_urls = pattern.sub(r'\\2\\3',urls)print(subbed_urls) The pattern can also be used to direcly find all the occurances, so that we don't need to define test and test2 by ouselves. 123456789101112131415161718192021222324252627282930313233pattern = re.compile(r'(\\w+)-(\\w+)-(\\w+)')matches1 = pattern.findall(text_to_search)for match in matches1: print(match) pattern2 = re.compile(r'\\d{3}-\\d{3}-\\d{4}')matches2 = pattern2.findall(text_to_search)for match in matches2: print(match)### Check match at the begin of the stringpattern = re.compile(r'Start')matches = pattern.match(sentence)print(matches)pattern = re.compile(r'sentence')matches = pattern.match(sentence)print(matches)### Case insensitive modepattern = re.compile(r'start', re.I) # re.I is a short version of re.IGNORECASEmatches = pattern.match(sentence)print(matches)### Actual functionality demo# Email searchtest(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9_.+-]+\\.[a-zA-Z0-9_.+-]+')test(r'(\\w+)@(\\w+)\\.(\\w+)')pattern = re.compile(r'(\\w+)@(\\w+)\\.(\\w+)')matches = pattern.finditer(text_to_search)for match in matches: print(match.group(1), match.group(2), match.group(3))","link":"/post/regex/"},{"title":"Regression Models: Linear Regression and Regularization","text":"Definition It is used for predicting the continuous dependent variable with the help of independent variables. The goal is to find the best fit line that can accurately predict the output for the continuous dependent variable. The model is usually fit by minimizing the sum of squared errors (OLS (Ordinary Least Square) estimator for regression parameters) Major algorithm is gradient descent: the key is to adjust the learning rate Explanation in layman terms: - provides you with a straight line that lets you infer the dependent variables - estimate the trend of a continuous data by a straight line. using input data to predict the outcome in the best possible way given the past data and its corresponding past outcomes Various RegulationsRegularization is a simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression. Convergence conditions differ note that regularization only apply on variables (hence is not regularized!) L2 norm: Euclidean distance from the origin L1 norm: Manhattan distance from the origin Elastic Net: Mixing L1 and L2 norms Ridge regression: where is cofficient; more widely used as compared to Ridge when number of variables increases Lasso regression: ; better when the data contains suspicious collinear variables Comparison with Logistic Regression Linear Regression: the outcomes are continuous (infinite possible values); error minimization technique is ordinary least square. Logistic Regression: outcomes usually have limited number of possible values; error minimization technique is maximal likelihood. ImplementationsBasic operations using sklearn packages 1234567891011from sklearn.linear_model import LinearRegressionX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])y = np.dot(X, np.array([1, 2])) + 3reg = LinearRegression(normalize=False, fit_intercept = True).fit(X, y)display(reg.score(X, y))display(reg.coef_) # regression coefficientsdisplay(reg.intercept_) # y-intercept / offsetreg.predict(np.array([[3, 5]])) Common Questions Is Linear regression sensitive to outliers? Yes! Is a relationship between residuals and predicted values in the model ideal? No, residuals should be due to randomness, hence no relationship is an ideal property for th model What is the range of learning rate? 0 to 1 Advanced: Analytical solutionsHere let's discuss some more math-intensive stuff. Those who are not interested can ignore this part (though it gives a very important guide on regression models) 1. A detour into Hypothesis representationWe will use to denote the independent variable and to denote dependent variable. A pair of is called training example. The subscripe in the notation is simply index into the training set. We have training example then . The goal of supervised learning is to learn a hypothesis function , for a given training set that can used to estimate based on . So hypothesis fuction represented as where are parameter of hypothesis.This is equation for Simple / Univariate Linear regression. For Multiple Linear regression more than one independent variable exit then we will use to denote indepedent variable and to denote dependent variable. We have independent variable then . The hypothesis function represented as where are parameter of hypothesis, Number of training exaples, Number of independent variable, is training exaple of feature. 2. Matrix FormulationIn general we can write above vector as Now we combine all aviable individual vector into single input matrix of size and denoted it by input matrix, which consist of all training exaples, We represent parameter of function and dependent variable in vactor form as So we represent hypothesis function in vectorize form . 3. Cost functionA cost function measures how much error in the model is in terms of ability to estimate the relationship between and .We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference of observed dependent variable in the given the dataset and those predicted by the hypothesis function. To implement the linear regression, take training example add an extra column that is feature, where . ,where and input matrix will become as Each of the m input samples is similarly a column vector with n+1 rows being 1 for our convenience, that is . Now we rewrite the ordinary least square cost function in matrix form as Let's look at the matrix multiplication concept,the multiplication of two matrix happens only if number of column of firt matrix is equal to number of row of second matrix. Here input matrix of size , parameter of function is of size and dependent variable vector of size . The product of matrix will return a vector of size , then product of will return size of unit vector. 4. Normal EquationThe normal equation is an analytical solution to the linear regression problem with a ordinary least square cost function. To minimize our cost function, take partial derivative of with respect to and equate to . The derivative of function is nothing but if a small change in input what would be the change in output of function. where Now we will apply partial derivative of our cost function, I will throw part away since we are going to compare a derivative to . And solve , Here because of unit vector. Partial derivative , ,, hence this is the normal equation for linear regression. Advanced: Model Evaluation and Model Validation1. Model evaluationWe will predict value for target variable by using our model parameter for test data set. Then compare the predicted value with actual valu in test set. We compute Mean Square Error using formula is statistical measure of how close data are to the fitted regression line. is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it's mean. 100% indicated that model explains all the variablity of the response data around the mean. where = Sum of Square Error, = Sum of Square Total. Here is predicted value and is mean value of .Below is a sample code for evaluation 12345678910111213141516171819202122232425262728# Normal equationy_pred_norm = np.matmul(X_test_0,theta)#Evaluvation: MSEJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]# R_square sse = np.sum((y_pred_norm - y_test)**2)sst = np.sum((y_test - y_test.mean())**2)R_square = 1 - (sse/sst)print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)print('R square obtain for normal equation method is :',R_square)&gt;&gt;&gt; The Mean Square Error(MSE) or J(theta) is: 0.17776161210877062&gt;&gt;&gt; R square obtain for normal equation method is : 0.7886774197617128# sklearn regression moduley_pred_sk = lin_reg.predict(X_test)#Evaluvation: MSEfrom sklearn.metrics import mean_squared_errorJ_mse_sk = mean_squared_error(y_pred_sk, y_test)# R_squareR_square_sk = lin_reg.score(X_test,y_test)print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)print('R square obtain for scikit learn library is :',R_square_sk)&gt;&gt;&gt; The Mean Square Error(MSE) or J(theta) is: 0.17776161210877925&gt;&gt;&gt; R square obtain for scikit learn library is : 0.7886774197617026 The model returns value of 77.95%, so it fit our data test very well, but still we can imporve the the performance of by diffirent technique. Please make a note that we have transformer out variable by applying natural log. When we put model into production antilog is applied to the equation. 2. Model ValidationIn order to validated model we need to check few assumption of linear regression model. The common assumption for Linear Regression model are following Linear Relationship: In linear regression the relationship between the dependent and independent variable to be linear. This can be checked by scatter ploting Actual value Vs Predicted value The residual error plot should be normally distributed. The mean of residual error should be 0 or close to 0 as much as possible The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot. Linear regession assumes that there is little or no *Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor VIF identifies correlation between independent variables and strength of that correlation. , If VIF &gt;1 &amp; VIF &lt;5 moderate correlation, VIF &lt; 5 critical level of multicollinearity. Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern. The model assumption linear regression as follows In our model the actual vs predicted plot is curve so linear assumption fails The residual mean is zero and residual error plot right skewed Q-Q plot shows as value log value greater than 1.5 trends to increase The plot is exhibit heteroscedastic, error will insease after certian point. Variance inflation factor value is less than 5, so no multicollearity. Linearity plot and Residual plot. Q-Q Plot and HomoScedasticity plot","link":"/post/regressions-1/"},{"title":"Regression Models: Logistic Regression","text":"Definition We have a mathematical function which gives a value between and , and to convert it to a value between (0,1), we need a Sigmoid function or a logistic function We can visualize it as a boundary (the decision boundary) to separate 2 categories on a hyperplane, where each dimension is a variable (a certain type of information) The algorithm used is also gradient descent Common Questions What is a logistic function? Answer: . What is the range of values of a logistic function? Answer: The values of a logistic function will range from 0 to 1. The values of Z will vary from to . What are the cost functions of logistic function? Answer: The popular 2 are Cross-entropy or log loss. Note that MSE is not used as squaring sigmoid violates convexity (cause local extrema to appear). Basic Implementation12345678from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionX, y = load_iris(return_X_y=True)clf = LogisticRegression(random_state=2).fit(X, y)clf.predict(X[:2, :])clf.predict_proba(X[:2, :])clf.score(X, y) NotesIn fact, logistic regression is simple, but the key thing here is actually on the mathematics behind gradient descent and its multi-dimensional variations. I'll discuss about them in future posts.","link":"/post/regressions-2/"},{"title":"Regression Models: GAM, GLM and GLMM","text":"OverviewGeneralized linear model (GLM) is a cure to some issues posted by ordinary linear regression. In the well-known linear regression model, we often assume . However, it often assumes that is not bounded when is not bounded. However, very often, we must restrict the values of within a fixed range. This may invalidated the ordinary linear model as the function behaviors near the boundary points can be very off. Generalized linear models aim to deal with this issue by allowing for that have arbitrary distributions (not just gaussian distribution), a function of (the link function) to vary linearly with (rather than assuming that a direct linear relationship between and ). Generalized Additive Model (GAM) and Generalized Linear Mixed Model (GLMM) are extensions to GLM with special functions applied to differenet elements in . GLMWe note that GLM has three major parts: An exponential family of probability distributions: , some examples include: normal exponential gamma chi-squared beta Dirichlet Bernoulli categorical Poisson A function of predictor (in GLM it is , in extended models, it can be other things, see GAM and GLMM), we can estimate via maximum likelihood or Bayesian methods like laplace approximation and Gibbs sampling, etc. A link function such that (sometime we may have tractable distribution for variance 1. Pros and Cons for GLM and GLMM Pros: Easy to interpret Easy to grasp Coefficients can be further used in numerical models Easy to extend: link functions, fixed and random effects, correlation structures Cons: Not good for dynamic models (the model is not linear and transformation may not help or would loose information Generalized additive models (GAMs) GAMs are extensions to GLMs in which the linear predictor is not restricted to be linear in the covariates but is the sum of smoothing functions applied to the each . For example, Is useful if relationship between Y and X is likely to be non-linear but we don't have any theory or any mechanistic model to suggest a particular functional form Each is linked with by a smoothing function instead of a coefficient GAMS are data-driven rather than model-driven, that is, the resulting fitted values do not come from an a priori model (non-parametric) All of the distribution families allowed with GLM are available with GAM 1. Pros and Cons for GAM Pros: By combining the basis functions GAMs can represent a large number of functional relationship (to do so they rely on the assumption that the true relationship is likely to be smooth, rather than wiggly) Particularly useful for uncovering nonlinear effects of numerical covariates, and for doing so in an \"automatic\" fashion More Flexible as now each sample's Y is associated with its X by a smoothing function instead of a coefficient Cons: Interpretability of the coefficient need to be estimated graphically Coefficients are not easily transferable to other datasets and parameterization Very sensitive to gaps in the data and outliers Lack underlying theory for the use of hypothesis tests one solution is to do bootstrapping and get aggregated result for more reliable confidence bands 2. Examples of GAM (different predictor representation functions): Loess (Locally weighted regression smoothing) The key factor is the span width (usually set to be a proportion of the data set: 0.5 as a standard starting point) Main idea: Split the data into separate blobs using sliding windows and fit linear regressions in each blob/interval Pros: Easily interpretable. At each test case, a local linear model is fit (eventually explained by linear behaviours) a popular way to see smooth trends on scatterplots Cons: If there are a lot of data points, fitting a LOESS over the entire range of the predictor can be slow because so many local linear regressions must be fit. Regression Splines (piecewise polynomials over usually a finite range) Main constraint is that the splines must remain smooth and continuous at knots To avoid overfitting of splines, penalty terms are added The penalty term also reflects the degree of smoothness in the regression The less smooth the regression is (after fitting the spline functions), the higher the penalty terms Pros: cover all sorts of nonlinear trends and are computationally very attractive because spline terms fit exactly into a least squares linear regression framework. Least squares models are very easy to fit computationally Cons: It is possible to create multidimensional splines by creating interactions between spline terms for different predictors. This suffers from the curse of dimensionality like KNN because we are trying to estimate a wavy surface in a large dimensional (many variable) space where data points will only sparsely cover the many many regions of the space GLMMThe model has the form: where is the design matrix for the random effects (the random complement to the fixed ). is a vector of the random effects (the random complement to the fixed ). The random effects are just deviations around the value in , which is the mean. Usually is a sparse matrix that assigns random effects to each element. We nearly always assume that with being the covariance matrix of the random effects. Assuming that the random effects are independent, we can have being a diagonal matrix with entries and . 1. Code implementationI recommend beginners to use statsmodels package because the output via .summary() function is very clear to read. For advanced users, you may implement the function yourself by referring to the mathematical expressions and package documentations from the following statsmodels: statsmodels.formula.api.mixedlm pymc3 theano pystan tensorflow keras 2. A sample code using statsmodels12345678910111213141516171819202122import statsmodels.formula.api as smffrom patsy import dmatricesformula = \"rt ~ group*orientation*identity\"#formula = \"rt ~ -1 + cbcond\"md = smf.mixedlm(formula, tbltest, groups=tbltest[\"subj\"])mdf = md.fit()print(mdf.summary())fe_params = pd.DataFrame(mdf.fe_params,columns=['LMM'])random_effects = pd.DataFrame(mdf.random_effects)random_effects = random_effects.transpose()random_effects = random_effects.rename(index=str, columns={'groups': 'LMM'})#%% Generate Design Matrix for later useY, X = dmatrices(formula, data=tbltest, return_type='matrix')Terms = X.design_info.column_names_, Z = dmatrices('rt ~ -1+subj', data=tbltest, return_type='matrix')X = np.asarray(X) # fixed effectZ = np.asarray(Z) # mixed effectY = np.asarray(Y).flatten()nfixed = np.shape(X)nrandm = np.shape(Z)","link":"/post/regressions-3/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part I","text":"IntroductionRecently I've been learning about reinforcement learning from amazing lectures from David Silver. These provide an overview of the classical algorithms in RL and potential challenges for future researches, in the subsequent blogs, I'll talk about the major aspects of RL and provide some solid math details on how algorithms in RL is executed. What is Reinforcement LearningAn RL agent may include one or more of these components: Policy: agent’s behaviour function It is a map from state to action Deterministic policy: a = Stochastic policy: Value function: how good is each state and/or action Value function is a prediction of future reward Used to evaluate the goodness/badness of states And therefore to select between actions Model: agent’s representation of the environment A model predicts what the environment will do next predicts the next state predicts the next (immediate) reward, this often takes the form of expectation It is derived for a classical problem - Exploration vs Exploitation. There is no supervisor, only a reward signal. Sometimes, feedback is delayed, not instantaneous. Time really matters (sequential, non i.i.d data); and Agent's actions affect the subsequent data it receives. Prediction vs ControlRL problems is often classified into a prediction problem or a control problem Prediction: Given a policy, evaluate the future Control: Find the best policy Markov Decision Process (MDP)Before venturing into the exact algorithms, let's lay out some fundamental math concepts here. Prior Knowledge Basic Probability Knowledge Basic Stochastic Process: Markov Chain Transition Matrix Problem setup This is an RL setting where the environment is fully observable The current state completely characterises the process Almost all RL problems can be formalised as MDPs (Bandit are MDP with 1 state &amp; finite/infinite actions) Terminologies Markov Reward Process A Markov reward process is a Markov chain with values. In addition to , we have reward function and a discount factor Return The return is the total discounted reward from time-step : Intuitively, the discount factor favours future rewards at a nearer date State-value function The state value function of an MRP is the expected return starting from state Bellman Equation We apply one-step analysis on and observe that: Therefore, we find that value function of any state is only dependent on its outgoing states (successors) We can then construct a matrix equation: , which can be solved in (hence very expensive) For large state set, more efficient algorithms utilising Dynamic programming (DP) is preferred MDP A Markov decision process (MDP) is a Markov reward process with decisions In addition to states, we have a set of actions The probability and reward functions now conditionally depend on and simultaneous as follows Policy A policy is a distribution over actions given states: A policy is stationary (the probability does not change for different iterations) State-value function and Action-value function for MDP (Differs from 3.) State-value function: the expectation taken w.r.t the policy Action-value function: Note that Applying Bellman equation on and Note that we cannot do simple one-step analysis since there are 2 variables and now. Instead, we try to apply OSA on w.r.t , and then do OSA on w.r.t to get Bellman Expectation Equation on , and swap the 2 variables’ order to derive equation for First step (Bellman Expectation Equation): Second step (Bellman Optimality Equation): Optimality We try to maximize and (Goal 1) Theorem suggests existence of policy that achieves Goal 1 deterministically. An optimal policy can be found by choosing actions for each state such that the choices maximise over Solving for optimality The Bellman equations in 8 is often nonlinear and has no closed form in general We often need to apply sequential methods to solve it Value iteration Policy Iteration Q-Learning Sarsa To read more on extensions, refer to Page 49 of this slides.","link":"/post/reinfocement-1/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part II","text":"Dynamic Programming in RLIntroduction DP assumes full knowledge of the MDP A prediction problem: The input is an MDP/MRP and a policy . The output is a value function . A control problem: The input is an MDP. The output is the optimal value function and an optimal policy . Synchronous DPThe following table summarizes the type of problems that is solved synchronously via iteration/evaluation algorithms: Problem Bellman Equation Algorithm Prediction Bellman Expectation Equation Iterative Policy Evaluation Control Bellman Expectation Equation Policy Iteration + Greedy Policy Improvement Policy Iteration Control Bellman Optimality Equation Value Iteration Iterative Policy Evaluation Problem: evaluate a given policy Algo sketch: Assign each state with an initial value (for example: ) Following the policy, compute the updated value function using the Bellman Expectation Equation Iterate until convergence (proven later) Policy Improvement Upon Evaluation of a policy , we can seek to greedily improve the policy such that we obtain . (expr 1) The greedy approach acts as selecting . (eq 1) We can prove that eq 1 leads to expr 1 as follows: In one step: . Note that is a deterministic policy. Observe that Hence Basically, we find that this method is equivalent to solving the Bellman Optimality equation. So we obtain as an optimal policy Note that this process of policy iteration always converges to . Drawback: Policy Iteration always Evaluation an entire Policy before it starts to improve on the policy. This may be highly inefficient if the evaluation of a policy takes very long time (e.g. infinite MDP) To deal with the Drawback, we utilise DP Value Iteration. Value Iteration We improve the value function in each iteration Note that we are only improving the value function, where this value function is based on any explicit policy Intuition: start with final rewards (again all 0 for example) and work backwards Now assume we know the solution to a subproblem , then we can find by one-step look ahead: Therefore, we can always update the value function in each iteration backwards until convergence. Contraction Mapping Theorem To be updated upon publishing the markdown Refer to page 28 - 42 (DP) Asynchronous DPThere are 3 simple ideas, which I haven't learning in detail: In-place dynamic programming Prioritised sweeping Real-time dynamic programming","link":"/post/reinfocement-2/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part III","text":"Model - Free Prediction1. IntroductionThis is an important chapter that lays the fundamental work for model-free control algorithms. In this blog we shall see a few important ideas (MC, TD, online/offline, forward/backward learning) being discussed. While this chapter is not math-intense, it is imperative for us to remember the concepts before moving onto control algorithms. To begin with, note that this is a prediction problem. Hence we are still only going to predict the final based on a given . However, the Model-free part suggests that we no longer require an MDP to be explicitly defined/given. This is because we are going to use a sampling strategy. Sampling:If a strategy derives certain functions (in this case ) directly via episodes of observations, we say that this strategy applies sampling method. 2. Monte Carlo (MC) Policy Evaluation MC learns from complete episodes (no bootstrapping) Monte-Carlo policy evaluation uses empirical mean return instead of expected return First-step MC For each state , only conisder the first time that is visited in each episode (update at most 1 time per run) Increment counter Increment total return Estimate mean return value The estimator converges to when number of visits approaches infinity Every-step MC For each state , conisder each time that is visited in each episode (update at most 1 time per run) For instance, , then we update and where The remaining part are the same as First-step MC Incremental Monte-Carlo Updates Based on the idea Hence we take perspective of each episodes observations instead of states . In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes. Hence the formula is tweeked. Limitation:Since MC only updates when the episode terminates (hence 'complete'), it cannot be applied to process which may run infinitely. 3. Temporal Difference (TD) Learning A solution to the 'incomplete' episodes problem by applying bootstrapping The above is replaced by an estimate (estimated return) based on the Bellman Expectation Equation. is called TD target is called TD error As now only depends on the next time-step and not the entire episode result , we say it is online (and hence the other method which updates everything when episode ends is offline). 4. Bias Variance trade-off Computation using is unbiased, but with high variance due to all rewards from transitions and actions selected at random Computation using TD target is biased, but with much lower variance, since we we only have one reward to be varied (note that is known and fixed during update of By the classical trade-off analysis, we see that TD is more sensitive to inital values TD is usually more efficient than MC 5. More comparisons between TD and MC MC TD converges to solution with minimum mean-squared error max likelihood Markov model Convergence with function approximation Yes No Exploits Markov property No Yes More efficient in Non-Markov env Markov env 6. TD()Now we explore alternative estimators for n-step return: -return: Here by utilising , we have TD() as a new evaluation policy. However, notice from the expression above that is forward-looking, meaning that this would require transition steps until the episodes end. This faces exactly the same limitation as MC!!! 7. Solution: Backward online evaluationInstead of look for values in future time-steps, we use values from earlier time-steps. This method is called a backward view of TD(). In essence, we update online, every step, from incomplete sequences. Based on a theorem, the sum of offline updates is identical for forward-view and backward-view TD(). 8. Eligibility TraceThe key to Backward TD() is eligbility trace, we intuitively derive contributing factors from earlier time steps as follows: Frequency heuristic: assign credit to most frequent states Recency heuristic: assign credit to most recent states ( discount rate) Combing 2 heuristics above, we obtain a rule and 𝟙.Hence the new update formula is . Observe now that we need to update for every state upon each time-step. 9. Additonal note TD(1) is roughly equivalent to every-visit Monte-Carlo TD(0) is exactly equivalent to simple TD Model Free Control1. Main objectiveInstead of estimating the value function, we try to optimize the value function with an unknown MDP. 2. Recap of On-Policy vs Off-Policy On-policyLearn about from experience sampled from previous rounds in .In essence trying to improve by running it using current agent iteself. Off-policyLearn about from experience sampled from previous rounds (or complete run) in .In essence trying to improve by Observing another policy getting run by another agent and deduce several directions to improve . 3. Generalised Policy Iteration With Monte-Carlo EvaluationIn general we can following the policy iteration method introduced in chapter 3 following 2 steps: Policy evaluation - Monte-Carlo policy evaluation: We want to evaluate . However, with out MDP, we cannot determine easily using a simple State-Value Function. So we must resort to a Action-Value Function, i.e., . We further observe that in each iteration, we must run the full policy to obtain for every action/state pair. This is highly inefficient as we do not know how long it takes. Hence instead we do episodic updates using . That is, we do not fully evaluate that policy, but sample state-action pair with current policy for times per episode and immediately improve the policy upon that. This manually set by us imposes guarantee on sampling complexity. We call the strategy above GILE MC control as it satisfies the GILE property. In conclusion, the evaluation phase is as follows: Sample kth episode using For each state and action in the episode: , Policy improvement: -Greedy policy improvement We choose actions that greedily maximizes the . We allow some degree of exploration by making such greedy choice with probability. Nothat this -Greedy policy works as we always have improvement like the proof shown in DP note. 4. GILE propertyGreedy in the Limit with Infinite Exploration (GLIE) has the following 2 parts: All state-action pairs are explored infinitely many times: The policy converges on a greedy policy, For example, -greedy is GLIE if reduces to zero at .This property enables the following theorem: GLIE Monte-Carlo control converges to the optimal action-value function: 5. Temporal Difference method From MC to TDTemporal-difference (TD) learning has several advantages over Monte-Carlo (MC) Lower variance Online Incomplete sequences Sarsa update of The most basic form is . Now recall from model-free prediction the variations of TD: n-step Sarsa Forward View Sarsa() Backward View Sarsa(): we use eligibility traces in an online algorithm . In each iteration, we upate for every pair. Off-policy learningIn this case, we evaluate target policy to compute or , but the evaluation was based on another (ongoing or completed) policy run It has the following advantages: Learn from observing humans or other agents Re-use experience generated from old policies Learn about optimal policy while following exploratory policy Learn about multiple policies while following one policy Importance sampling for Off-policyWe note that we can estimate the expectation of a different distribution via: One may trie to use returns generated from to evaluate via multiple sampling corrections: and then However, this multiple chaining may result in: Invalud computation when one of the while Dramatically increasing variance Hence, we consider adopting TD target for importance sampling instead of actual return . This removes the multiple chaining as the expression becomes: Unfortunately, in the above expression, we are still sticking to the policy in choosing when we update our for . This is not very reasonable, as our policy could potentially have a better choice of action. Importance sampling discounts this fact. Hence we may seek for alternative solution that removes to need to do importance sampling. 6. Q-LearningQ-learning is a method that resolves the above issue. We now consider update based on : we choose maximizer . This allows both behaviour and target policies to improve. Note that in this case, is improved via a -greedy case since is chosen randomly with probability and by theorem.","link":"/post/reinfocement-3/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part IV","text":"Value Function ApproximationWe know various methods can be applied for function approximation. For this note, we will mainly consider those differentiable methods: Linear Approximation and Neural Nets 1. Stochastic Gradient Descent (SGD)Here let's review a basic approximation strategy for gradient-based method: Stochastic Gradient Descent. First our aim is to minimize the mean square error (MSE) between our estimator and the true function. The error is represented by To attain we need to update the gradient until convergence. A full gradient update has the issue of converging at local minimum. Hence stochastic sampling with will work better in general. 2. LinearizationWe begin by considering a linear model. So where is the feature vector/representation of the current state space. The stochastic update in SGD is also updated to . On the other hand, we don't have an oracle for a known in practice, so we need ways to estimate it. This is where algorithm design comes in. Algorithm analysis1. linear Monte-Carlo policy evaluation To represent , we use . In every epoch, we apply supervised learning to “training data”: . The update is now Note that Monte-Carlo evaluation converges to a local optimum As is unbiased, it works even when using non-linear value function approximation 2. TD Learning We use for . TD(0) has the update formula: Linear TD(0) converges (close) to global optimum On the other hand we can use -return as substitute. This is a TD() method. Forward view linear TD(): Backward view linear TD() requires eligibility trace: 3. Convergence of Prediction Algorithms On\\Off-policy Algorithm Table-lookup Linear Non-Linear On-Policy MC Y Y Y On-Policy TD(0) Y Y N On-Policy TD() Y Y N Off-Policy MC Y Y Y Off-Policy TD(0) Y N N Off-Policy TD() Y N N Action-Value Function ApproximationNow we don't simply approximate a value function , but approximate action-value function instead. The main idea is just find . Both MC and TD work the same way exactly by substituting these items inside the expressions. ImprovementsGradient TDSome more recent improves aim to resolve the failure of convergence of off-policy TD algorithms. This gave birth to a Gradient TD algorithm that converges in both linear and non-linear cases. This requires an additional parameter to be added and tuned which reprsents the gradient of projected Bellman error. In a similar fashion, a gradient Q-learning is also invented, but with no gurantee on non-linear model convergence. Least Squares Prediction and Experience ReplayLS estimator is known to approximate well in general. So instead of correctly approximating , it may also be ideal to approximate instead. It is found that SGD with Experience Replay converges in this case. By \"Experience Replay\" we are storing the history in each epoch instead of discarding them after each iteration. And we randomly selection some of these \"data\" for stochastic update in SGD. Deep Q-Networks (DQN) DQN uses experience replay and fixed Q-targets It takes actions based on a -greedy policy Store transition in replay memory (experience replay) Sample random mini-batch of transitions from Compute Q-learning targets w.r.t. old, fixed parameters (fixed Q-target: not the latest but a computed some batches ago) In general, LS-based methods work well in terms of convergence but suffers from computational complexity.","link":"/post/reinfocement-4/"},{"title":"Reinforcement Learning - Theoretical Foundations: Part V","text":"Policy Gradient1. General Overview Model based RL: Pros 'Easy' to learn: via procedure similar to supervised learning Learns everything from the data Cons Objective captures irrelevant info May focus on irrelevant details Computing policy (planning) is non-trivial and expensive (offline/static evaluation) Value based RL: Pros Closer to True objective Fairly well understandable - somewhat similar to regression Cons Still not the true objective (similar to model-based) Policy based RL: Pros Always try to find the true objective (directly targeting the policy) Cons Ignore some useful learnable knowledge (like policy state/action values) [but can be overcomed by combining with value-function approximation] 2. Model-free Policy based RL We directly parametrize the policy via; Advantages: Better convergence properties (gradient method) Effective in high-dimensional or continuous action spaces Can learn stochastic optimal policies (like a scissor-paper-stone game) Sometimes policies are simple while values and models are more complex (large environment but easy policy) Disadvantages: Susceptible to local optima (especially with non-linear function approximator) Often obtain knowledge that is specifc and does not always generalize well (high variance) Ignores a lot of information in the data (when used in isolation) 3. Policy Objective functionHere is a list of functions that can be used potentially measure the quality of policy . Each is evaluated depending on the things we are concerned about: In episodic environment, we can use the start value: In continuous environment, we can use the average value: where is the probability of being in state in the long run (long-term proportion) Otherwise, we replace the value function with the reward function so: Since the main target is now to optimize , we can now simply apply gradient-based method to solve the problem (in this case, gradient ascent) 4. Computing the policy gradient analyticallyWe know that the most important part of any policy function is just the policy expression . Hence, assuming that is differentiable, we find: We then say that the score function (gradient base) of a policy is just . We further note that if is an expectaion function dependent on , i.e., , we can always apply this gradient base inside the expectation for gradient computation: $$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}{\\pi{\\theta}}[f(S,A)\\nabla_{\\theta} \\log \\pi_{\\theta}(S,A)]$$ This is called the score function trick. One useful property is that if b does not depend on the action . (expr 1) Now consider formally, we have the following Policy Gradient Theorem: For any differentiable policy , the policy gradient where is the long-term value Proof: Let's consider the expected return as the objective where is the filtration. note here is dependent on as it affects the filtration. Now: where is policy probabilty of this filtration . (Applying the score function trick) So: We further notice that is now a constant for every pair in for . However, for any , does not depend on , and by expr 1 above, we can see that . 5. Actor-Critic Algorithm Most basic Q Actor-Critic policy gradient still has high variance We can use a critic to estimate the action-value function: Actor-critic algorithms maintain two sets of parameters Critic: Updates action-value function parameters Actor: Updates policy parameters , in direction suggested by critic Actor-critic algorithms follow an approximate policy gradient: The critic is solving a familiar problem: policy evaluation, which now can be solve using methods via value-based methods. However, this approximation of the policy gradient introduces bias. A biased policy gradient may not find the right solution. We can choose value function approximation carefully so that this bias is removed. This is possible because of the Compatible Function Approximation Theorem below: If the following two conditions are satisfied: Value function approximator is compatible to the policy Value function parameters w minimise the mean-squared error Then exactly. Advantage Actor-Critic Recall expr 1, we can again apply this on to further reduce the variance introduced by a large value. Consider . This is called an advantage function. Since does not depend on the actions, so plays no role here. Then expr 1 with formula results in the following TD Actor-Critic We now apply approximation again (Compatible Function Approximation Theorem) on using the an estimated TD error In practice we can use an approximate TD error So now the critic update is Note given this variant that Critic can estimate value function from many targets at different time-scales. (MC, TD(0), Forward/Backward TD() Natural Actor-Critic refers to Policy Gradient for more on this content.","link":"/post/reinfocement-5/"},{"title":"SQL: Pick up the Basic within a day","text":"OverviewThis blog is for people who have learnt SQL at some points of their study (just like me): We can quickly recap on various important concepts in SQL. Basic Concepts Primary key: Always unique for each row of the table, it must be NOT NULL (automatically set when PRIMARY KEY is specified) Helps to identify each row even when row attributes are the same A table must have and only have 1 primary key Types: Surrogate key : An artificial key that has no mapping to anything (or business value) in real world. Natural key: A key that has mapping to real world thing: example: social security number/ NRIC / Passport number composiite key: 2 column entries combined to form a key Motivation: sometimes individuals of 2 entries cannot uniquely identify a row; Foreign key: Stores the primary key of a row in another database table The foreign key's column name NOT necessary to coincide with the foreign table's primary key column name A table can have more than 1 foreign key (or no foreign key at all) advance concept: Q: is it possible that TABLE A's foreign key is TABLE B's primary key and TABLE B's foreign key is TABLE A's primary key? A: Yes! cyclic dependency is valid in SQL. Example: employee's emp_id is department's manager_id; department's branch_id is employee's department_id. Q: is it possible that TABLE A's foreign key relates to itself? A: Yes! used to define relationships between rows within a table. Example: employee's super_id refers to a row in employee's table. Data Types: INT: – Whole number DECIMAL(M,N): – Decimal numbers - exact value, M digits, N after decimal point VARCHAR(K): – Sring of text of length K BLOB: – Binary Large Object, stores large data DATE: – 'YYYY-MM-DD' TIMESTAMP – 'YYYY-MM-DD HH:MM:SS' Difference between DROP and DELETE: DELETE DROP Data Manipulation Language command Data Definition Language Command To remove tuples from a table To remove entire schema, table, domain or constraints from the database Basic Operations Create Database 123SHOW DATABASES;CREATE DATABASE July_05;USE July_05; Logical Query Processing (IMPT) Step 1. FROM (includes JOINS) Step 2. WHERE Step 3. GROUP BY Step 4. HAVING Step 5. SELECT Step 6. ORDER BY CAUTION about column ordering: columns evaluated at later steps must be created in earlier steps Table Opeartions 123456789101112131415CREATE TABLE student ( student_id INT PRIMARY KEY, student_name VARCHAR(50), major VARCHAR(20), thr_id INT, -- can also remove the PRIMARY KEY above and add a line below -- PRIMARY KEY(student_id) FOREIGN KEY(thr_id) REFERENCES teacher(emp_id) ON DELETE SET NULL);-- Show all columns properties using the DESCRIBE keywordDESCRIBE student;-- Name Null Type-- student_id INT-- name VARCHAR(50)-- major VARCHAR(20) Delete or modify a table 123456DROP TABLE student;ALTER TABLE student ADD gpa DECIMAL(3,2);ALTER TABLE student DROP COLUMN gpa;ALTER TABLE student MODIFY COLUMN major TINYINT(1) UNSIGNED;ALTER TABLE student ADD CONSTRAINT pk_id PRIMARY KEY (student_id);ALTER TABLE student ADD CONSTRAINT fk_id FOREIGN KEY (thr_id) REFERENCES teacher(emp_id) ON DELETE SET NULL; Row Insertion 123-- Two ways of insertionINSERT INTO student VALUES(2, 'Kate', 'Sociology');INSERT INTO student(student_id, name) VALUES(3, 'Claire'); More properties of column 123456789CREATE TABLE student ( student_id INT AUTO_INCREMENT, -- id increase automaically if not specified student_id2 INT IDENTITY(1, 1) -- similar to AUTO_INCREMENT except -- IDENTITY(seed, increment) enables one to self define the starting value (seed) and the increment amount (increment) name VARCHAR(50) NOT NULL, -- name value cannot be empty major VARCHAR(20) UNIQUE, -- each row's major value must be unique across the table info VARCHAR(10) DEFAULT 'undecided', --info has 'undecided' as default value PRIMARY KEY(student_id)); Update the table 1234567-- Modify the contentUPDATE studentSET major = 'Biochemistry', name = 'What'WHERE major = 'Biology' or major = 'Chemistry'; -- if no WHERE is applied, the set applies to all-- Delete entriesDELETE FROM studentWHERE student_id = 5; SELECT keyword 1234567891011121314151617181920212223242526-- partial selectionSELECT student.name, student.majorFROM studentORDER BY major, name DESC; -- by default ascending order, DESC change to descending-- or both major and name descending by SELECT name, major FROM studentORDER BY major DESC, student_id DESC- other optional selection techniqueFROM student...WHERE major = 'chemistry' OR major = 'Bio';...WHERE name IN ('kate', 'Claire', 'Jack'); -- the use of IN keyword...WHERE birth_day BETWEEN '1970-01-01' AND '1975-01-01';...WHERE (birth_day &gt;= '1970-01-01' AND sex = 'F') OR salary &gt; 80000;...LIMIT 2 OFFSET 1;...SELECT TOP(100) -- select the 100 rows in the frontSELECT ... INTO samples -- select those columns into the \"sample\" table comparison keyword 1&lt;, &gt; , &lt;=, &gt;=, =, &lt;&gt; (means not equal to), AND, OR, ANY, ALL Functions to call 123456SELECT COUNT(sex), sexSELECT AVG(salary)SELECT SUM(salary)FROM employeeWHERE sex = 'F' AND birth_date &gt; '1971-01-01';GROUP BY sex; Wildcard 1234-- It is often used to find the string containing certain characters;SELECT *FROM clientWHERE client_name LIKE '%LLC'; UNION Motivation: row combine (fixed columns) Used to combine the multiple select statement into 1; Vertical join (add rows of the latter SELECT below the rows of former SELECT) Warning: each entry within the same column must have the same data-type1234567SELECT client.client_name AS Non_Employee_Entities, client.branch_id AS Branch_ID-- here the renaming using AS is very important to make the unioned row's column more logical-- e.g the client.branch_id and branch_supplier.branch_id unioned to be Branch_ID and branch_id separately in the table returnedFROM clientUNIONSELECT branch_supplier.supplier_name, branch_supplier.branch_idFROM branch_supplier; 12345678910111213SELECT * FROM( (SELECT CITY, LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) = (SELECT MIN(LENGTH(CITY)) FROM STATION) ORDER BY CITY) UNION (SELECT CITY, LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) = (SELECT MAX(LENGTH(CITY)) FROM STATION) ORDER BY CITY)) AS K -- note the use of AS is MUST includedORDER BY CITY JOIN Motivation: column combine (fixed row) The second table is used as an auxilary table for additional column entries in the first table1234SELECT employee.emp_id, employee.first_name, branch.branch_nameFROM employeeJOIN branch -- LEFT JOIN, RIGHT JOINON employee.emp_id = branch.mgr_id; 12345678910111213141516SELECT * FROM( (SELECT * FROM STATION AS P ORDER BY LENGTH(P.CITY) DESC ) AS A LEFT JOIN (SELECT * FROM STATION AS K ORDER BY LENGTH(K.CITY) DESC ) AS B ON A.ID = B.ID) -- here should not have AS ORDER BY A.CITY -- Must specify A or ambiguous warning Different types of join: INNER JOIN: the usual type of JOIN;Only those rows that match the ON criteria in both tables will be included and joined LEFT JOIN:All those rows in the left table are included but rows in the right table are included only when they match the ON criteria RIGHT JOIN:the symmetric idea with LEFT JOIN OUTER JOIN:All the rows in both tables are included (empty columns in the resultant table rows are treated with NULL) Nested query12345SELECT employee.first_name, employee.last_nameFROM employeeWHERE employee.emp_id IN (SELECT works_with.emp_id FROM works_with WHERE works_with.total_sales &gt; 50000); ON DELETE12ON DELETE SET NULL -- set the foreign key to null if the primary key which the foreign key refers to gets deletedON DELETE CASCADE -- delete the entire row if the primary key gets deleted, especially important if set null cannot be done (i.e the foreign key cannot be set to null) Trigger test123456789101112131415161718192021222324252627282930CREATE TABLE trigger_test ( message VARCHAR(100));-- the following code needs to be manually typed in mySQL codeDELIMITER $$ -- change the delimiter to $$CREATE TRIGGER my_trigger BEFORE INSERT ON employee FOR EACH ROW BEGIN INSERT INTO trigger_test VALUES('added new employee'); -- note the use of ; delimiter here END$$ -- we need to use the $$ as delimiter which is declared in line 168DELIMITER ; -- change the delimiter back to ;-- Conditional trigger_testDELIMITER $$CREATE TRIGGER my_trigger BEFORE INSERT -- can also be UPDATE, DELETE ON employee FOR EACH ROW BEGIN IF NEW.sex = 'M' THEN INSERT INTO trigger_test VALUES('added male employee'); ELSEIF NEW.sex = 'F' THEN INSERT INTO trigger_test VALUES('added female'); ELSE INSERT INTO trigger_test VALUES('added other employee'); END IF; END$$DELIMITER ;-- possible to drop the trigger case (done in client terminal):DROP TRIGGER my_trigger CTE: Common Table Expression1234567891011121314151617181920212223242526WITH Number -- here Number is the name of the CTE, can be anything AS(SELECT CustomerId , NTILE(1000) OVER(ORDER BY CustomerId) AS NFROM dbo.Customers),TopCustomer -- here we define the second CTE here, notice the comma \",\" above, indicates that the WITH keyword is still effectiveAS(SELECT MAX(CustomerId) AS CustIdFROM NumberGROUP BY N)SELECT -- this SELECT is together with the CTE Expression, not separate query C2.*INTO dbo.CustomersSample FROM TopCustomer AS C1INNER JOIN dbo.Customers AS C2 ON C1.CustId = C2.CustomerIdSELECT * FROM dbo.CustomersSample -- with the above cte method, we created a randomized sample in the dbo.customers table Functions and procedures Procedure Creation and Execution1234567891011DELIMITER $$CREATE PROCEDURE FizzBuzz()BEGIN DECLARE N INT DEFAULT 1; WHILE N &lt;= 100 DO SET N = N + 1; END WHILE;END$$DELIMITER ;CALL FizzBuzz(); Function Creation 12345678910111213141516DELIMITER $$CREATE FUNCTION multi( -- if function alrea exists, CREATE is changed to ALTER n INT , m INT) RETURNS INTDETERMINISTICBEGIN DECLARE result INT; SET result = m * n; RETURN result;END$$DELIMITER ;SELECT your_db_name.multi(2,3) AS result; Check if the function exists 1SHOW FUNCTION STATUS WHERE db = 'your database name'; String and numeric operations on valuesString operations 123456789101112SELECt UPPER(email) up , LOWER(last_name) low , CONCAT(first_name, ' ', last_name) full_name , LENGTH(email) email_len , CONCAT_WS(' | ', first_name, last_name) full_name_with_separator , TRIM(' hello ') AS trimmed , RIGHT(email, 3) AS right_three , LPAD(customer_id, 5, '000') AS left_zero_padding , FORMAT(address_id, 3) AS formated_3_float_pointFROM customerLIMIT 10; Regex Matching1 1234567SELECT CONCAT(first_name, ' ', last_name) FROM customerWHERE last_name ~ '^[^aeiou]' AND last_name ~* '[aeiou]$'ORDER BY right(first_name, 2);-- ~ : Case-sensitive, compares two statements, returns true if the first string is contained in the second-- ~* : Case-insensitive, compares two statements, returns true if the first string is contained in the second-- !~ : Case-sensitive, compares two statements, returns false if the first string is contained in the second-- !~* : Case-insensitive, compares two statements, return false if the first string is contained in the second Numeric Functions 1234567891011121314SELECT RAND() AS rand_num , ROUND(RAND() * 10, 2) AS rand_round_2_decimal , CEIL(RAND()) AS num_ceil , FLOOR(RAND()) as num_floor , RADIANS(180) AS pi_from_radian , DEGREES(3.141592653589793) AS pi_from_degree , ABS(-3) AS absolute_val , POWER(CUSTOMER_ID, 2) AS id_square , DATEDIFF(shop_date.date, return_date.date) AS usage_period , CONV(CUSTOMER_ID, 10, 16) AS to_hex , IFNULL(potential_NUll_column, 0) AS replacing_null_with_zeroFROM customerLIMIT 10; 1234-- A BlunderSELECT REPLACE(amount, 0, 1)FROM paymentLIMIT 5; Some Advanced operations: Window functions OVER clause determines window (the set of rows to operate on) PARTITION BY splits the result set into partitions on which the window function is applied Functions Available: Aggregate - COUNT, SUM, MIN, MAX, AVG Ranking - ROW_NUMBER, RANK, DENSE_RANK, NTILE Offset - FIRST_VALUE, LAST_VALUE, LEAD, LAG Statistical - PERCENT_RANK, CUME_DIST, PERCENTILE_CONT, PERCENTILE_DIST Windows Functions also have FRAMES ROWS RANGE 1. Demo on PARTITION BY1234567891011121314151617181920212223242526272829-- the non-window function wayWITH CTEAS(SELECT Sales_Id , SUM(Line_Total) AS TotalFROM Sales_DetailsGROUP BY Sales_Id);SELECT * FROM CTE AS AINNER JOIN Sales_Details AS B ON A.Sales_Id = B.Sales_Id; -- the window function waySELECT Sales_Id , Sales_Date , Item , Price , Quantity , Line_Total , COUNT(Line_Total) OVER(PARTITION BY Sales_Id) AS Line_Count , SUM(Line_Total) OVER(PARTITION BY Sales_Id) AS Sales_Total , SUM(Line_Total) OVER(PARTITION BY Sales_Date) AS Daily_Total , SUM(Line_Total) OVER() AS TotalFROM Sales_DetailsORDER BY Sales_Total; 2. On Ranking FunctionsRanking functions are available as part of Window Functions: ROW_NUMBER() unique incrementing integers RANK() same rank for same values, but keep the counting rolling 1, 1 (duplicate), 3, 4, 5 DENSE_RANK(): same rank for same values, but only increase rank by 1 when values change 1, 1 (duplicate), 2, 3, 4 RANK() vs DENSE_RANK(): RANK() will have rows with identical rank/ gaps in rank if we get tied values NTILE(N) assigns tile number based on the number of tiles required, just assign each row with a value from 0 - N ,in increase order Example: 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, …. ,N, N, N, N Usage: for partitioning/selective sampling of the data 12345678910111213141516171819SELECT -- note that here we use ORDER BY instead of PARTITION BY because order/rank is sort of important rather than the fixed set of value Sales_Id , Sales_Total , ROW_NUMBER() OVER(ORDER BY Sales_Total DESC) AS rownum , RANK() OVER(ORDER BY Sales_Total DESC) AS rnk , DENSE_RANK() OVER(ORDER BY Sales_Total DESC) AS dense , NTILE(3) OVER(ORDER BY Sales_Total DESC) AS ntleFROM dbo.Sales_2SELECT -- This is the modified way, we rank individual set of rows by adding on the PARTITION BY Sales_Id , Sales_Cust_Id , Sales_Total , ROW_NUMBER() OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS rownum , RANK() OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS rnk , DENSE_RANK() OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS dense , NTILE(3) OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Total DESC) AS ntleFROM dbo.Sales_2ORDER BY Sales_Cust_Id 3. GROUP BY1234567891011121314151617181920SELECT Sales_Cust_Id , SUM(Sales_Total) AS Total , RANK() OVER(ORDER BY SUM(Sales_Total) DESC) AS rnk -- note that we used SUM(Sales_Total) not Sales_Total or Total because we need the order of SUM(Sales_Total) for each customer and Total is not defined well , DENSE_RANK() OVER(ORDER BY SUM(Sales_Total) DESC) AS dnseFROM dbo.Sales_2WHERE Sales_Date &gt;= '2019-03-01'GROUP BY Sales_Cust_IdORDER BY rnk-- special OVER clause operationSELECT Sales_Customer_Id , SUM(Sales_Amount) AS Cust_Total , SUM(SUM(Sales_Amount)) -- this declaration will be wrong as the system says cannot aggregate over another aggregation , SUM(SUM(Sales_Amount)) OVER(ORDER BY (SELECT NULL)) AS Grand_Total -- this is the proper way as the aggregation is down to the OVER Clause not the SUM(Sales_Amount) function , AVG(SUM(Sales_Amount)) OVER(ORDER BY (SELECT NULL)) AS Average_Cust_Total , CAST((SUM(Sales_Amount) / SUM(SUM(Sales_Amount)) OVER(ORDER BY (SELECT NULL))) * 100 AS DECIMAL(6,2)) AS PctFROM dbo.SalesGROUP BY Sales_Customer_Id 4. Window FRAMES123456789101112131415161718192021222324252627SELECT Sales_Id , Sales_Date , Sales_Total , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS [Running Total] -- note that SUM is a window function here -- ROWS BETWEEN ... AND CURRENT ROW gives FRAME that is the set of rows from UNBOUNDED PRECEDING to this CUR ROW -- [Runnig Total] =&gt; need to put [] between a phrase with empty space \" \" , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS BETWEEN k PRECEDING AND CURRENT ROW) AS [Running Total] -- this line has the FRAME only between the CURRENT ROW and the k rows before it; , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS UNBOUNDED PRECEDING) AS [Running Total] -- this line is a simplified version for BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW , FROM dbo.Sales_2WHERE Sales_Cust_Id = 3ORDER BY Sales_DateSELECT Sales_Id , Sales_Date , Sales_Total , SUM(Sales_Total) OVER(ORDER BY Sales_Date ROWS UNBOUNDED PRECEDING) AS [Running Total] , CAST(AVG(Sales_Total) OVER(PARTITION BY Sales_Cust_Id ORDER BY Sales_Date ROWS UNBOUNDED PRECEDING) AS DECIMAL(8, 2)) AS [Running Average] -- this line enables running average for individual customers for all of them -- CAST .. AS DECIMAL(8,2) reduces the resultant running average into 2 decimal pointsFROM dbo.Sales_2ORDER BY Sales_Date 5. Lag and Lead Useful for trend analysis LAG - return the value from the previous row LEAD - return the value from the next row Format: 1LAG([Column], [Offset], [Value if NULL]) Demo:123456789SELECT Sales_Customer_Id , Sales_Date , LAG(Sales_Amount, 2, 0) OVER(PARTITION BY Sales_Customer_Id ORDER BY Sales_Date) AS PrevValue -- get the Sales_Amount 2 days before, if no value is in the entry 2 days before, set it to 0 (default is NULL) , Sales_Amount , LEAD(Sales_Amount, 2, 0) OVER(PARTITION BY Sales_Customer_Id ORDER BY Sales_Date) AS NextValue -- idea is the same, just change it to laterFROM dbo.Sales 6. Rolling window12345678SELECT * , SUM(SalesAmount) OVER(ORDER BY [Date] ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS Total -- the Window FRAME method and SUM function together makings the window \"rolling\" , SUM(SalesAmount) OVER(ORDER BY [Date] ROWS BETWEEN CURRENT ROW AND 9 FOLLOWING) AS Forward -- we use FOLLOWING for the future rowsFROM #TempSales -- nothing fancy about # sign hereORDER BY [Date] -- here [] is needed because Date itself is a SQL keyword 7. Variable Specification123456789101112131415161718SET GLOBAL some_global_variable = 1;SET @n = 10;SELECT @n AS num;SET@id = (SELECT payment_id FROM payment WHERE customer_id = 2 LIMIT 1);SELECT @id AS new_id;WITH cte AS(SELECT customer_id, COUNT(payment_id) ccFROM payment pGROUP BY customer_id),cnt AS (SELECT cc, COUNT(*) AS tcc, MAX(cc) OVER() AS mcc FROM cte GROUP BY cc)SELECT *FROM cteINNER JOIN cnt ON cnt.cc = cte.cc AND (cnt.tcc = 1 OR cnt.cc = cnt.mcc)ORDER BY cte.cc DESC, customer_id ASC; ConclusionThe above codes demonstrate the majorities of the SQL codes formats an engineer would ever need in its daily CRUD operations already. Thanks for reading!","link":"/post/sql-1/"},{"title":"SQL: Index and Optimization","text":"OverviewTo be honest, I'm not a pro-SQL programmer. I'm still on my journey to learn more about database and query optimization. In this blog I will just give whatever I've learnt about indexing and optimization and its mostly based on MySQL. Hope it helps! Guidelines Single Sheet query is much better than Multiple Sheet If multiple sheet is needed, Use JOIN well: Small Sheet drive Large Sheet (for e.g. left join in this case) Establish proper indexing Don't JOIN too many sheets as well Try best NOT to use subquery or Cartesian Product Window Funtions can be very helpful Indexes Allow faster retrieval of data Question: Why don't we just create loads of indexes? Ansewr: There is a trade-off, if loads of indexes exists on a table then those indexes need to be updated or maintained. In this case, DML operations suffer. 1. Index operations1234567891011-- show indicesSHOW INDEX FROM your_db_name.customer;-- Add indexALTER TABLE paymentADD INDEX idx_pay (payment_id); -- [index] can be appended by [unique] to ensure each index is uniqueCREATE FULLTEXT INDEX idx_staff ON customer (email); -- [fulltext] only applicable to string data-- Drop IndexDROP INDEX idx_pay ON payment For the full list of operations, you may refer to the official documentation of MySQL1 2. Clustered Indexes ALTER TABLE Permission WHen a table does not have a clustered index then the table is stored as a heap, if the table has a clustered index it is stored as a B-tree Data is stored in order of clustered index Only one clustered index can exists on one table Clustered indexes are effective on columns that consistent of unique increasing integers (like identity_set) When a primary key is created a unique clustered index is automatically created - this can be beneficial for queries that involve joins on this column. TODO Discuss B-Tree Study B+Tree and update","link":"/post/sql-2/"},{"title":"SQL: Going into Applications with MySQL and MongoDB","text":"IntroductionThis is a blog to note down some important concepts revolving MongoDB and MySQL, two of the most popular databases reprensentative of their respective domains: NoSQL and SQL. Many people know how to use these DBMS, but fail to appreciate their characteristics, when and why they are used in certain business solutions. I try to give as much high level comparisons as possible. This ensures that People can at least answer some basic interview questions when they look for a job using these tools. MongoDB A NoSQL database for high volumn data storage Dynamic schemas: creating entries without prior restriction of the data structure Represent data as of JSON documents and use JSON Query (JavaScript) Supports sharding and replication: it partitions data across multiple servers 1. ShardingThe components of a Shard include: A Shard – A MongoDB instance which holds the subset of the data. In production environments, ALL shards need to be part of replica sets. Config server – A mongodb instance which holds metadata about the cluster, basically information about the various mongodb instances which will hold the shard data. A Router – A mongodb instance responsible to re-directing the commands send by the client to the right servers. 2. The benefits of NoSQL in MongoDB Schema Free: MongoDB has a pre-defined structure that can be defined and adhered to, but also, if you need different documents in a collection, it can have different structures. Scaled both Horizontal and Vertical: Improve system's processing power via Horizontal: Adding more machines to expand the pool of resources Vertical: Adding more power to a single machine (CPU/Storage) Optimized for WRITE performances 3. The disadvantages of Non-SQL (without fixed schema) in MongoDB Does not support use of Foreign Keys Does not support optimization of JOIN operations MongoDB is not strong ACID (Atomic, Consistency, Isolation &amp; Durability) No Stored Procedure or functions, business logic must be implemented in the backend after data is retrieved (like Node.js). This may cause the operations to slow down. MySQL Relational Database (RDBMS) Represents data in tables and rows Predefine the Schema for the tables in the database Use SQL Supports Master-slave replication and master-master replication, i.e. copy data from one server to another Optimized for high performance JOIN across multiple tables 1. Disadvantages of MySQL (or traditional RDBMS) Scaled Only Vertically Transactions related to system catalog are not ACID compliant Sometimes a server crash can corrupt the system catalog Stored procedures are not cacheable MYSQL tables which is used for the procedure or trigger are most pre-locked. Risk of SQL injection attacks (if there is no predefined schema design, there is less of such a problem) Which to choose Characteristics MongDB MySQL Data nature A lot of unstructured data Mostly Structured data Application Real-time analytics, content management, various mobile apps Applications that requires multi-row transactions such as an accounting system Service priority Cloud Based Security and ACID/BASE rules are very improtant Data Volumn Large, high-speed volumn of data Stable data flow TODO Update content on MySQL (All the interview questions &amp; all the basic knowledge) InnoDB storage engine Sharding Indexing B+Tree Red-Black Tree Update MongoDB sharding policies Discuss Distrbuted Concensus policies","link":"/post/sql-3/"},{"title":"Some Supervised Learning Models","text":"OverviewAlmost everyone who learned about data science or machine learning knows what supervised learning is. However, not many have dived deep into the details of those well-known models. In this blog, I will share some critical aspects of these models (mainly mathematical) that will become helpful in both research and practical work. One note on functionality is that these models work for both regression and classification problems. KNN1. Definition K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions) Non-parametric technique Distance functions can be Euclidean: Manhattan (Or Hamming in the case of Classification): Minkowski: Preprocessing Standardized Distance: One major drawback in calculating distance measures directly from the training set is in the case where variables have different measurement scales or there is a mixture of numerical and categorical variables. The solution is to do standardization on each variable Dimension Reduction: Usually KNN's speed gets much slower when number of attributes increase. Hence we need to reduce the number of dimensions using techniques such as PCA and SVD 2. Choice of K In general, a large K value is more precise as it reduces the overall noise; however, the compromise is that the distinct boundaries within the feature space are blurred (Lower prediction accuracy if K is too large). Need to use cross validation to determine an optimal K 3. Strength and Weakness Advantage The algorithm is simple and easy to implement. There's no need to build a model, tune several parameters, or make additional assumptions. The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section). Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates \"We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset\". Disadvantages The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase. KNN doesn't know which attributes are more import ant Doesn't handle missing data gracefully Slow during prediction (not training) 4. Suitable scenario KNN is bad if you have too many data points and speed is important In ensemble model: k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending When you are solving a problem which directly focusses on finding similarity between observations, K-NN does better because of its inherent nature to optimize locally (i.e: KNN-search) Real Life Example: a simple recommender system (e.g: Given our movies data set, what are the 5 most similar movies to a movie query) 5. Interview Questions Use 1 line to describe KNN Answer: KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression). 6. Simple implementations123456789101112131415161718192021222324252627282930313233343536373839404142434445## For Regressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neighbors import KNeighborsRegressorfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import confusion_matrix,mean_squared_error,accuracy_scorefrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_split##Randomly generate some datadata = pd.DataFrame(np.random.randint(low = 2,high = 100,size = (1000, 4)), columns=[\"Target\",\"A\",\"B\",\"C\"])data.head()train_x,test_x,train_y,test_y = train_test_split(data.iloc[:,1:],data.Target,test_size = 0.2)print(train_x.shape, test_x.shape)scaler = MinMaxScaler(feature_range=(0,1))scaler.fit(train_x)scaled_train_x = pd.DataFrame(scaler.transform(train_x),columns=['A','B','C'])scaled_test_x = pd.DataFrame(scaler.transform(test_x),columns=[\"A\",\"B\",\"C\"])###Basic Performance testingknn_regressor = KNeighborsRegressor(n_neighbors=3,algorithm=\"brute\",weights=\"distance\")knn_regressor.fit(scaled_train_x, train_y)train_pred = knn_regressor.predict(scaled_train_x)test_pred = knn_regressor.predict(scaled_test_x)print(mean_squared_error(train_y,train_pred))print(mean_squared_error(test_y,test_pred))###Grid Search to determine Kknn_regressor = KNeighborsRegressor(algorithm=\"brute\",weights=\"distance\")params = {\"n_neighbors\": [1,3,5],\"metric\": [\"euclidean\", \"cityblock\"]}grid = GridSearchCV(knn_regressor,param_grid=params,scoring=\"neg_mean_squared_error\",cv=5)grid.fit(scaled_train_x, train_y)print(grid.best_params_)print(grid.best_score_)best_knn = grid.best_estimator_train_pred = best_knn.predict(scaled_train_x)test_pred = best_knn.predict(scaled_test_x) SVM1. Definition Normally a binary classification Basic Ideology: SVM is based on the idea of finding a hyperplane that best separates the features into different domains. Both for Regression and classification SVR SVC Support vectors: The points closest to the hyperplane margin maximizing hyperplane: the bound that maximize the distances from support vectors hard margin SVM: If the points are linearly separable then only our hyperplane is able to distinguish between them. Then we have very strict constraints to correctly classify each and every datapoint Soft margin SVM: If the points are not linearly separable then we need an update so that our function may skip few outliers and be able to classify almost linearly separable points. For this reason, we introduce a new Slack variable(ξ) We use CV to determine whether allowing certain amount of misclassification results in better classification in the long run Kernel: used to systematically find the specfic transformation that leads to class separation Polynomial Kernel: where = constant term and = degree of kernel done via Dot Product of a Feature Engineered Matrix Radial basis function kernel (RBF)/ Gaussian Kernel: where = Euclidean distance between &amp; γ: As the value of increases the model gets overfits. As the value of decreases the model underfits For Gaussian kernel: Most Important Idea abut Kernel: Our powerful kernel function actually calculate the high-dimensional relationships WITHOUT actually transforming the data to higher dimensions Multiclass classification: 2 types of strategy One vs. All: N-class instances then N binary classifier models, then pick the prediction of a non-zero class which is the most certain. One-vs-Rest classification One vs. One: N-class instances then N* (N-1)/2 binary classifier models (adopted in SVM). At prediction time, a voting scheme is applied: all C(C−1)/2 classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier. decision_function_shape='ovo' in the parameter to specify one-vs-one, else default is ovr 2. Pros &amp; ConsPros It is really effective in the higher dimension. Its solution is global optimal Effective when the number of features are more than training examples. Great when the data is noise-free and separable Less affected by outliers (if they are not the support vectors) SVM is suited for extreme case binary classification. Cons For larger dataset, it requires a large amount of time to process. Does not perform well in case of overlapped classes Cannot handle categorical data must convert via proper encoding Selection of hyperparameter/Kernel can be difficult resulting boundary plane are very difficult to interpret 3. Application When you need a non-linear approximator, use it When your dataset has a lot of features, use it When the matrix is sparse, use it When the data is unstructured, it is not used 4. Simple Implementation1234567from sklearn.svm import SVCsvc=SVC(kernel='linear') # Choices include 'rbf', 'poly', 'sigmoid'svc.fit(X_train,y_train)y_pred=svc.predict(X_test)print('Accuracy Score:')print(metrics.accuracy_score(y_test,y_pred)) Decision Tree1. Definition Decision Tree is a tree-based model that predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data). use a layered splitting process, where at each layer they try to split the data into two or more groups, so that data that fall into the same group are most similar to each other (homogeneity), and groups are as different as possible from each other (heterogeneity). It apples a top-down approach to data, so that given a data set, DTs try to group and label observations that are similar between them, and look for the best rules that split the observations that are dissimilar between them until they reach certain degree of similarity. Non-parametric technique Pruning: a technique used to deal with overfitting, that reduces the size of DTs by removing sections of the Tree that provide little predictive or classification power. Simpler trees prefered (according to Occam's Razor) Post-prune: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made. 2. Types of DT CHAID (Chi-squared Automatic Interaction Detection) multiway DT chooses the independent variable that has the strongest interaction with the dependent variable. The selection criteria: For regression: F-test For classification: chi-square test Has no pruning function CART (Classification And Regression Tree) binary DT handles data in its raw form (no preprocessing needed), can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables. The selection metric: For Classification: Gini Impurity Index where is the % of data with label in the split The lower value indicates a better spliting For Regression: Least Square Deviation (LSD) the sum of the squared distances (or deviations) between the observed values and the predicted values. Often refered as 'sqaured residual', lower LSD means better split doesn't use an internal performance measure for Tree selection/testing Iterative Dichotomiser 3 (ID3) classification DT Entropy: Single Attribute: Multiple Attribute: where → Current state and → Selected attribute The higher the entropy, the harder it is to draw any conclusions from that information. Follows the rule — A branch with an entropy of zero is a leaf node and A brach with entropy more than zero needs further splitting The selection metric: Information Gain: where is number of splits and is a particular split The higher the gain, the better the split Limitation: it can't handle numeric attributes nor missing values C4.5 The successor of ID3 and represents an improvement in several aspects can handle both continuous and categorical data (regression + classification) can deal with missing values by ignoring instances that include non-existing data The selection metric: Gain ratio: a modification of Information gain that reduces its bias and is usually the best option Windowing: the algorithm randomly selects a subset of the training data (called a \"window\") and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. It captures all the \"rare\" instances together with sufficient \"ordinary\" cases. Can be pruned: pruning method is based on estimating the error rate of every internal node, and replacing it with a leaf node if the estimated error of the leaf is lower. 3. Strength and Weakness Advantage The algorithm is simple and easy to implement. Require very little data preparation The cost of using the tree for inference is logarithmic in the number of data points used to train the tree. Hence the training speed is high Good interpretability. Disadvantages Overfitting is quite common with decision trees simply due to the nature of their training. It's often recommended to perform some type of dimensionality reduction such as PCA so that the tree doesn't have to learn splits on so many features high variance, which means that a small change in the data can result in a very different set of splits, making interpretation somewhat complex. vulnerable to becoming biased to the classes that have a majority in the dataset. It's always a good idea to do some kind of class balancing such as class weights, sampling, or a specialised loss function. In more technical terms: it always look for a greedy option to split, thus more inclined towards a locally optimal split instead of a gloablly optimal one 4. Suitable scenarioConsideration: If the goal is better predictions, we should prefer RF, to reduce the variance. If the goal is exploratory analysis, we should prefer a single DT , as to understand the data relationship in a tree hierarchy structure. If there is a high non-linearity &amp; complex relationship between dependent &amp; independent variables, a tree model will outperform a classical regression method. When computational power is low, DT should be used When important features in the attributes are already identified, DT can be used When you demand more interpretability, DT should be used Use cases: healthcare industry: the screening of positive cases in the early detection of cognitive impairment Environment/Agriculture: DTs are used in agriculture to classify different crop types and identify their phenological stages/recognize different causes of forest loss from satellite imagery Sentiment Analysis: identify emotion from text Finance: Fraud Detection 5. Simple implementations123456789from sklearn import treedt = tree.DecisionTreeClassifier(random_state=1, max_depth=4)dt.fit(data_train, label_train)dt_score_train = dt.score(data_train, label_train) print(\"Training score: \",dt_score_train)dt_score_test = dt.score(data_test, label_test)print(\"Testing score: \",dt_score_test)dt2.predict(data_pred) Naive Bayes1. Definition The Naïve Bayes Classifier belongs to the family of probability classifier, using Bayesian theorem. The reason why it is called 'Naïve' because it requires rigid independence assumption between input variables. The classification formula is simple: Why is it called 'Naive': It is naive because while it uses conditional probability to make classifications, the algorithm simply assumes that all features of a class are independent. This is considered naive because, in reality, it is not often the case. Laplace Smoothing is also applied in some cases to solve the problem of zero probability. Different types of NB: Gaussian: It is used in classification and it assumes that features follow a normal distribution. Multinomial: It is used for discrete counts. For example, let's say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of 'word occurring in the document', we have 'count how often word occurs in the document', you can think of it as 'number of times outcome number x_i is observed over the n trials'. Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with 'bag of words' model where the 1s &amp; 0s are 'word occurs in the document' and 'word does not occur in the document' respectively. You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, \"ensembling, boosting, bagging\" won't help since their purpose is to reduce variance. Naive Bayes has no variance to minimize. 2. Pros &amp; ConsPros It is easy and fast to predict the class of the test data set. It also performs well in multi-class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption). Cons Can't learn the relationship among the features because assumes feature independence the assumption of independent predictors unlikely to hold. In real life, it is almost impossible that we get a set of predictors which are completely independent. 3. Applications Realtime prediction (because it's fast) When Dataset is Huge (high-dimension) When training dataset is small Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments) Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not. 4. Simple Implementation123456789from sklearn.naive_bayes import GaussianNBmodel = GaussianNB()# fit the model with the training datamodel.fit(train_x,train_y)# predict the target on the train datasetpredict_train = model.predict(train_x)print('Target on train data',predict_train)","link":"/post/supervised-learning/"},{"title":"Topic Modeling with Latent Dirichlet Allocation","text":"OverviewTopic modeling: Topic modelling refers to the task of identifying topics that best describes a set of documents. In this blog, we discuss about an popular advanced model that Definition To explain in plain word: LDA imagines a fixed set of topics. Each topic represents a set of words. The goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics. An important note to take is that LDA aims to explain the document-level idea, meaning it has less focus on the meaning of each word/phrase in the document, but rather the topic the document falls under Dirichlet Process: A family of stochastic process to produce a probability distribution Used in Bayesian Inference to describe the prior knowledge about the distribution of random variables Dirichlet Distribution: Basically a multivariate generalisation of the Beta distribution: where is a beta distribution Outputs: where Often called “a distribution of distribution” symmetric Dirichlet distribution: a special case in the Dirichlet distribution where all are equal, hence use a single scalar in the model representation Impact of : (a scaling vector for each dimension in ) : Sparsity increases The distribution is likely bowl-shaped (most probable vectors are sparse vectors like or In LDA, it means a document is likely to be represented by just a few of the topics Sparcity decreases We will have a unimodel distribution (most probable vectors are in the center) In LDA, it means a document is likely to contain most of the topics makes documents more similar to each other The conjugate prior of multinomial distribution is a Dirichlet distribution LDA\\s keywords k: Number of topics a document belongs to (a fixed number) V : Size of the vocabulary M: Number of documents N: Number of words in each document w: A word in a document. This is represented as a one hot encoded vector of size V W: represents a document (i.e. vector of \"w\"s) of N words D: Corpus, a collection of M documents z: A topic from a set of k topics. A topic is a distribution words. For example it might be, Animal = (0.3 Cats, 0.4 Dogs, 0 AI, 0.2 Loyal, 0.1 Evil) θ: The topic distribution for each of the document based on a parameter α β: The Dirichlet distribution based on parameter η LDA's procedure This is quite complicated LDA's document generation α has a topic distribution for each document (θ ground for each document) a (M x K) shape matrix η has a parameter vector for each topic. η will be of shape (k x V) In the above drawing, the constants actually represent matrices, and are formed by replicating the single value in the matrix to every single cell. θ is a random matrix based on dirichlet distribution, where represents the probability of the th document to containing words belonging to the th topic a relatively low β is also a dirichlet distribution as θ, represents the probability of the th topic containing the th word in a vocabulary of size ; The higher the , the more th topic is likely to contain more of the words, and makes the topics more similar to each other Detailed steps: For each topic, draw a distribution over words For each document Draw a vector of topic proportions . E.g: [climate = 0.7, trade = 0.2, housing = 0.1, economy = 0] For each word slot allocated, draw a topic assignment , then draw a word We want to infer the join probability given our observations, We infer the hidden variables or latent factors by observing the corpse of documents, i.e. finding The learning part Idea 1: Gibbs sampling: A point-wise method (Possible but not optimal) Intuition: The setting which generates the original document with the highest proability is the optimal machine The mathematics of collapsed gibbs sampling (cut back version) Recall that when we iterate through each word in each document, we unassign its current topic assignment and reassign the word to a new topic. The topic we reassign the word to is based on the probabilities below. where - number of word assignments to topic in document - number of assignments to topic in document - smoothing parameter (hyper parameter - make sure probability is never 0) - number of words in document - don’t count the current word you’re on - total number of topics - number of assignments, corpus wide, of word to topic - number of assignments, corpus wide, of word to topic - smoothing parameter (hyper parameter - make sure probability is never 0) - sum over all words in vocabulary currently assigned to topic size of vocabulary i.e. number of distinct words corpus wide Done with each word in a document (to classify them into a topic) Done in an iterative way (different topics for same words in a document: 1st \"happy\" may be topic 1, which affects 2nd \"happy\" to be topic 2 in the same document) Main steps: For each word in a document : The word will be allocated to Note that is the one used in the original and in Iterate until each document &amp; word's topic is upadted Aggregate the results from all documents to update the word distribution for each topic Repeat the previous steps until corpus objective converges Idea 2: variational inference: The key concept of variance inference is approximate posterior with a distribution using some known families of distribution that is easy to model and to analyze. Then, we train the model parameters to minimize the KL-divergence between q and p. KL-divergence: ,also called \"relative entropy\" Further reduction in complexity for high dimensional distribution is possible Idea 3: Mean-field variational inference breaks up the joint distribution into distributions of individual variables that are tractable and easy to analyze It is not easy to optimize KL-divergence directly. So let us introduce the Evidence lower bound (ELBO) by maximizing ELBO, we are minimizing KL-divergence: view explanation here When minimizing ELBO, we don’t need Z. No normalization is needed. In contrast, KL’s calculation needs the calculated entity to be a probability distribution. Therefore, we need to compute the normalization factor Z if it is not equal to one. Calculating Z is hard. This is why we calculate ELBO instead of KL-divergence. There are a lot of math details involving exponential family operations, but the general picutre is captured by the graph below Evaluation using similarity query Ok, now that we have a topic distribution for a new unseen document, let's say we wanted to find the most similar documents in the corpus. We can do this by comparing the topic distribution of the new document to all the topic distributions of the documents in the corpus. We use the Jensen-Shannon distance metric to find the most similar documents. What the Jensen-Shannon distance tells us, is which documents are statisically \"closer\" (and therefore more similar), by comparing the divergence of their distributions. Jensen-Shannon is symmetric, unlike Kullback-Leibler on which the formula is based. This is good, because we want the similarity between documents A and B to be the same as the similarity between B and A. The formula is described below. For discrete distirbutions and , the Jensen-Shannon divergence, is defined as where and is the Kullback-Leibler divergence The square root of the Jensen-Shannon divergence is the Jensen-Shannon Distance: The smaller the Jensen-Shannon Distance, the more similar two distributions are (and in our case, the more similar any 2 documents are) Pros &amp; ConsPros An effective tool for topic modeling Easy to understand/interpretable variational inference is tractable θ are document-specific, so the variational parameters of θ could be regarded as the representation of a document , hence the feature set is reduced. z are sampled repeatedly within a document — one document can be associated with multiple topics. Cons Must know the number of topics K in advance Hard to know when LDA is working - topics are soft-clusters so there is no objective metric to say \"this is the best choice\" of hyperparameters LDA does not work well with very short documents, like twitter feeds Dirichlet topic distribution cannot capture correlations among topics Stopwords and rare words should be excluded, so that the model doesnt overcompensate for very frequent words and very rare words, both of which do not contribute to general topics. Real-word application Text classification Book recommender Article clustering/image clustering understanding the different varieties topics in a corpus (obviously) getting a better insight into the type of documents in a corpus (whether they are about news, wikipedia articles, business documents) quantifying the most used / most important words in a corpus document similarity and recommendation. Long Code example123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235# import dependencies%matplotlib inlineimport pandas as pdimport numpy as npimport nltkfrom nltk.corpus import stopwordsimport gensimfrom gensim.models import LdaModelfrom gensim import models, corpora, similaritiesimport refrom nltk.stem.porter import PorterStemmerimport timefrom nltk import FreqDistfrom scipy.stats import entropyimport matplotlib.pyplot as pltimport seaborn as sns# Read in data; only keep essential columns and English language articlesdf = pd.read_csv('lda_fake.csv', usecols = ['uuid','author','title','text','language','site_url','country'])df = df[df.language == 'english']df = df[df['text'].map(type) == str]df['title'].fillna(value=\"\", inplace=True)df.dropna(axis=0, inplace=True, subset=['text'])# shuffle the datadf = df.sample(frac=1.0)df.reset_index(drop=True,inplace=True)# Define some functions to clean and tokenize the datadef initial_clean(text): \"\"\" Function to clean text of websites, email addresess and any punctuation We also lower case the text \"\"\" text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text) text = re.sub(\"[^a-zA-Z ]\", \"\", text) text = text.lower() # lower case the text text = nltk.word_tokenize(text) return textstop_words = stopwords.words('english')def remove_stop_words(text): \"\"\" Function that removes all stopwords from text \"\"\" return [word for word in text if word not in stop_words]stemmer = PorterStemmer()def stem_words(text): \"\"\" Function to stem words, so plural and singular are treated the same \"\"\" try: text = [stemmer.stem(word) for word in text] text = [word for word in text if len(word) &gt; 1] # make sure we have no 1 letter words except IndexError: # the word \"oed\" broke this, so needed try except pass return textdef apply_all(text): \"\"\" This function applies all the functions above into one \"\"\" return stem_words(remove_stop_words(initial_clean(text)))# clean text and title and create new column \"tokenized\"t1 = time.time()df['tokenized'] = df['text'].apply(apply_all) + df['title'].apply(apply_all)t2 = time.time()print(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")# We'll use nltk to get a word frequency (by count) here and only keep the top most used words to train the LDA model on# first get a list of all wordsall_words = [word for item in list(df['tokenized']) for word in item]# use nltk fdist to get a frequency distribution of all wordsfdist = FreqDist(all_words)len(fdist) # number of unique words# choose k and visually inspect the bottom 10 words of the top kk = 50000top_k_words = fdist.most_common(k)top_k_words[-10:]# choose k and visually inspect the bottom 10 words of the top kk = 15000top_k_words = fdist.most_common(k)top_k_words[-10:]# k = 50,000 is too high, as the bottom words aren't even real words and are very rarely used (once in entire corpus)# k = 15,000 is much more reasonable as these have been used at least 13 times in the corpus# define a function only to keep words in the top k wordstop_k_words,_ = zip(*fdist.most_common(k))top_k_words = set(top_k_words)def keep_top_k_words(text): return [word for word in text if word in top_k_words] df['tokenized'] = df['tokenized'].apply(keep_top_k_words)# document lengthdf['doc_len'] = df['tokenized'].apply(lambda x: len(x))doc_lengths = list(df['doc_len'])df.drop(labels='doc_len', axis=1, inplace=True)print(\"length of list:\",len(doc_lengths), \"\\naverage document length\", np.average(doc_lengths), \"\\nminimum document length\", min(doc_lengths), \"\\nmaximum document length\", max(doc_lengths)) # plot a histogram of document lengthnum_bins = 1000fig, ax = plt.subplots(figsize=(12,6));# the histogram of the datan, bins, patches = ax.hist(doc_lengths, num_bins)ax.set_xlabel('Document Length (tokens)', fontsize=15)ax.set_ylabel('Normed Frequency', fontsize=15)ax.grid()ax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))plt.xlim(0,2000)ax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-', label='average doc length')ax.legend()ax.grid()fig.tight_layout()plt.show()### Drop short articlesLDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.From the histogram above, droping all articles less than 40 tokens seems appropriate.# only keep articles with more than 30 tokens, otherwise too shortdf = df[df['tokenized'].map(len) &gt;= 40]# make sure all tokenized items are listsdf = df[df['tokenized'].map(type) == list]df.reset_index(drop=True,inplace=True)print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df), \"articles\")# create a mask of binary valuesmsk = np.random.rand(len(df)) &lt; 0.999train_df = df[msk]train_df.reset_index(drop=True,inplace=True)test_df = df[~msk]test_df.reset_index(drop=True,inplace=True)def train_lda(data): \"\"\" This function trains the lda model We setup parameters like number of topics, the chunksize to use in Hoffman method We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize \"\"\" num_topics = 100 chunksize = 300 dictionary = corpora.Dictionary(data['tokenized']) corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']] t1 = time.time() # low alpha means each document is only represented by a small number of topics, and vice versa # low eta means each topic is only represented by a small number of words, and vice versa lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2) t2 = time.time() print(\"Time to train LDA model on \", len(df), \"articles: \", (t2-t1)/60, \"min\") return dictionary,corpus,lda dictionary,corpus,lda = train_lda(train_df)# show_topics method shows the the top num_words contributing to num_topics number of random topicslda.show_topics(num_topics=10, num_words=20)# select and article at random from train_dfrandom_article_index = np.random.randint(len(train_df))bow = dictionary.doc2bow(train_df.iloc[random_article_index,7])# get the topic contributions for the document chosen at random abovedoc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])# bar plot of topic distribution for this documentfig, ax = plt.subplots(figsize=(12,6));# the histogram of the datapatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)ax.set_xlabel('Topic ID', fontsize=15)ax.set_ylabel('Topic Contribution', fontsize=15)ax.set_title(\"Topic Distribution for Article \" + str(random_article_index), fontsize=20)ax.set_xticks(np.linspace(10,100,10))fig.tight_layout()plt.show()# select and article at random from test_dfrandom_article_index = np.random.randint(len(test_df))print(random_article_index)new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])print(test_df.iloc[random_article_index,3])new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])# bar plot of topic distribution for this documentfig, ax = plt.subplots(figsize=(12,6));# the histogram of the datapatches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)ax.set_xlabel('Topic ID', fontsize=15)ax.set_ylabel('Topic Contribution', fontsize=15)ax.set_title(\"Topic Distribution for an Unseen Article\", fontsize=20)ax.set_xticks(np.linspace(10,100,10))fig.tight_layout()plt.show()def jensen_shannon(query, matrix): \"\"\" This function implements a Jensen-Shannon similarity between the input query (an LDA topic distribution for a document) and the entire corpus of topic distributions. It returns an array of length M where M is the number of documents in the corpus \"\"\" # lets keep with the p,q notation above p = query[None,:].T # take transpose q = matrix.T # transpose matrix m = 0.5*(p + q) return np.sqrt(0.5*(entropy(p,m) + entropy(q,m))) def get_most_similar_documents(query,matrix,k=10): \"\"\" This function implements the Jensen-Shannon distance above and retruns the top k indices of the smallest jensen shannon distances \"\"\" sims = jensen_shannon(query,matrix) # list of jensen shannon distances return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances # this is surprisingly fastmost_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)most_similar_df = train_df[train_df.index.isin(most_sim_ids)]most_similar_df['title']","link":"/post/topic-modeling/"},{"title":"Unsupervised Learning: Measures about Clustering","text":"OverviewUnsupervised learning is a vast topic, and clustering is really a big part (yet not all) of it. Whenever we have some ideas about clusteirng, we should first ask: is this idea comparable to some existing works? Now to answer this, we need some evaluation strategies and choose measures for such evaluations. This is what today's blog will talk about. Distance MetricsWe have four most popular distance metrics outlined below. In essense, one should understand the structure of each metric, and when to use them. Minkowski Distance: Minkowski distance is a metric in Normed vector space. Formula p = 1, Manhattan Distance p = 2, Euclidean Distance p = ∞, Chebychev Distance Manhattan Distance: We use Manhattan Distance if we need to calculate the distance between two data points in a grid like path. Euclidean Distance: Euclidean distance formula can be used to calculate the distance between two data points in a plane. Cosine Distance: Mostly Cosine distance metric is used to find similarities between different documents. In cosine metric we measure the degree of angle between two documents/vectors(the term frequencies in different documents collected as metrics). This particular metric is used when the magnitude between vectors does not matter but the orientation. Formula: Evaluation Methods1. Clustering Tendency Before evaluating the clustering performance, making sure that data set we are working has clustering tendency and does not contain uniformly distributed points is very important. Hopkins Test: a statistical test for spatial randomness of a variable, can be used to measure the probability of data points generated by uniform data distribution. Null Hypothesis () : Data points are generated by non-random, uniform distribution (implying no meaningful clusters) Alternate Hypothesis (): Data points are generated by random data points (presence of clusters) If the H value is between {0.01, …,0.3}, the data is regularly spaced. If the H value is around 0.5, it is random. If the H value is between {0.7, …, 0.99}, it has a high tendency to cluster. The Hopkins Test is highly influenced by outliers Sample Code123456789101112131415161718import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)from sklearn.decomposition import PCAfrom sklearn import datasetsfrom sklearn.preprocessing import scalefrom pyclustertend import hopkins ## the hopkins testfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltheart_df = pd.read_csv(\"heart.csv\")X = heart_df[heart_df.columns[~heart_df.columns.isin([\"target\"])]].valuesy = heart_df[heart_df.columns[heart_df.columns.isin([\"target\"])]].values.flatten()display(hopkins(X, X.shape[0]))display(hopkins(scale(X),X.shape[0])) 2. Number of Optimal ClustersMainly 2 Direction Domain knowledge — Domain knowledge might give some prior knowledge on finding number of clusters. For example, in case of clustering iris data set, if we have the prior knowledge of species (sertosa, virginica, versicolor) , then k = 3. Domain knowledge driven k value gives more relevant insights. Data driven approach — If the domain knowledge is not available, mathematical methods help in finding out right number of clusters. Mainly 2 Methods Statistical approach: Gap statistic is a powerful statistical method to find the optimal number of clusters, k. Sum of within-cluster (intra-cluster) variance is calculated for different values of k. : Sum-of-within-Cluster variance of original data set for k clusters : Sum-of-within-cluster variance of reference data set (null reference data set of uniform distribution) of k clusters Formula: As Gap statistic quantifies this deviation, More the Gap statistic means more the deviation. Cluster number with maximum Gap statistic value corresponds to optimal number of cluster. Sample Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Gap Statistics%matplotlib inlineimport timeimport hashlibimport scipyimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom sklearn.cluster import KMeansfrom sklearn.datasets.samples_generator import make_blobsplt.rcParams['figure.figsize'] = 10, 10x, y = make_blobs(750, n_features=2, centers=12)plt.scatter(x[:, 0], x[:, 1])plt.show()def optimalK(data, nrefs=3, maxClusters=15): \"\"\" Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie Params: data: ndarry of shape (n_samples, n_features) nrefs: number of sample reference datasets to create maxClusters: Maximum number of clusters to test for Returns: (gaps, optimalK) \"\"\" gaps = np.zeros((len(range(1, maxClusters)),)) resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]}) for gap_index, k in enumerate(range(1, maxClusters)): # Holder for reference dispersion results refDisps = np.zeros(nrefs) # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop for i in range(nrefs): # Create new random reference set randomReference = np.random.random_sample(size=data.shape) # Fit to it km = KMeans(k) km.fit(randomReference) refDisp = km.inertia_ refDisps[i] = refDisp # Fit cluster to original data and create dispersion km = KMeans(k) km.fit(data) origDisp = km.inertia_ # Calculate gap statistic gap = np.log(np.mean(refDisps)) - np.log(origDisp) # Assign this loop's gap statistic to gaps gaps[gap_index] = gap resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True) return (gaps.argmax() + 1, resultsdf) # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal k, gapdf = optimalK(x, nrefs=5, maxClusters=15)print(f'Optimal k is: {k}') Elbow method: Within-cluster variance is a measure of compactness of the cluster. Lower the value of within cluster variance, higher the compactness of cluster formed. Sample Code12345678910111213141516171819202122232425262728293031# Elbow Methodimport matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn.datasets.samples_generator import (make_blobs, make_circles, make_moons)from sklearn.cluster import KMeans, SpectralClusteringfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import silhouette_samples, silhouette_score# Import the datadf = pd.read_csv('old_faithful.csv')# Standardize the dataX_std = StandardScaler().fit_transform(df)sse = []list_k = list(range(1, 10))for k in list_k: km = KMeans(n_clusters=k) km.fit(X_std) sse.append(km.inertia_)# Plot sse against kplt.figure(figsize=(6, 6))plt.plot(list_k, sse, '-o')plt.xlabel(r'Number of clusters $k$')plt.ylabel('Sum of squared distance') 3. Clustering QualityThere are majorly two types of measures to assess the clustering performance. For more details, check sklearn document on cluster performance evaluation. Extrinsic Measures: Require ground truth labels. Examples are Adjusted Rand index, Fowlkes-Mallows scores, Mutual information based scores, Homogeneity, Completeness and V-measure. Intrinsic Measures: Does not require ground truth labels. Examples are Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index etc. 123456789101112131415161718192021222324252627282930313233343536373839404142434445# silhouette analysisfor i, k in enumerate([2, 3, 4]): fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) # Run the Kmeans algorithm km = KMeans(n_clusters=k) labels = km.fit_predict(X_std) centroids = km.cluster_centers_ # Get silhouette samples silhouette_vals = silhouette_samples(X_std, labels) # Silhouette plot y_ticks = [] y_lower, y_upper = 0, 0 for i, cluster in enumerate(np.unique(labels)): cluster_silhouette_vals = silhouette_vals[labels == cluster] cluster_silhouette_vals.sort() y_upper += len(cluster_silhouette_vals) ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1) ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1)) y_lower += len(cluster_silhouette_vals) # Get the average silhouette score and plot it avg_score = np.mean(silhouette_vals) ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green') ax1.set_yticks([]) ax1.set_xlim([-0.1, 1]) ax1.set_xlabel('Silhouette coefficient values') ax1.set_ylabel('Cluster labels') ax1.set_title('Silhouette plot for the various clusters', y=1.02); # Scatter plot of data colored with labels ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels) ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250) ax2.set_xlim([-2, 2]) ax2.set_xlim([-2, 2]) ax2.set_xlabel('Eruption time in mins') ax2.set_ylabel('Waiting time to next eruption') ax2.set_title('Visualization of clustered data', y=1.02) ax2.set_aspect('equal') plt.tight_layout() plt.suptitle(f'Silhouette analysis using k = {k}', fontsize=16, fontweight='semibold', y=1.05);","link":"/post/unsupervised-learning/"},{"title":"Model Validations and Performance Evaluators","text":"OverviewVery often, people came up with various models with completely different underlying logics. In order to do reasonable comparisons among them, we must perform proper evaluations of the model performances and validate these models' efficacy via some datasets. This is where cross-validation comes in and performance metrics become extremely important. In this blog, I will first discuss about various techniques applied in cross-validation, followed by some analysis of commonly used metrics/scores Cross validationWhen we try to train a model, we require a training set and a test set. Very often, we need to split the training set itself so that we can use part of it as validation set: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set. Unfortunately, this would be too wasteful of the dataset we have. To fully utilize our datasets, we need cross-validation, which can split our datasets in to folds, and we pick some folds as the choice for validation set. After several iterations ran with different choices, we take an average of the evaluation scores as the final performance score for the model. The above strategy is often known as k-fold CV. It is computationally expensive, but it makes good use of the entire training set. Cross Validation MethodsBased on the choice of folds we pick, we may end up with different CV methods. Below is a list of methods we can use and their intuitions: Fixed index partition K-Folds: partition the into k equal-sized subsets. choose 1 set from all subsets as the validation set. Run the model obtained only on this validation once. Repeated K-Folds: run K-Folds n times, producing different splits and validation sets in each repetition. Fixed validation set size Leave One Out: Choose only one entry as the validation set Leave P Out: Choose only entries as the validation set Fixed label ratio Stratified K-Folds: Each set contains approximately the same percentage of samples of each target class as the complete set. Random index partition Suffle &amp; Split K-Folds: If we randomly pick entries in to get the resultant k partitioned subsets, we are equivalently doing shuffling before the partition. All these data partition methods are very intuitive and can be found in sci-kit learn. Here I'll just give a demo of how code should be run for CV. 12345from sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import ShuffleSplitclf = svm.SVC(kernel='linear', C=1, random_state=42) # Some model herecv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0) # You can specify the partition method here scores = cross_val_score(clf, X, y, cv=cv) # clf - model; X - training set; y - test set Performance MetricsNow let's move on to the next section on the metrics applied to evaluate the machine learning algorithms. There are metrics used for different tasks: Supervised Classification Supervised Regression Unsupervised Clusetering In this blog post, we will consider mainly on classification models, as the metrics for the remaining two task will be directly applied in these tasks to improve their model performances. We will leave it to interested readers to read my posts on models for these two tasks to explore the related metrics. ClassificationNote: Sometimes we have multiclass and multilabel problems instead of binary classifications. The answer from this post gives clear definitions for both problems: Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. Multilabel classification assigns to each sample a set of target labels. This can be thought of as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these. In the following discussions, I'll use [m-c] and [m-l] as labels to denote if the metric is applicable to Multiclass classification and Multilabel classification respectively. 1. Accuracy Score [m-l]This is the ratio of number of correct predictions to the total number of input samples It works well only if there are equal number of samples belonging to each class. Otherwise, it may lead to over/under-estimation of the model's true performance due to imbalanced data. 2. Balanced accuracy score [m-c]To address the problem above, we can introduce this score, which averages over recall scores per class where each sample is weighted according to the inverse prevalence of its true class. This formulation ensures that the score won't be too high just because one class's accuracy is high 3. Top-k accuracy score [m-l] [m-c]A generalization of accuracy score: prediction is considered correct as long as the true label is associated with one of the k highest predicted scores. 4. Confusion Matrix [m-c]The matrix which describes the complete performance of the model. This gives a good intuition over how well the classification is done. We can look at a sample pic here.1 A sample confusion matrix Based on the counts of corrected labeled and wrongly labeled samples, we can compute the following terms (say with regard to label 'sentosa', YES = is 'sentosa', NO = not 'sentosa') True Positives (TP) : The cases in which we predicted YES and the actual output was also YES. True Negatives (TN) : The cases in which we predicted NO and the actual output was NO. False Positives (FP) : The cases in which we predicted YES and the actual output was NO. Equivalently Type-I error. False Negatives (FN) : The cases in which we predicted NO and the actual output was YES. Equivalently Type-II error. These four terms are crucial as they help to compute the following terms 1: Precision: , intuitively the ability of the classifier not to label as positive a sample that is negative. It focuses on Type-I error. Recall: , intuitively the ability of the classifier to find all the positive samples. It focuses on Type-II error. F1-score: , a balanecd value (harmonic mean) of the precision and recall. We can generalize it to F-beta score, which allows more variated balanced between precision and recall: 5. Jaccard Similarity [m-l] [m-c]The jaccard_score function computes the average of Jaccard similarity coefficients, also called the Jaccard index, between pairs of label sets. 6. Hinge Loss [m-c]It only considers prediction errors. It is widely used in maximal margin classifiers such as support vector machines.Suppose the label is in , output label is , true label is , then we have 7. Receiver operating characteristic (ROC) and Area Under Curve(AUC)ROC is a plot of True Positive Rate (TPR)/Recall against False Positive Rates (FPR): A sample image looks like this 1: The area under the ROC curve is know as AUC, and it equals the probability that a randomly chosen positive example ranks above (is deemed to have a higher probability of being positive than negative) a randomly chosen negative example. The greater the value, the better is the performance.Unfortunately, ROC curves aren't a good choice when your problem has a huge class imbalance.","link":"/post/validation/"},{"title":"Variational Autoencoder (VAE)","text":"A short Intro to VAE1. BackgroundThere mainly 2 types of deep generative models: Generative Adversarial Network (GAN) Variational Autoencoder (VAE) We will discuss about VAE in this blog. In future blogs, we will venture into the details of GAN. 2. A basic intuitionA VAE is an autoencoder whose encodings distribution is regularised (via variational inferenece) during the training in order to ensure that its latent space has good properties allowing us to generate some new data. 3. Encoder &amp; Decoderencoder is an agent that transform older featurer representation to a new set of feature representation (usually a lower dimension) using selection or extraction and decoder is an agent producing the reverse process. The encoded representation can span a feature space of certain dimensionality. We call it latent space. Furthermore, we know that certain properties/information of original features may be lost if we encode them. So we categorize transformations as lossy or lossless transformation. we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. 4. AutoencoderAutoencoder is done by setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks. We usually use a Mean Square Error as the loss function for backpropagation. This is often compared with PCA. When the structure of encoder/decoder gets deeper and more non-linear, we observe that autoencoder can still proceed to a high dimensionality reduction while keeping reconstruction loss low. There are mainly 2 drawbacks of Autoencoder: the lack of interpretable and exploitable structures in the latent space (lack of regularity) Difficulty in reducing a large number of dimensions while keeping the major part of the data structure information in the reduced representations As a result of the above two drawbacks, we may generate meaningless data if we simply encode into a latent space and sample random points from it to decode (a generative model often requires this). This issue leads to the need of regularization for the latent's space distribution. This is the motivation for Variational Autoencoder. VAE in detailThe key idea for VAE is that, instead of encoding an input as a single point, we encode it as a distribution over the latent space. In essence, we don't have point-wise estimation, but an estimation of original inputs' distribution (hence Bayesian inference is here of significant help). Next the evalution of an error is based on a new sample drawn from the distribution estimator and compared with original sample. (In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. See why this is the case in mathematical details below) Because of regularization, we now have an additional term inside loss, which is the KL divergence. We see that the KL divergence between 2 Gaussians has a closed form and hence can be computed easily. 1. All the math down hereWe require two assumptions: a latent representation is sampled from the prior distribution ; the data is sampled from the conditional likelihood distribution Now we note here that the “probabilistic decoder” is naturally defined by , that describes the distribution of the decoded variable given the encoded one, whereas the “probabilistic encoder” is defined by , that describes the distribution of the encoded variable given the decoded one. These two expressions remind us easily of the Bayes Rule, which we have . We assume and . Since f is arbitrary, the evidence term is often an intractable integral. Hence we need Variational Inference (VI) to help us approximate directly via a easily tractable distribution (often a Gaussian distribution ). We define where are parametrized functions. With uncertainty in , we aim to obtain optimal as 2. Practical idea: Neural NetworkNow that we have an optimization problem which may be solved using NN, we still need to address a few issues. First, the entire space of is too large, so we need to constrain the optimisation domain and decide to express f, g and h as neural networks. In practice, g and h are not defined by two completely independent networks but share a part of their architecture and their weights (with dependent paramters) For simplicity of computation, we often require to be a multidimensional Gaussian distribution with diagonal covariance matrix. With this assumption, h(x) is simply the vector of the diagonal elements of the covariance matrix and has then the same size as g(x). On the other hand NN models , which represents the mean of assumed as a Gaussian with fixed covariance. Using these ideas, we can first sample and evaluate an followed by error evaluation and backpropagation. Note here that the sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called reparametrisation trick, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture. This is done by producing a concrete sample from . So we have a Monte-carlo estimation via sampling to replace the expectation term in the loss term (1). We then backpropagate error to obtain the final result. Code implementationTo Be Updated","link":"/post/variational-autoencode/"},{"title":"Variational Inference","text":"Introduction1. Background of Bayesian methodsIn the field of machine learning, most would agree that frequentist approaches played a critical role in the development of early classical models. Nevertheless, we are witnessing the increasing significance of Bayesian methods in modern study of machine learning and data modelling. The simple-looking Bayes' rule has inspired a lot wonderful models in areas like topic modelling, representation learning and hyperparameter optimization. In these models, the latent variables are the focus of the study. By analysing several data on the observed variables , we hope to get some meaningful information (for example, a point estimate or an entire distribution) about these latent variables. 2. Problem with Bayesian methods: intractable integralWhile the rule looks easily understandable, the numerical computation is hard in reality. One major issue is the intractable integral we need to compute in order to get the , which is often called the \"model evidence\". This is often because the search space for is combinatorially too large, making the computation extremely expensive. A common approach to deal with this problem is to approximate the posterior probability directly. Some popular choices include Monte Carlo Sampling methods and variational inference. In this report, we will introduce the variational methods, which are perhaps the most widely used inference technique in machine learning. We will analyse a particularly famous technique in variational methods, mean-field variational inference. 3. Main idea of variational inferenceIn variational inference, we can avoid computing the intractable integral by magically modelling the posterior directly. The main trick here is to approximate the unknown distribution with some similar distribution q. Since we can choose the q to belong to a certain family of distribution (hence tractable), the problem is now transformed into an optimization problem about the parameters of . Understanding Variational Bayesian methodIn this section, we demonstrate the theory behind variational Bayesian methods. 1. Kullback-Leibler DivergenceAs mentioned above, variational inference needs a distribution to approximate the posterior distribution . Therefore we need to gauge how well a candidate approximates the posterior. A common measure is Kullback-Leibler Divergence (often called KL divergence). KL divergence is defined as Where means the expected value with respect to distribution . The formula can be interpreted as follows: if is low, the divergence is generally low. if is high and is high, the divergence is low. if is high and is low, the divergence is high, hence the approximation is not ideal. Take note of the following about use of KL divergence in Variational Bayes: KL divergence is not symmetric, it's easy to see from the formula that as the approximation distribution is usually different from the target distribution . In general, we focus on approximating some regions of as good as possible (Figure 1 (a)). It is not necessary for the to nicely approximate every part of As a result (usually called forward divergence) is not ideal. Because for some regions which we don't want to care, if , the KL divergence will be very large, forcing to take a different form even if it fits well with other regions of (refer to Figure 1(b)). On the other hand, (usually called reverse KL divergence) has the nice property that only regions where requires and to be similar. Consequently, reverse KL divergence is more commonly used in Variational Inference. 2. Evidence lower boundUsually we don't directly minimizing KL divergence to obtain a good approximated distribution. This is because computing divergence still depends on the posterior . The computation involves the \"evidence\" term which is expensive to compute, as shown in the formula below: The approximation using reverse KL divergence usually gives good empirical results, even though some regions of may be compromised Figure 1: Reverse KL vs Forward KL divergence: The left image has a better approximation on part of . (a) Reverse KL simulation (b) Forward KL simulation We can directly conclude by the fact that the term is than the log of evidence. We can also proof this result using Jensen's inequality as follows: By the definition of marginal probability, we have , take log on both side we have: The last 2 lines follow from Jensen's Inequality which states that for a convex function , we have This term is known as the Evidence Lower Bound, or Since log does not depend on , we can treat it as a constant from the perspective of optimizing . Hence, minimizing is now equivalent to maximizing General procedureIn general, a variational inference starts with a family of variational distribution (such as the mean-field family described below) as the candidate for . We can then use the manually chosen to compute the ELBO terms and . Afterwards, we optimize the parameters in to maximize the ELBO value using some optimization techniques (such as coordinate ascent and gradient methods). Mean Field Variational Family1. The \"Mean Field\" AssumptionsAs shown above, the particular variational distribution family we use to approximate the posterior is chosen by ourselves. A popular choice is called the mean-field variational family. This family of distribution assumes joint approximation distribution to be factorized over some partition of the latent variables. This implies mutual independence among the n fractions in the partition. In particular: we have where is factorized into . For simplicity, we assume that each fraction only contains 1 latent variable , it is often referred as \"naive mean field\". This family is nice to analyse because we can model each distribution with a tractable distribution based on the problem set-up. Do note that a limitation of this family is that we cannot easily capture the interdependence among the latent variables. 2. Derivation of optimal Now in order to derive the the optimal form of distribution for and thus the overall , we need to go back to the ELBO optimization with this mean-field family assumption. Recall the formula for ELBO (we use here as it is the convention): We express this formula in terms of as using functional integral (see appendix A): With this new expression, we can consider maximizing with respect to each of the . The optimal form of is the one which maximizes , that is: We take the derivative with respect to using Lagrange multipliers and set to 0 yields: where is a normalization constant that plays minimal role in the variable update. The funtional derivative of this expression actually requires some knowledge about calculus of variations, specifically Euler-Lagrange equation. 3. Variable update with Coordinate AscentFrom equation we found that . Therefore iterative optimization algorithms like Coordinate Ascent can be applied to update the latent variables to reach their optimal form. Note that all the 's are interdependent during the update, hence in each iteration, we need to update all the 's. As short description for the coordinate ascent in this setup will be: Compute values (if any) that can be directly obtained from data and constants Initialize a particular to an arbitrary value Update each variable with the step function Repeat step 3 until the convergence of ELBO A more detailed example of coordinate ascent will be shown in next section with the univariate gaussian distribution example. A point to take note that in general, we cannot guarantee the convexity of ELBO function. Hence, the convergence is usually to a local maximum. Example with Univariate GaussianWe demonstrate the mean-field variational inference with a simple case of observations from univariate Gaussian model. We first assume there are observations from a Gaussian distribution satisfying: Here is inverse of variance (hence one-to-one correspondence). From the derivation of we know we need to compute the log joint probability . We will first derive an explicit formula for it by expanding the join probability into conditional probability: where is a constant. Note that sometimes some latent variable has higher priority that others. The choice of this variable depends on the exact question in hand. 1. Compute independent and Next, we apply approximation via . By the mean-field assumption, we have . We proceed to find the optimal form of and : Compute the expression for : Note that here is a shortcut representation for , and all are constant terms not involved in the optimization update. From the expression above, it's easy to observe that follows a Gaussian distribution with , where: Compute the expression for A closer look at the result suggest that follows a Gaussian distribution with Gamma , where: 2. Variable update until ELBO convergence}Now that we have and , we only need to update their parameters: Using the updated and , we can then compute with Hence the coordinate ascent algorithm can be applied here: Compute and as they can be derived directly from the data and constants based on their formula Initialize to some random value Update with current values of and Update with current values of and Compute ELBO value with the variables &amp; updated with the parameters in step 1 - 4 Repeat the last 3 steps until ELBO value doesn't vary by much As a result of the algorithm, we obtain an approximation for the posterior distribution of and given observations . Extension and Further resultIn this section, we briefly outline some more theory and reflection about general variational Bayesian methods. Due to space limitations, we only provide a short discussion on each of these. Exponential family distributions in Varational InferenceA nice property of the exponential family distribution is the presence of conjugate priors in closed forms. This allows for less computationally intensive approaches when approximating posterior distributions (due to reasons like simpler optimization algorithm applicable and better analytical forms). Further more, Gharamani &amp; Beal even suggested in 2000 that if all the belong to the same exponential family, the update of latent variables in the optimization procedure can be exact. A great achievement in the field of variational inference is the generalized update formula for Exponentialfamily-conditional models. These models has conditional densities that are in exponential family. The nice property of exponential family leads to an amazing result that the optimal approximation form for posteriors are in the same exponential family as the conditional. This has benefits a lot of well-known models like Markov random field and Factorial Hidden Markov Model. Comparison to other Inference methodsThe ultimate results of variational inference are the approximation for the entire posterior distribution about the parameters and variables in the target problem with some observations instead of just a single point estimate. This serves the purpose of further statistical study of these latent variables, even if their true distributions are analytically intractable. Another group of inference methods commonly used to achieve the similar aim is Markov chain Monte Carlo (MCMC) methods like Gibbs sampling, which seeks to produce reliable resampling of given observations that help to approximate latent variables well. Another common Bayesian method that has a similar iterative variable update procedure is Expectation Maximization (EM). For EM, however, only point estimates of posterior distribution are obtained. The estimates are \"Expectation maximizing\" points, which means any information about the distribution around these points (or the parameters they estimate) are not preserved. On the other hand, despite the advantage of \"entire distribution\" Variational inference has, its point estimates are often derived just by the mean value of the approximated distributions. Such point estimates are often less significant compared to those derived using EM, as the optimum is not directly achieved from the Bayesian network itself, but the optimal distributions inferred from the network. Popular algorithms applying variational inferenceThe popularity of variational inference has grown to even surpass the classical MCMC methods in recent years. It is particularly successful in generative modeling as a replacement for Gibbs sampling. The methods often show better empirical result than Gibbs sampling, and are thus more well-adopted. We here showcase some popular machine learning models and even deep learning models that heavily rely on variational inference methods and achieved great success: Latent Dirichlet Allocation: With the underlying Dirichlet distribution, the model applies both variational method (for latent variable distribution) and EM algorithm to obtain an optimal topic separation and categorization. variational autoencoder: The latent Gaussian space (a representation for the input with all the latent variables and parameters) is derived from observations, and fine-tuned to generate some convincing counterparts (a copy for instance) of the input. These models often rely on a mixture of statistical learning theories, but variational inference is definitely one of the key function within them.","link":"/post/variational-inference/"}],"tags":[{"name":"Statistics","slug":"Statistics","link":"/tags/Statistics/"},{"name":"A&#x2F;B testing","slug":"A-B-testing","link":"/tags/A-B-testing/"},{"name":"Data Mining&#x2F;Data Engineering","slug":"Data-Mining-Data-Engineering","link":"/tags/Data-Mining-Data-Engineering/"},{"name":"Big Data","slug":"Big-Data","link":"/tags/Big-Data/"},{"name":"Distributed System","slug":"Distributed-System","link":"/tags/Distributed-System/"},{"name":"Cloud Computing","slug":"Cloud-Computing","link":"/tags/Cloud-Computing/"},{"name":"Software Engineering","slug":"Software-Engineering","link":"/tags/Software-Engineering/"},{"name":"Database System","slug":"Database-System","link":"/tags/Database-System/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Clustering","slug":"Clustering","link":"/tags/Clustering/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","link":"/tags/Unsupervised-Learning/"},{"name":"Data Analytics","slug":"Data-Analytics","link":"/tags/Data-Analytics/"},{"name":"Matrix Computation","slug":"Matrix-Computation","link":"/tags/Matrix-Computation/"},{"name":"Dimensionality Reduction","slug":"Dimensionality-Reduction","link":"/tags/Dimensionality-Reduction/"},{"name":"Ensemble","slug":"Ensemble","link":"/tags/Ensemble/"},{"name":"Boosting","slug":"Boosting","link":"/tags/Boosting/"},{"name":"Supervised Learning","slug":"Supervised-Learning","link":"/tags/Supervised-Learning/"},{"name":"Bagging","slug":"Bagging","link":"/tags/Bagging/"},{"name":"Random Forest","slug":"Random-Forest","link":"/tags/Random-Forest/"},{"name":"Gradient Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Hidden Markov Models","slug":"Hidden-Markov-Models","link":"/tags/Hidden-Markov-Models/"},{"name":"Dynamic Programming","slug":"Dynamic-Programming","link":"/tags/Dynamic-Programming/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"front-end","slug":"front-end","link":"/tags/front-end/"},{"name":"blog-building","slug":"blog-building","link":"/tags/blog-building/"},{"name":"Recommender Systems","slug":"Recommender-Systems","link":"/tags/Recommender-Systems/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Deep learning","slug":"Deep-learning","link":"/tags/Deep-learning/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Regularization","slug":"Regularization","link":"/tags/Regularization/"},{"name":"Control Theory","slug":"Control-Theory","link":"/tags/Control-Theory/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Evaluation Methods","slug":"Evaluation-Methods","link":"/tags/Evaluation-Methods/"},{"name":"Statistical Inference","slug":"Statistical-Inference","link":"/tags/Statistical-Inference/"},{"name":"Representaiton Learning","slug":"Representaiton-Learning","link":"/tags/Representaiton-Learning/"},{"name":"Bayesian Statistics","slug":"Bayesian-Statistics","link":"/tags/Bayesian-Statistics/"}],"categories":[{"name":"Blogs","slug":"Blogs","link":"/categories/Blogs/"},{"name":"Anime","slug":"Anime","link":"/categories/Anime/"},{"name":"Enlightenment","slug":"Enlightenment","link":"/categories/Enlightenment/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"}],"pages":[{"title":"404","text":"I don’t know how you ended up here, but you have jumped over the edge of this blog. Maybe it’s the end of the internet and you can power off your machine now… /@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@ @@@@@@@@@ ,@@@@@ @@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@ ,@@@@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@@@ @@@@@@@@@@@@ @@@@@@@@@ (@@@@@@@@@@@@ #@@@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@, @@@@@@@@@. @@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@ @@@@@@@@@@ @@@@@@@@@ @@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@@@ @@@@@@@@@@@@@ @@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@ … or you climb back and read one of my recent posts :)","link":"/404.html"},{"title":"About Me","text":"Who am IMy name is Zhenlin Wang. I'm from Ningbo, China. Currently I'm a master student in the Machine Learning Department in Carnegie Mellon University. My academic interest and work passion lie across the fields of Data Science, Machine Learning/AI and Software Engineering (though they are somewhat connected under the field of Computer Science…). The path to my passion Before university, I was quite obsessed with applying mathematical models to the financial industry. I dreamed of being an equity researcher and predicting flows in the stock and derivatives market. In pursuit of this goal, I read an entire textbook on corporate finance and written all kinds of note on accounting models. However, I was wrong. When I entered the equity research team in the investment club in my university, I quickly realized that people were not \"relying\" on the models for judgement. They \"abuse\" these models to fit their assumptions in order to sell their investment plans to customers. This is not what I wanted. So I tried an alternative path - quantitative research. Being an applied math student, it\\'s fairly easy to imagine that I would be fascinated by the significant amount of math used in quantitative research team in the investment club. This stood up as a completely different track from equity research. I was learning so much data science and machine learning knowledge in this team. I knew I\\'ve found my true passion. In the following semester, I enrolled in a second major in Computer Science to learn more about fundamental CS knowledge. I started building up my statistics/data science skills. A youtube channel, StatQuest with Josh Starmer really helped me a lot. Until today, I\\'m glad to see it being supported by the community and is still diligently uploding new tutorials on various concepts in stats/DS/ML. Further down the path, I started exploring boundaries in some ML sub-fields, namly Bayesian Optimization, Bandit and Reinforcement Learning. I conducted several researches under the guidance of my supervisors, and luckily published some papers. I believe in the motto of Learn to live, Live to learn. No matter how old I am and where I am, I\\'ll keep myself challenged with learning new things. At current stage, expanding my passion from theoretical knowledge to real-life application is my major goal. Solving real-world problems with ML breakthroughs require the careful construction of tools and technologies. Thus, learning and practice software engineering has also become part of my life nowadays. Looking backI received my high school and undergradute education in Singapore. It is a second hometown to me. I have no words but only genuine gratitude and respect for this city state. In Singapore, I've met lots of excellent people, learnt countless lessons, and found my true love . My colorful youth life remains in Singapore forever (芳华永驻此狮城~). Leisure timeI listen and play musics sometimes. I used to play violin, but nowadays, I prefer harmonica as I can carry it anywhere easily. I go workout regularly. Since Pittsburgh has lots of tennis court, I'm planning on playing tennis with my friends as well. I love eating Thai food and Japanese food, my favorites are Pat Thai, Basil Chicken, Sushi/sashimi and Suki Yaki. An ACG FanI'm a Japanese culture lover, specifically it's ACG culture and it's history. I read manga, watch anime and used to play some galgame. I used to be quite into the anime merchandise (an otaku) but now I just treat it as an interest and watch some old animes for chilling, I'll casually write some reviews on animes I watched again recently. My favorite anime is Steins;Gate. It tells a story of a man leaping through time challenging destiny using a time machine. My favicon is also a tribute to this series. The perseverence of the main protagonist, Okabe Rintarou, motivated me to keep on despite various setbacks from time to time. Steins Gate - Okabe Rintarou This is the end this self-introduction (you call this an intro???), have fun browsing through my blogs. El Psy Congroo!","link":"/intro/index.html"},{"title":"Publication","text":"Best Arm Identification with Safety Constraints. Zhenlin Wang, Andrew Wagenmaker, Kevin Jamieson.The 25th International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.[arXiv] Max-min Grouped Bandits. Zhenlin Wang, Jonathan Scarlett.Association for the Advancement of Artificial Intelligence (AAAI), 2022.[arXiv]","link":"/publication/index.html"}]}