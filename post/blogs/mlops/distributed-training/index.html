<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Understanding Distributed Training in Deep Learning - Criss Wang&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Criss Wang&#039;s Log Book"><meta name="msapplication-TileImage" content="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Criss Wang&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="What&amp;#39;s important in industry AND research, not taught in school???"><meta property="og:type" content="blog"><meta property="og:title" content="Understanding Distributed Training in Deep Learning"><meta property="og:url" content="https://criss-wang.github.io/post/blogs/mlops/distributed-training/"><meta property="og:site_name" content="Criss Wang&#039;s Log Book"><meta property="og:description" content="What&amp;#39;s important in industry AND research, not taught in school???"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://www.mishalaskin.com/_next/image?url=/images/pipeline_parallel.png&amp;w=3840&amp;q=75"><meta property="article:published_time" content="2024-03-04T05:00:00.000Z"><meta property="article:modified_time" content="2024-03-24T06:16:07.720Z"><meta property="article:author" content="Zhenlin Wang"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Distributed Systems"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://www.mishalaskin.com/_next/image?url=/images/pipeline_parallel.png&amp;w=3840&amp;q=75"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://criss-wang.github.io/post/blogs/mlops/distributed-training/"},"headline":"Understanding Distributed Training in Deep Learning","image":[],"datePublished":"2024-03-04T05:00:00.000Z","dateModified":"2024-03-24T06:16:07.720Z","author":{"@type":"Person","name":"Zhenlin Wang"},"publisher":{"@type":"Organization","name":"Criss Wang's Log Book","logo":{"@type":"ImageObject","url":"https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"}},"description":"What&#39;s important in industry AND research, not taught in school???"}</script><link rel="canonical" href="https://criss-wang.github.io/post/blogs/mlops/distributed-training/"><link rel="icon" href="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto Slab:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp" alt="Criss Wang&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" style="font-weight: bold" href="/">Criss&#039;s Time Machine</a><a class="navbar-item" style="font-weight: bold" href="/categories/Blogs">Machine Learning</a><a class="navbar-item" style="font-weight: bold" href="/categories/Software">Software Engineering</a><a class="navbar-item" style="font-weight: bold" href="/categories/Projects">Projects</a><a class="navbar-item" style="font-weight: bold" href="/research">Research</a><a class="navbar-item" style="font-weight: bold" href="/archives">Archives</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Criss-Wang/"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-4 is-size-5-mobile has-text-weight-normal">Understanding Distributed Training in Deep Learning</h1><div class="article-meta is-size-7 level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-check"> </i><time dateTime="2024-03-04T05:00:00.000Z" title="2024-03-04T05:00:00.000Z">2024-03-04</time></span><span class="level-item"><i class="far fa-folder-open"> </i><a class="link-muted" href="/categories/Blogs/">Blogs</a></span></div></div><div><hr style="background-color:grey"></div><div style="padding-bottom:5px"></div><div class="content"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Since last year, the quest for large X models have been nonstop, and people kept exploring the possibility to build more universal and robust models. While some still put a doubt if models with more parameters will be effective, most have faith in the <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.08361.pdf">scaling law</a> proposed by DeepMind and OpenAI researchers. The progress in 1 year is promising, as it seems that we are steadily moving towards the era of AGI. However, the education barely follows. College and Unversity are still bound by the budget to enable students to get in touch to large model training, especially when it comes to multi-gpu / multi-node distributed training. In light of this, I would love to share what I understand about distributed training, and how can we get started in this domain to catch up with recent industrial progress.</p>
<h2 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h2><ul>
<li>Leverages multiple compute resources—often across multiple nodes or GPUs—simultaneously, accelerating the model training process.</li>
<li>Mainly a form of parallelism, requires some understanding of low-level operation system (memory, communication and GPU architecture)</li>
<li>For those interested, I will recommend taking <a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~15418/index.html">CMU 15-418 Parallel Computer Architecture and Programming</a> to get an in-depth understanding.</li>
</ul>
<h2 id="2-Parallelism-in-Training"><a href="#2-Parallelism-in-Training" class="headerlink" title="2. Parallelism in Training"></a>2. Parallelism in Training</h2><ul>
<li><p>Two primary forms of parallelism: <strong>model parallelism</strong> and <strong>data parallelism</strong></p>
</li>
<li><p><strong>Model Parallelism</strong>:</p>
<ul>
<li>Used when a model doesn’t fit into the memory of a single device.</li>
<li>Different parts of the model are placed on different devices, enabling the training process to occur across multiple GPUs or nodes. This approach is particularly useful for exceptionally large models.</li>
</ul>
</li>
<li><p><strong>Data Parallelism</strong>:</p>
<ul>
<li>Split the dataset across various devices, with each processing a unique subset of the data.</li>
<li>The model’s parameters are then updated based on the collective gradients computed from these subsets (with different strategies).</li>
</ul>
</li>
</ul>
<h2 id="3-Strategies-in-detail"><a href="#3-Strategies-in-detail" class="headerlink" title="3. Strategies in detail"></a>3. Strategies in detail</h2><p><strong>[Note]</strong>: I'll mainly use PyTorch in this blog as it is the most popular and convenient choice. It is mainly based on <code>torch.distributed</code> package. In the meantime, some convenient scripts are created by <em>Lightning AI</em> with their own libraries. I'll show some code using their library for people who just want a shortcut and get rid of the details behind distributed training.</p>
<ol>
<li><p>Data Parallelism</p>
<ul>
<li>How <code>DistributedDataParallel</code> works:<ul>
<li>NCCL: multi-GPU, multi-node <strong>communication</strong> primitives. all-gather, all-reduce, broadcast, reduce-scatter, reduce routines, point-to-point send/receive. High bandwidth, low latency on <strong>PCIe</strong> and <strong>NVLink</strong> interconnects</li>
<li>All GPUs share same initial weights. Aggregate all gradients in different GPUs and update the weight collectively.</li>
<li>Need to update optimizer state and weights after AllReduce.</li>
</ul>
</li>
<li>DDP Implementation</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### DDP - PyTorch Version</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">   <span class="comment"># Initialize distributed environment</span></span><br><span class="line">   dist.init_process_group(backend=<span class="string">'nccl'</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># Create model</span></span><br><span class="line">   model = YourModel()</span><br><span class="line">   model = DDP(model)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># Load data and distribute it across processes</span></span><br><span class="line">   train_loader = DistributedSampler(YourDataset())</span><br><span class="line"></span><br><span class="line">   <span class="comment"># Training loop</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">      <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = YourLoss(outputs, labels)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward and optimize</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br></pre></td></tr></table></figure>

<ul>
<li><p>For more advanced details like <strong>RPC-Based Distributed Training (RPC)</strong> and <strong>Collective Communication (c10d)</strong>, refer to <code>torch.distributed</code> <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/dist_overview.html">original docs</a></p>
</li>
<li><p>Fully Sharded DP (FSDP)</p>
<ol>
<li>What is in the GPU memory (x params, FP16)<ol>
<li>Params: 2x (fp16 with 2 bytes)</li>
<li>Gradients: 2x</li>
<li>Optimizer (AdamW)<ul>
<li>Param copy: 4x (float32)</li>
<li>Momentum: 4x</li>
<li>Variance: 4x</li>
</ul>
</li>
</ol>
</li>
<li>How FSDP works<ol>
<li>FSDP unit (vertical splitting), can be:<ul>
<li>A layer splitted</li>
<li>A stage splitted</li>
<li>A group of layers splitted</li>
</ul>
</li>
<li>Sharding<ul>
<li>Storing the FSDP unit on <code>FlatParameter</code></li>
<li>Split <code>FlatParameter</code> on multiple nodes (after zero padding for divisible property)</li>
</ul>
</li>
<li>All-Gather<ul>
<li>performed by NCCL</li>
<li>gather all parts and sync across all nodes</li>
<li>Done before both forward and backwards</li>
<li>discard peer parts after forward/backward</li>
</ul>
</li>
<li>Reduce-scatter<ul>
<li>performed via NCCL</li>
<li>Each node gets part of the result of gradient (backward only)</li>
<li>Note that All-Reduce is not used coz it broadcast same results to all nodes</li>
<li>E.g. Each node <code>i</code> has all gradients <code>G_i1, G_i2, ..., G_in</code>, after reduce-scatter, each node will have gradient redistributed, with node <code>i</code> getting <code>sum of G_ki</code>, where k spans from 1 to n</li>
</ul>
</li>
</ol>
</li>
<li>Reason to use/not to use FSDP<ol>
<li>When to use<ul>
<li>Model size is too large (not data size)</li>
<li>More communication between GPUs</li>
<li>Hence trade memory for speed: more GPU memory cost due to communication, however, communication overhead reduced via NCCL acceleration</li>
<li>If want to trade speed for memory, see <strong>activation checkpointing</strong></li>
</ul>
</li>
<li>When not to use<ul>
<li>For models &lt; 100 million params, consider activation-checkpointing and reversible layers</li>
<li>Recommend to use BFloat16 instead of Float16 (Float16 requires ShardedGradScaler)</li>
<li>Mixed Precision Training Concern (Package compatibility)</li>
</ul>
</li>
</ol>
</li>
<li>FSDP Implementation</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### FSDP Version</span></span><br><span class="line"><span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> (</span><br><span class="line">   FullyShardedDataParallel,</span><br><span class="line">   CPUOffload,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> torch.distributed.fsdp.wrap <span class="keyword">import</span> (</span><br><span class="line">   default_auto_wrap_policy,</span><br><span class="line">   enable_wrap,</span><br><span class="line">   wrap</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">model</span>(nn.Module):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.layer1 = nn.Linear(<span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">      self.layer2 = nn.Linear(<span class="number">4</span>, <span class="number">16</span>)</span><br><span class="line">      self.layer3 = nn.Linear(<span class="number">16</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">model = DistributedDataParallel(model())</span><br><span class="line">fsdp_model = FullyShardedDataParallel(</span><br><span class="line">   model(),</span><br><span class="line">   fsdp_auto_wrap_policy=default_auto_wrap_policy,</span><br><span class="line">   cpu_offload=CPUOffload(offload_params=<span class="literal">True</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom wrap</span></span><br><span class="line">wrapper_kwargs = <span class="type">Dict</span>(cpu_offload=CPUOffload(offload_params=<span class="literal">True</span>))</span><br><span class="line"><span class="keyword">with</span> enable_wrap(wrapper_cls=FullyShardedDataParallel, **wrapper_kwargs):</span><br><span class="line">   fsdp_model = wrap(model())</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Model Parallelism</p>
<ul>
<li>split horizontally</li>
<li>Implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">model_parallel</span>(nn.Module):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.layer_1 = nn.Sequential(...)</span><br><span class="line">      self.layer_2 = nn.Sequential(...)</span><br><span class="line">      self.layer_1.cuda(<span class="number">0</span>)</span><br><span class="line">      self.layer_2.cude(<span class="number">1</span>)</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      x = x.cuda(<span class="number">0</span>)</span><br><span class="line">      x = self.layer_1(x)</span><br><span class="line">      x = x.cuda(<span class="number">1</span>)</span><br><span class="line">      x = self.layer_2(x)</span><br><span class="line">      x = ...</span><br><span class="line">      <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
<li>Inefficient sometimes: in the code above, GPU may be idle if layer 2 is not run during training</li>
<li>Does not work well if the model architecture does not naturally lend itself to being divided into parallelizable segments.</li>
</ul>
</li>
<li><p>Pipeline Parallelism<br><img src="https://www.mishalaskin.com/_next/image?url=/images/pipeline_parallel.png&w=3840&q=75"></p>
<ul>
<li>Mixed data and model parallelism, involves scheduling of data flow</li>
<li>Split into multiple stages, and each stage is assigned to a different device</li>
<li>The output of one stage is fed as input to the next stage.</li>
<li>Sometimes inefficient and suffers from idle time when machines are waiting for other machines to finish their stages: pipeline is waiting for a stage to finish in both the forward and backward pass, the period when some machine are idle aer referred to as a <em>bubble</em>.</li>
</ul>
</li>
<li><p>Tensor parallelism</p>
<ul>
<li>Split vertically + horizontally (in units of a tensor)</li>
<li>Can be more effective as it leverages efficiencies within matrix multiplication by spliting a tensor up into smaller fractions and expedite the computation</li>
<li>The detail can be expanded into another blog, however, I will refer you to this <a target="_blank" rel="noopener" href="https://www.mishalaskin.com/posts/tensor_parallel">excellent blog</a> instead of reinventing the wheel myself again.</li>
<li>Might require models specifically designed to take advantage of this form of parallelism. It may not be as universally applicable as data or model parallelism.</li>
</ul>
</li>
<li><p><code>torchrun</code></p>
<ul>
<li>An elegant way to run distributed training using <code>torch.distributed</code> package. Please refer to details <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/elastic/run.html">here</a></li>
<li>Make use of rendezvous backend to achieve high availability and failure recovery</li>
<li>A few major advantages include:<ul>
<li>Single-node multi-worker</li>
<li>Multi-node</li>
<li>Multi-GPU</li>
<li>Fault tolerant</li>
<li>Elastic</li>
</ul>
</li>
</ul>
</li>
<li><p>Distributed Training on the Cloud</p>
<ul>
<li>Since most of the resources are available from the cloud, and they are on-demand, it is common practice to migrate local code to be run on remote servers. You can spin up GPU resources (usually more capable than your local version) yourself and manage the dependencies/monitoring independenly, or you can resort to integrated solutions like AWS SageMaker or Azure ML or Google AI Studio as they often provide convenient API endpoints to interact with those GPU instances. In many scenarios, their management include inter-gpu/inter-node communication as well, which is a big plus.</li>
<li>As an example, you can setup AWS accordingly and run your distributed training using SageMaker via <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html">this tutorial</a></li>
<li>A sample script is as follows:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sagemaker.pytorch <span class="keyword">import</span> PyTorch</span><br><span class="line"></span><br><span class="line">estimator = PyTorch(</span><br><span class="line">   ...,</span><br><span class="line">   instance_count=<span class="number">2</span>,</span><br><span class="line">   instance_type=<span class="string">"ml.p4d.24xlarge"</span>,</span><br><span class="line">   <span class="comment"># Activate distributed training with SMDDP</span></span><br><span class="line">   distribution={ <span class="string">"pytorchddp"</span>: { <span class="string">"enabled"</span>: <span class="literal">True</span> } }  <span class="comment"># mpirun, activates SMDDP AllReduce OR AllGather</span></span><br><span class="line">   <span class="comment"># distribution={ "torch_distributed": { "enabled": True } }  # torchrun, activates SMDDP AllGather</span></span><br><span class="line">   <span class="comment"># distribution={ "smdistributed": { "dataparallel": { "enabled": True } } }  # mpirun, activates SMDDP AllReduce OR AllGather</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Other packages</p>
<ul>
<li><p>PyTorch Lightning - a lightweight PyTorch wrapper that provides a high-level interface for researchers and practitioners to streamline the training of deep learning models. It abstracts away many of the boilerplate code components traditionally required for training models, making the code cleaner, more modular, and more readable. It requires little setup of code and just need to insert a few parameters to the trainer</p>
<ul>
<li>Example<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trainer = L.Trainer(</span><br><span class="line">   max_epochs=<span class="number">3</span>,</span><br><span class="line">   callbacks=callbacks,</span><br><span class="line">   accelerator=<span class="string">"gpu"</span>,</span><br><span class="line">   devices=<span class="number">4</span>,  <span class="comment"># &lt;-- NEW</span></span><br><span class="line">   strategy=<span class="string">"ddp"</span>,  <span class="comment"># &lt;-- NEW</span></span><br><span class="line">   precision=<span class="string">"16"</span>,</span><br><span class="line">   logger=logger,</span><br><span class="line">   log_every_n_steps=<span class="number">10</span>,</span><br><span class="line">   deterministic=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Hugging Face <code>Accelerate</code>: a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code. It is still built on <code>torch_xla</code> and <code>torch.distributed</code>, but have get users rid of writing custom code to adapt to these platforms.</p>
<ul>
<li>Benefits include easy utilization of ZeRO Optimizer from DeepSpeed, achieve FSDP and mixed-precision training as well.</li>
<li>Example</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line">accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line">model, optimizer, training_dataloader, scheduler = accelerator.prepare(</span><br><span class="line">    model, optimizer, training_dataloader, scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> training_dataloader:</span><br><span class="line">      optimizer.zero_grad()</span><br><span class="line">      inputs, targets = batch</span><br><span class="line">      inputs = inputs.to(device)</span><br><span class="line">      targets = targets.to(device)</span><br><span class="line">      outputs = model(inputs)</span><br><span class="line">      loss = loss_function(outputs, targets)</span><br><span class="line">      accelerator.backward(loss)</span><br><span class="line">      optimizer.step()</span><br><span class="line">      scheduler.step()</span><br></pre></td></tr></table></figure>

<ul>
<li>In terminal, run <code>accelerate launch {my_script.py}</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="4-Challenges-and-Solutions"><a href="#4-Challenges-and-Solutions" class="headerlink" title="4. Challenges and Solutions"></a>4. Challenges and Solutions</h2><ol>
<li>Communication Overhead:</li>
</ol>
<ul>
<li><p>In distributed training, the exchange of information between devices becomes a potential bottleneck. As the number of devices increases, coordinating updates and sharing gradients become more complex.</p>
</li>
<li><p>Solutions:</p>
<ul>
<li><p>Optimized Communication Protocols: Leveraging optimized communication protocols, such as NVIDIA NCCL for GPU communication, helps minimize the latency associated with inter-device communication.</p>
</li>
<li><p>Gradient Accumulation: By accumulating gradients locally on each device before synchronization, communication frequency is reduced. This strategy can be beneficial in scenarios where frequent synchronization is not necessary.</p>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Fault Tolerance:</li>
</ol>
<ul>
<li><p>In distributed environments, hardware failures or network issues are inevitable. Ensuring fault tolerance is essential to maintain the integrity of the training process.</p>
</li>
<li><p>Solutions</p>
<ul>
<li><p>Checkpointing: Regularly saving model checkpoints allows training to resume from the most recent checkpoint in case of a failure. This practice minimizes data loss and ensures continuity.</p>
</li>
<li><p>Redundancy: Introducing redundancy by running multiple instances of the training job across different nodes adds a layer of resilience. Load balancing techniques can be employed to distribute tasks effectively.</p>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li>Scaling Issues:</li>
</ol>
<ul>
<li>Scaling distributed training to a large number of nodes presents challenges in terms of efficiency and resource management.</li>
<li>Strategies<ul>
<li>Dynamic Resource Allocation: Implementing dynamic resource allocation ensures that resources are allocated efficiently based on the current load. Kubernetes and other orchestration tools can facilitate dynamic scaling.</li>
<li>Parameter Servers: Utilizing parameter servers, which are dedicated servers responsible for storing and distributing model parameters, can enhance the scalability of distributed training.</li>
</ul>
</li>
</ul>
<h2 id="5-Common-Mistakes"><a href="#5-Common-Mistakes" class="headerlink" title="5. Common Mistakes"></a>5. Common Mistakes</h2><ol>
<li>Not pipelining</li>
</ol>
<ul>
<li>Pipeline Parallelism is always something to include. Notice the use of ZeRO-3 also uses pipeline parallelism</li>
</ul>
<ol start="2">
<li>Not balancing pipeline stages</li>
</ol>
<ul>
<li>There will be some brief periods where either a machine is idle and waiting on the next minibatch from the previous machine or takes longer than other machines to execute its computation, thus slowing down the pipeline.</li>
<li>You should ideally construct your pipeline such that each machine does as close to the same amount of computation as possible. This means timing how long it takes data to get through different layers in the model, timing how long forward and backward propagation takes for each model partition, and ensuring roughly equivalent data sizes across mini-batches. This is critical for optimizing pipeline efficiency.</li>
<li>To achieve this, setting up profiler like PyTorch Profiler is critical for evaluation of computations done during model training</li>
</ul>
<ol start="3">
<li>Weight staleness</li>
</ol>
<ul>
<li>When model training is pipelined across multiple machines, there is a delay that happens between when the forward computation on data occurs and when the gradients based on that computation are backpropagated to update the model weights. As a result, forward propagation are calculated using weights that aren't updated with the latest gradients.</li>
<li>Solution: <strong>weight stashing</strong><br>A system “maintains multiple versions of a model’s weights, one for each minibatch.” After the completion of each forward pass, the system can store a model’s weights as part of the state associated with that minibatch. When the time comes for backpropagation, the weights associated with that minibatch are retrieved from the stash and used for the gradient computation. This ensures that the same version of weights are used for the forward and backward pass over a single minibatch of data within a pipelined stage, and statistical convergence is improved.</li>
</ul>
<ol start="4">
<li>Driver and library inconsistencies between machines</li>
</ol>
<ul>
<li>Containerization / Virtualization using tools like Docker solves the problem</li>
</ul>
<ol start="5">
<li>Wrong type of Optimizer Update</li>
</ol>
<ul>
<li>Example: Synchronous vs Asynchronous SGD</li>
<li>Asynchronous SGD (<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf">HogWild</a> as a popular choice) which showed that SGD could be run in parallel, without locks, and without too much effect on algorithm convergence. Asynchronous SGD allows weight updates to proceed without each machine waiting for the other to send their gradients.</li>
</ul>
<ol start="6">
<li>Network issues, firewalls, ports, and communication errors</li>
</ol>
<ul>
<li>Solutions:<ul>
<li>Relying less on network for communication</li>
<li>If necessary to communicate, a process must specify the IP address and port number across which to transmit this information</li>
<li>Backup Frequently</li>
<li>Better logging</li>
</ul>
</li>
</ul>
<ol start="7">
<li>Slow data transmission</li>
</ol>
<ul>
<li>Solutions:<ul>
<li>Avoid making RPC calls</li>
<li>Try higher bandwidth interconnects like NVLink and Infini-band</li>
<li>FP32 -&gt; FP16 / Mixed precision</li>
<li>transmit a subset of gradients as soon as they are calculated (i.e. sending the gradients of a single layer) while at the same time, backpropagation is being performed on subsequent layers.</li>
</ul>
</li>
</ul>
<h2 id="6-A-complete-Distributed-DL-pipeline"><a href="#6-A-complete-Distributed-DL-pipeline" class="headerlink" title="6. A complete Distributed DL pipeline"></a>6. A complete Distributed DL pipeline</h2><ol>
<li><p>Distributed Training Setup:</p>
<ul>
<li>Set up a distributed computing environment, typically using a cluster or cloud infrastructure like AWS, Google Cloud, or Azure.</li>
<li>Ensure that all nodes in the cluster have the necessary libraries (TensorFlow, PyTorch, etc.) and dependencies installed.</li>
<li>Split the training dataset across nodes to distribute the workload.</li>
</ul>
</li>
<li><p>Synchronization and Communication:</p>
<ul>
<li>Implement a synchronization mechanism to ensure that the model’s weights are updated consistently across all nodes.</li>
<li>Choose a communication protocol (e.g., Parameter Server, AllReduce) for aggregating gradients and exchanging model updates.</li>
</ul>
</li>
<li><p>Model Initialization:</p>
<ul>
<li>Initialize the same model architecture with random weights on all nodes.</li>
<li>Setup model to follow data parallelism</li>
</ul>
</li>
<li><p>Training Loop (The main discussion we had in the blog):</p>
<ul>
<li>Start the training loop on each node with its batch of data.</li>
<li>Compute gradients for the batch, update local weights, and synchronize with other nodes.</li>
<li>Repeat this process for a predefined number of epochs.</li>
<li>Implement early stopping to prevent overfitting and save the best-performing model checkpoint during training.</li>
<li>Periodically evaluate the model’s performance on the validation dataset to ensure it’s learning effectively.</li>
<li>Save model checkpoints at regular intervals during training to resume from a specific point in case of interruptions.</li>
<li>If necessary, scale up the distributed training environment by adding more nodes to further accelerate training or handle larger datasets.</li>
</ul>
</li>
<li><p>Monitoring and Logging:</p>
<ul>
<li>Implement monitoring and logging to track training progress, including loss, accuracy, and other relevant metrics.</li>
<li>Use tools like TensorBoard or custom logging solutions to visualize training statistics.</li>
</ul>
</li>
<li><p>Hyperparameter Tuning:</p>
<ul>
<li>Perform hyperparameter tuning, which may include learning rate adjustments, batch sizes, and other parameters, to optimize the model’s performance.</li>
<li><strong>Note</strong>: you should set a budget alert before this, as running multiple experiments (on a large model) in a distributed setting can be very COSTLY!!!</li>
</ul>
</li>
<li><p>Post-training Analysis:</p>
<ul>
<li>This can go before/after/hand-in-hand with step 6, as part of model tuning</li>
<li>Analyze the trained model’s performance on the test dataset to assess its generalization capabilities.</li>
</ul>
</li>
<li><p>Deployment:</p>
<ul>
<li>Deploy the trained model for inference in your production environment, whether it’s on the cloud or at the edge.</li>
<li>Sometime this requires distributing model weights across servers as well</li>
</ul>
</li>
<li><p>Additional Fine-tuning (Optional):</p>
<ul>
<li>Fine-tune the model as needed based on deployment feedback or new data.</li>
<li>Checkout Hugging Face’s <a target="_blank" rel="noopener" href="https://huggingface.co/docs/trl/en/index">TRL library</a> &amp; its tutorials to understand more.</li>
</ul>
</li>
<li><p>Documentation:</p>
</li>
</ol>
<ul>
<li>Document the entire distributed training process, including configuration settings, data preprocessing steps, and model architecture, for future reference.</li>
</ul>
<ol start="11">
<li>Maintenance and Updates:</li>
</ol>
<ul>
<li>Regularly update and maintain the distributed training system, including libraries, dependencies, and data pipelines, to ensure its reliability and performance.</li>
</ul>
<p>For the basic scripts without distributed training and with basic DDP. You may refer to the <a target="_blank" rel="noopener" href="https://theaisummer.com/distributed-training-pytorch/">tutorial here</a>. If you want a one-off solution, please refer to the code below.</p>
<h2 id="7-A-more-challenging-code-using-native-PyTorch"><a href="#7-A-more-challenging-code-using-native-PyTorch" class="headerlink" title="7. A more challenging code using native PyTorch"></a>7. A more challenging code using native PyTorch</h2><p>If you are interested in building it from scratch with PyTorch directly, checkout the following code (if you don’t understand the syntax, please DIY)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""A demo on how to setup custom trainer with efficient training"""</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> apex.amp <span class="keyword">as</span> amp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNet</span>(nn.Module):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span><br><span class="line">      <span class="built_in">super</span>(ConvNet, self).__init__()</span><br><span class="line">      self.layer1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">      self.layer2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">      self.fc = nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">32</span>, num_classes)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      out = self.layer1(x)</span><br><span class="line">      out = self.layer2(out)</span><br><span class="line">      out = out.reshape(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">      out = self.fc(out)</span><br><span class="line">      <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ddp_setup</span>()  <span class="comment">#(rank: int, world_size: int):</span></span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">      rank: Unique ID of each</span></span><br><span class="line"><span class="string">      world_size: Total number of processes</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">   <span class="comment"># multi-gpu setup</span></span><br><span class="line">   <span class="comment"># os.environ['MASTER_ADDR'] = 'your master node ip'</span></span><br><span class="line">   <span class="comment"># os.environ['MASTER_PORT'] = '8888'</span></span><br><span class="line">   <span class="comment"># dist.init_process_group(</span></span><br><span class="line">   <span class="comment">#     backend='nccl',</span></span><br><span class="line">   <span class="comment">#     init_method='env://',</span></span><br><span class="line">   <span class="comment">#     world_size=world_size,</span></span><br><span class="line">   <span class="comment">#     rank=rank</span></span><br><span class="line">   <span class="comment"># )</span></span><br><span class="line">   dist.init_process_group(backend=<span class="string">"nccl"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Trainer</span>:</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">      self,</span></span><br><span class="line"><span class="params">      model: torch.nn.Module,</span></span><br><span class="line"><span class="params">      train_data: DataLoader,</span></span><br><span class="line"><span class="params">      optimizer: torch.optim.Optimizer,</span></span><br><span class="line"><span class="params">      criterion: torch.nn.Module,</span></span><br><span class="line"><span class="params">      <span class="comment"># gpu_id: int,</span></span></span><br><span class="line"><span class="params">      save_every: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">      snapshot_path: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">      <span class="comment"># self.gpu_id = gpu_id</span></span><br><span class="line">      self.local_rank = <span class="built_in">int</span>(os.environ[<span class="string">"LOCAL_RANK"</span>])</span><br><span class="line">      self.global_rank = <span class="built_in">int</span>(os.environ[<span class="string">"RANK"</span>])</span><br><span class="line">      self.model = model.to(self.local_rank)</span><br><span class="line">      self.train_data = train_data</span><br><span class="line">      self.optimizer = optimizer</span><br><span class="line">      self.criterion = criterion</span><br><span class="line">      self.epochs_run = <span class="number">0</span></span><br><span class="line">      self.save_every = save_every</span><br><span class="line"></span><br><span class="line">      self.model, self.optimizer = amp.initialize(</span><br><span class="line">            self.model, self.optimizer, opt_level=<span class="string">'O1'</span>)</span><br><span class="line">      <span class="keyword">if</span> os.path.exists(snapshot_path):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Loading Snapshot"</span>)</span><br><span class="line">            self._load_snapshot(snapshot_path)</span><br><span class="line">      self.model = DDP(self.model, device_ids=[self.local_rank])</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">_load_snapshot</span>(<span class="params">self, snapshot_path</span>):</span><br><span class="line">      <span class="string">"""Resume training from previous checkpoint"""</span></span><br><span class="line">      snapshot = torch.load(snapshot_path)</span><br><span class="line">      self.model.load_state_dict(snapshot[<span class="string">'model'</span>])</span><br><span class="line">      self.optimizer.load_state_dict(snapshot[<span class="string">'optimizer'</span>])</span><br><span class="line">      self.epochs_run = snapshot[<span class="string">"epochs_resume"</span>]</span><br><span class="line">      amp.load_state_dict(snapshot[<span class="string">'amp'</span>])</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f"Resuming training from snapshot at Epoch <span class="subst">{self.epochs_run}</span>"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">_run_batch</span>(<span class="params">self, source, targets</span>):</span><br><span class="line">      self.optimizer.zero_grad()</span><br><span class="line">      output = self.model(source)</span><br><span class="line">      loss = self.criterion(output, targets)</span><br><span class="line">      loss.backward()</span><br><span class="line">      <span class="keyword">with</span> amp.scale_loss(loss, self.optimizer) <span class="keyword">as</span> scaled_loss:</span><br><span class="line">            scaled_loss.backward()</span><br><span class="line">      self.optimizer.step()</span><br><span class="line">      <span class="keyword">return</span> loss.item()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">_run_epoch</span>(<span class="params">self, epoch: <span class="built_in">int</span>, total_epochs: <span class="built_in">int</span></span>):</span><br><span class="line">      self.model.train()</span><br><span class="line">      <span class="keyword">for</span> i, (source, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.train_data):</span><br><span class="line">            source = source.to(self.local_rank)</span><br><span class="line">            targets = targets.to(self.local_rank)</span><br><span class="line">            loss = self._run_batch(source, targets)</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> self.local_rank == <span class="number">0</span>:</span><br><span class="line">               <span class="built_in">print</span>(</span><br><span class="line">                  <span class="string">f'[GPU<span class="subst">{self.global_rank}</span>] Epoch [<span class="subst">{epoch + <span class="number">1</span>}</span>/<span class="subst">{total_epochs}</span>], Step [<span class="subst">{i + <span class="number">1</span>}</span>/<span class="subst">{<span class="built_in">len</span>(self.train_data)}</span>], Loss: <span class="subst">{loss:<span class="number">.4</span>f}</span>'</span>)</span><br><span class="line"></span><br><span class="line">      self.model.<span class="built_in">eval</span>()</span><br><span class="line">      <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> i, (source, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.val_data):</span><br><span class="line">               source = source.</span><br><span class="line">               targets = targets.</span><br><span class="line">               loss = ...</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">_save_snapshot</span>(<span class="params">self, save_dir, epoch, model_seed</span>):</span><br><span class="line">      path = <span class="string">f"<span class="subst">{save_dir}</span>/base_model_seed=<span class="subst">{model_seed}</span>_epoch=<span class="subst">{epoch}</span>.pt"</span></span><br><span class="line">      torch.save({</span><br><span class="line">            <span class="string">'model'</span>: self.model,  <span class="comment"># if saving state_dict, use .module.state_dict()</span></span><br><span class="line">            <span class="string">'optimizer_state_dict'</span>: self.optimizer.state_dict(),</span><br><span class="line">            <span class="string">'scheduler_state_dict'</span>: self.scheduler.state_dict(),</span><br><span class="line">            <span class="string">'amp'</span>: amp.state_dict(),</span><br><span class="line">            <span class="string">'epochs_resume'</span>: self.epochs_run</span><br><span class="line">      }, path)</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch}</span> | Training snapshot saved at <span class="subst">{path}</span>"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, total_epochs: <span class="built_in">int</span>, model_seed: <span class="built_in">int</span>, save_dir: <span class="built_in">str</span></span>):</span><br><span class="line">      <span class="string">"""Training script"""</span></span><br><span class="line">      <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.epochs_run, total_epochs):</span><br><span class="line">            self._run_epoch(epoch, total_epochs)</span><br><span class="line">            <span class="keyword">if</span> self.local_rank == <span class="number">0</span> <span class="keyword">and</span> epoch % self.save_every == <span class="number">0</span>:</span><br><span class="line">               self._save_snapshot(save_dir, epoch, model_seed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_train_params</span>():</span><br><span class="line">   train_set = MyTrainDataset(<span class="number">2048</span>)</span><br><span class="line">   model = ConvNet()</span><br><span class="line">   optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">   criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">   <span class="keyword">return</span> train_set, model, optimizer, criterion</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_dataloader</span>(<span class="params">dataset: Dataset, batch_size: <span class="built_in">int</span>, num_workers: <span class="built_in">int</span>, sampler: DistributedSampler</span>):</span><br><span class="line">   <span class="keyword">return</span> DataLoader(</span><br><span class="line">      dataset,</span><br><span class="line">      batch_size=batch_size,</span><br><span class="line">      pin_memory=<span class="literal">True</span>,</span><br><span class="line">      shuffle=<span class="literal">True</span>,</span><br><span class="line">      num_workers=num_workers,</span><br><span class="line">      sampler=sampler</span><br><span class="line">   )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">args</span>):</span><br><span class="line">   <span class="string">"""Run entire pipeline"""</span></span><br><span class="line">   torch.manual_seed(args.seed)</span><br><span class="line">   <span class="comment"># rank = args.nr * args.gpus + gpu</span></span><br><span class="line">   <span class="comment"># ddp_setup(rank, args.world_size)</span></span><br><span class="line">   ddp_setup()</span><br><span class="line">   dataset, model, optimizer, criterion = load_train_params()</span><br><span class="line">   <span class="comment"># sampler = DistributedSampler(</span></span><br><span class="line">   <span class="comment">#     dataset, num_replicas=args.world_size, rank=rank)</span></span><br><span class="line">   sampler = DistributedSampler(dataset)</span><br><span class="line">   train_data = prepare_dataloader(</span><br><span class="line">      dataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">0</span>, sampler=sampler)</span><br><span class="line">   trainer = Trainer(model, train_data, optimizer,</span><br><span class="line">                     criterion, args.save_every, args.snapshot_path)</span><br><span class="line">   trainer.train(args.total_epochs, args.seed, args.save_dir)</span><br><span class="line">   dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">   <span class="string">"""Entry point</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   to call the script using torchrun (which manages the )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   e.g: 4 GPU per machine, 50 epochs, saving every 10 epoch</span></span><br><span class="line"><span class="string">   torchrun \</span></span><br><span class="line"><span class="string">   --nproc_per_node=4 \</span></span><br><span class="line"><span class="string">   --nnodes=2 \</span></span><br><span class="line"><span class="string">   --node_rank=0 \</span></span><br><span class="line"><span class="string">   --rdzv_id=456 \</span></span><br><span class="line"><span class="string">   --rdzv_backend=c10d \</span></span><br><span class="line"><span class="string">   --rdzv_endpoint=HOST_MACHINE:PORT \</span></span><br><span class="line"><span class="string">   FILE_NAME.py --epochs=50 --save_every=10</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   nproc_per_node: usually num of GPUs per machine</span></span><br><span class="line"><span class="string">   nnodes: num of machines</span></span><br><span class="line"><span class="string">   node_rank: ID: 0/1/2/.... for each machine</span></span><br><span class="line"><span class="string">   Notes on endpoint: choose endpoint whose machine has high network bandwidth</span></span><br><span class="line"><span class="string">   Note: Multi-GPU on single machine is much faster than Multi-node each with single GPU due to bandwidth limit over TCP</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">   parser = argparse.ArgumentParser()</span><br><span class="line">   <span class="comment"># parser.add_argument('-n', '--nodes', default=1,</span></span><br><span class="line">   <span class="comment">#                     type=int, metavar='N')</span></span><br><span class="line">   <span class="comment"># parser.add_argument('-g', '--gpus', default=1, type=int,</span></span><br><span class="line">   <span class="comment">#                     help='number of gpus per node')</span></span><br><span class="line">   <span class="comment"># parser.add_argument('-nr', '--nr', default=0, type=int,</span></span><br><span class="line">   <span class="comment">#                     help='ranking within the nodes')</span></span><br><span class="line">   parser.add_argument(<span class="string">'--epochs'</span>, default=<span class="number">2</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        metavar=<span class="string">'N'</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'number of total epochs to run'</span>)</span><br><span class="line">   parser.add_argument(<span class="string">'-s'</span>, <span class="string">'--seed'</span>, default=<span class="number">42</span>,</span><br><span class="line">                        <span class="built_in">type</span>=<span class="built_in">int</span>, metavar=<span class="string">'N'</span>)</span><br><span class="line">   parser.add_argument(<span class="string">'--save_every'</span>, default=<span class="number">5</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'interval to save the snapshot'</span>)</span><br><span class="line">   parser.add_argument(<span class="string">'--save_dir'</span>, default=<span class="string">'.'</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'directory to save the snapshot'</span>)</span><br><span class="line">   parser.add_argument(<span class="string">'--snapshot_path'</span>, default=<span class="string">'.'</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'path of the snapshot to restore training from'</span>)</span><br><span class="line">   args = parser.parse_args()</span><br><span class="line">   <span class="comment">#########################################################</span></span><br><span class="line">   args.world_size = args.gpus * \</span><br><span class="line">      args.nodes <span class="keyword">if</span> args.gpus &gt;= <span class="number">0</span> <span class="keyword">else</span> torch.cuda.device_count()</span><br><span class="line">   args.total_epochs = args.epochs</span><br><span class="line"></span><br><span class="line">   <span class="comment"># mp.spawn(main, nprocs=args.world_size, args=(args,))</span></span><br><span class="line">   run(args)</span><br><span class="line">   <span class="comment">#########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">########## POST MORTEM ###################</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Common Troubleshooting</span></span><br><span class="line"><span class="string">1. Check nodes can communicate through **TCP**</span></span><br><span class="line"><span class="string">2. Check inbound rules on a security group (on AWS)</span></span><br><span class="line"><span class="string">3. export NCCL_DEBUG=INFO to set verbose logs</span></span><br><span class="line"><span class="string">4. export NCCL_SOCKET_IFNAME to ensure TCP connection is correct</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">SLURM work scheduler setup TODO</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://neptune.ai/blog/distributed-training-errors">https://neptune.ai/blog/distributed-training-errors</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Understanding Distributed Training in Deep Learning</p><p><a href="https://criss-wang.github.io/post/blogs/mlops/distributed-training/">https://criss-wang.github.io/post/blogs/mlops/distributed-training/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Zhenlin Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-03-04</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-03-24</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/post/blogs/mlops/ml-post-training/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">MLOps Post Training Considerations</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/post/blogs/mlops/quantization/"><span class="level-item">Quantization in Deep Learning</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Selfie.webp" alt="Zhenlin Wang (Criss)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Zhenlin Wang (Criss)</p><div class="is-size-7 multiline is-block justify-content-center" style="white-space:pre;font-style: italic">Software Development
Machine Learning
Artificial Intelligence
</div><div style="padding-top: 10px;"></div><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sunnyvale, CA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">68</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">38</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">42</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Criss-Wang/" target="_blank" rel="noopener"><i class="fab fa-github"></i>   Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Criss-Wang/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/zhenlin-wang/"><i class="fab fa-linkedin-in"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:zhenlinw@cs.cmu.edu"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="CV" href="https://twitter.com/CrissWang4"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Mining-Data-Engineering/"><span class="tag">Data Mining/Data Engineering</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Supervised-Learning/"><span class="tag">Supervised Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Clustering/"><span class="tag">Clustering</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Database-System/"><span class="tag">Database System</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Data/"><span class="tag">Big Data</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistics/"><span class="tag">Statistics</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/System-Design/"><span class="tag">System Design</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommender-Systems/"><span class="tag">Recommender Systems</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Software-Engineering/"><span class="tag">Software Engineering</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analytics/"><span class="tag">Data Analytics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-Training/"><span class="tag">Distributed Training</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-Project/"><span class="tag">Python Project</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-System/"><span class="tag">Distributed System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cloud-Computing/"><span class="tag">Cloud Computing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/A-B-testing/"><span class="tag">A/B testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix-Computation/"><span class="tag">Matrix Computation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dimensionality-Reduction/"><span class="tag">Dimensionality Reduction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-Systems/"><span class="tag">Distributed Systems</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Management/"><span class="tag">Project Management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistical-Inference/"><span class="tag">Statistical Inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representaiton-Learning/"><span class="tag">Representaiton Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Markov-Models/"><span class="tag">Hidden Markov Models</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-Programming/"><span class="tag">Dynamic Programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bayesian-Statistics/"><span class="tag">Bayesian Statistics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-learning/"><span class="tag">Deep learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble/"><span class="tag">Ensemble</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Random-Forest/"><span class="tag">Random Forest</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regularization/"><span class="tag">Regularization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Evaluation-Methods/"><span class="tag">Evaluation Methods</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#1-Definition"><span class="level-left"><span class="level-item">2</span><span class="level-item">1. Definition</span></span></a></li><li><a class="level is-mobile" href="#2-Parallelism-in-Training"><span class="level-left"><span class="level-item">3</span><span class="level-item">2. Parallelism in Training</span></span></a></li><li><a class="level is-mobile" href="#3-Strategies-in-detail"><span class="level-left"><span class="level-item">4</span><span class="level-item">3. Strategies in detail</span></span></a></li><li><a class="level is-mobile" href="#4-Challenges-and-Solutions"><span class="level-left"><span class="level-item">5</span><span class="level-item">4. Challenges and Solutions</span></span></a></li><li><a class="level is-mobile" href="#5-Common-Mistakes"><span class="level-left"><span class="level-item">6</span><span class="level-item">5. Common Mistakes</span></span></a></li><li><a class="level is-mobile" href="#6-A-complete-Distributed-DL-pipeline"><span class="level-left"><span class="level-item">7</span><span class="level-item">6. A complete Distributed DL pipeline</span></span></a></li><li><a class="level is-mobile" href="#7-A-more-challenging-code-using-native-PyTorch"><span class="level-left"><span class="level-item">8</span><span class="level-item">7. A more challenging code using native PyTorch</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">9</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp" alt="Criss Wang&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2024 Zhenlin Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="/js/night.js" defer></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>