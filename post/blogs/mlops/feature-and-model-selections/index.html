<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Feature Selection &amp; Model Selections - Criss Wang&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Criss Wang&#039;s Log Book"><meta name="msapplication-TileImage" content="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Criss Wang&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="A rudimentary method forgotten by many practitioners nowadays"><meta property="og:type" content="blog"><meta property="og:title" content="Feature Selection &amp; Model Selections"><meta property="og:url" content="https://criss-wang.github.io/post/blogs/mlops/feature-and-model-selections/"><meta property="og:site_name" content="Criss Wang&#039;s Log Book"><meta property="og:description" content="A rudimentary method forgotten by many practitioners nowadays"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://criss-wang.github.io/images/Data%20Science%20Concept/filter.png"><meta property="og:image" content="https://criss-wang.github.io/images/Data%20Science%20Concept/wrapper.png"><meta property="og:image" content="https://criss-wang.github.io/images/Data%20Science%20Concept/embedded.png"><meta property="og:image" content="https://criss-wang.github.io/images/Data%20Science%20Concept/learning_curve.png"><meta property="og:image" content="https://criss-wang.github.io/images/Data%20Science%20Concept/validation_curve.png"><meta property="article:published_time" content="2019-06-04T04:00:00.000Z"><meta property="article:modified_time" content="2021-05-14T04:00:00.000Z"><meta property="article:author" content="Zhenlin Wang"><meta property="article:tag" content="Data Mining/Data Engineering"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://criss-wang.github.io/images/Data%20Science%20Concept/filter.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://criss-wang.github.io/post/blogs/mlops/feature-and-model-selections/"},"headline":"Feature Selection & Model Selections","image":["https://criss-wang.github.io/images/Data%20Science%20Concept/filter.png","https://criss-wang.github.io/images/Data%20Science%20Concept/wrapper.png","https://criss-wang.github.io/images/Data%20Science%20Concept/embedded.png","https://criss-wang.github.io/images/Data%20Science%20Concept/learning_curve.png","https://criss-wang.github.io/images/Data%20Science%20Concept/validation_curve.png"],"datePublished":"2019-06-04T04:00:00.000Z","dateModified":"2021-05-14T04:00:00.000Z","author":{"@type":"Person","name":"Zhenlin Wang"},"publisher":{"@type":"Organization","name":"Criss Wang's Log Book","logo":{"@type":"ImageObject","url":"https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"}},"description":"A rudimentary method forgotten by many practitioners nowadays"}</script><link rel="canonical" href="https://criss-wang.github.io/post/blogs/mlops/feature-and-model-selections/"><link rel="icon" href="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto Slab:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp" alt="Criss Wang&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" style="font-weight: bold" href="/">Criss&#039;s Time Machine</a><a class="navbar-item" style="font-weight: bold" href="/categories/Blogs">Machine Learning</a><a class="navbar-item" style="font-weight: bold" href="/categories/Software">Software Engineering</a><a class="navbar-item" style="font-weight: bold" href="/categories/Projects">Projects</a><a class="navbar-item" style="font-weight: bold" href="/research">Research</a><a class="navbar-item" style="font-weight: bold" href="/archives">Archives</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Criss-Wang/"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-4 is-size-5-mobile has-text-weight-normal">Feature Selection &amp; Model Selections</h1><div class="article-meta is-size-7 level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-check"> </i><time dateTime="2019-06-04T04:00:00.000Z" title="2019-06-04T04:00:00.000Z">2019-06-04</time></span><span class="level-item"><i class="far fa-folder-open"> </i><a class="link-muted" href="/categories/Blogs/">Blogs</a></span></div></div><div><hr style="background-color:grey"></div><div style="padding-bottom:5px"></div><div class="content"><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>Running machine learning models have become much easier in recent years. The prevalence of tutorials and model packages makes it much more convenient for people to apply various theoretically complex algorithms on their datasets and thrive. So to excel in the field of data science, one cannot simple KNOW how to use models, but also <strong>appreciate</strong> each model's significance and <strong>select</strong> proper models wisely. That's where feature selections and model selections come in. Both turn out to be challenging and extremely useful in the same time. In light of this, I want to take down the notes I learned through practice and tutorials some key aspects of these two things.</p>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><ol>
<li>Benefits<ul>
<li>It enables the machine learning algorithm to train faster.</li>
<li>It reduces the complexity of a model and makes it easier to interpret.</li>
<li>It improves the accuracy of a model if the right subset is chosen.</li>
<li>It reduces Overfitting</li>
</ul>
</li>
<li>Methods</li>
</ol>
<p>  Here we discuss about some widely used methods for feature selections. To facilitate the demo code, we require the following packages to be applied and data being tuned:<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, RidgeCV, LassoCV, Ridge, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#Loading the dataset</span></span><br><span class="line">x = load_boston()</span><br><span class="line">df = pd.DataFrame(x.data, columns = x.feature_names)</span><br><span class="line">df[<span class="string">"MEDV"</span>] = x.target</span><br><span class="line">X = df.drop(<span class="string">"MEDV"</span>,<span class="number">1</span>)   <span class="comment">#Feature Matrix</span></span><br><span class="line">y = df[<span class="string">"MEDV"</span>]          <span class="comment">#Target Variable</span></span><br></pre></td></tr></table></figure></p>
<ol>
<li><p>Filter Methods</p>
<ul>
<li>No mining algorithm included</li>
<li>Uses the exact assessment criterion which includes distance, information, dependency, and consistency. </li>
<li>The filter method uses the principal criteria of ranking technique and uses the rank ordering method for variable selection.</li>
<li>Generally used as a dasta preprocessing step </li>
<li>Several main filter methods based on the variable attributes:   <figure align="center">
<img src="/images/Data%20Science%20Concept/filter.png" width="500px">
<figcaption>filter methods</figcaption>
 </figure></li>
</ul>
</li>
<li><p>Wrapper Methods</p>
<ul>
<li>workflow: <figure align="center">
<img src="/images/Data%20Science%20Concept/wrapper.png" width="500px">
<figcaption>filter methods</figcaption>
 </figure>
- Use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset</li>
<li>Computationally expensive</li>
<li>3 Types:<ol>
<li><kbd>Forward Selection</kbd>: An iterative method<ul>
<li>Start with having no feature in the model. </li>
<li>In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.</li>
</ul>
</li>
<li><kbd>Backward Elimination</kbd>: An iterative method<ul>
<li>Start with all the features and removes the least significant feature at each iteration which improves the performance of the model. </li>
<li>We repeat this until no improvement is observed on removal of features.</li>
<li>E.g. If the p-value is above 0.05 then we remove the feature, else we keep it. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class="line">X_1 = sm.add_constant(X)</span><br><span class="line"><span class="comment"># Fitting sm.OLS model</span></span><br><span class="line">model = sm.OLS(y,X_1).fit()</span><br><span class="line">display(model.pvalues)</span><br><span class="line"><span class="comment"># Backward Elimination</span></span><br><span class="line">cols = <span class="built_in">list</span>(X.columns)</span><br><span class="line">pmax = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">len</span>(cols)&gt;<span class="number">0</span>):</span><br><span class="line">    p= []</span><br><span class="line">    X_1 = X[cols]</span><br><span class="line">    X_1 = sm.add_constant(X_1)</span><br><span class="line">    model = sm.OLS(y,X_1).fit()</span><br><span class="line">    p = pd.Series(model.pvalues.values[<span class="number">1</span>:],index = cols)      </span><br><span class="line">    pmax = <span class="built_in">max</span>(p)</span><br><span class="line">    feature_with_p_max = p.idxmax()</span><br><span class="line">    <span class="keyword">if</span>(pmax&gt;<span class="number">0.05</span>):</span><br><span class="line">        cols.remove(feature_with_p_max)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">selected_features_BE = cols</span><br><span class="line"><span class="built_in">print</span>(selected_features_BE)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><kbd>Recursive Feature elimination</kbd>: A greedy optimization algorithm<ul>
<li>It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. </li>
<li>It constructs the next model with the left features until all the features are exhausted. </li>
<li>It then ranks the features based on the order of their elimination <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = LinearRegression()</span><br><span class="line"><span class="comment">#Initializing RFE model</span></span><br><span class="line">rfe = RFE(model, <span class="number">7</span>)</span><br><span class="line"><span class="comment">#Transforming data using RFE</span></span><br><span class="line">X_rfe = rfe.fit_transform(X,y)  </span><br><span class="line"><span class="comment">#Fitting the data to model</span></span><br><span class="line">model.fit(X_rfe,y)</span><br><span class="line"><span class="built_in">print</span>(rfe.support_)</span><br><span class="line"><span class="built_in">print</span>(rfe.ranking_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span>  <span class="literal">True</span> <span class="literal">False</span>  <span class="literal">True</span> <span class="literal">False</span> <span class="literal">True</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">2</span> <span class="number">4</span> <span class="number">3</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">7</span> <span class="number">1</span> <span class="number">1</span> <span class="number">5</span> <span class="number">1</span> <span class="number">6</span> <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
 Here we took LinearRegression model with 7 features and RFE gave feature ranking as above, but the selection of number '7' was random. Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to 13. We then take the one for which the accuracy is highest. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#no of features</span></span><br><span class="line">nof_list=np.arange(<span class="number">1</span>,<span class="number">13</span>)            </span><br><span class="line">high_score=<span class="number">0</span></span><br><span class="line"><span class="comment">#Variable to store the optimum features</span></span><br><span class="line">nof=<span class="number">0</span>           </span><br><span class="line">score_list =[]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nof_list)):</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = <span class="number">0.3</span>, random_state = <span class="number">0</span>)</span><br><span class="line">    model = LinearRegression()</span><br><span class="line">    rfe = RFE(model,nof_list[n])</span><br><span class="line">    X_train_rfe = rfe.fit_transform(X_train,y_train)</span><br><span class="line">    X_test_rfe = rfe.transform(X_test)</span><br><span class="line">    model.fit(X_train_rfe,y_train)</span><br><span class="line">    score = model.score(X_test_rfe,y_test)</span><br><span class="line">    score_list.append(score)</span><br><span class="line">    <span class="keyword">if</span>(score&gt;high_score):</span><br><span class="line">        high_score = score</span><br><span class="line">        nof = nof_list[n]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Optimum number of features: %d"</span> %nof)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Score with %d features: %f"</span> % (nof, high_score))</span><br></pre></td></tr></table></figure>
 As seen from above code, the optimum number of features is 10. We now feed 10 as number of features to RFE and get the final set of features given by RFE method <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cols = <span class="built_in">list</span>(X.columns)</span><br><span class="line">model = LinearRegression()</span><br><span class="line"><span class="comment">#Initializing RFE model</span></span><br><span class="line">rfe = RFE(model, <span class="number">10</span>)             </span><br><span class="line"><span class="comment">#Transforming data using RFE</span></span><br><span class="line">X_rfe = rfe.fit_transform(X,y)  </span><br><span class="line"><span class="comment">#Fitting the data to model</span></span><br><span class="line">model.fit(X_rfe,y)              </span><br><span class="line">temp = pd.Series(rfe.support_,index = cols)</span><br><span class="line">selected_features_rfe = temp[temp==<span class="literal">True</span>].index</span><br><span class="line"><span class="built_in">print</span>(selected_features_rfe)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>(*) <kbd>Bidirectional Elimination</kbd>: A combination of <em>Forward Selection</em> &amp; <em>Backword Elimination</em></li>
</ol>
</li>
</ul>
</li>
<li><p>Self-defined Methods<br> There are many interesting methods that can be directly applied in experimentations. However, one method that caught my eyes is the Boruta method:</p>
<ul>
<li>Boruta Method (Using shadow features and random forest)<ul>
<li>The main reason I liked this is because its application on Random Forest and XGBoost models.</li>
<li>It generally works well with well structured data and relatively smaller datasets.</li>
<li>In the hindsight, it is still relatively slower as compared to some simpler selection criterion, and it does not handle <strong>multicollinearity</strong> immediately.</li>
<li>checkout <a target="_blank" rel="noopener" href="https://towardsdatascience.com/simple-example-using-boruta-feature-selection-in-python-8b96925d5d7a">this python tutorial</a> for more details</li>
</ul>
</li>
</ul>
</li>
<li><p>Embedded Methods<br> It combines the qualities of filter and wrapper methods. It's implemented by algorithms that have their own built-in feature selection methods</p>
<ul>
<li><p>Workflow</p>
  <figure align="center">
    <img src="/images/Data%20Science%20Concept/embedded.png" width="500px">
    <figcaption>Embedded Method Workflow</figcaption>
  </figure>
</li>
<li><p>Here in the demo code we will do feature selection using Lasso regularization. If the feature is irrelevant, lasso penalizes it's coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">reg = LassoCV()</span><br><span class="line">reg.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Best alpha using built-in LassoCV: %f"</span> % reg.alpha_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Best score using built-in LassoCV: %f"</span> % reg.score(X,y))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Best alpha using built-<span class="keyword">in</span> LassoCV: <span class="number">0.724820</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Best score using built-<span class="keyword">in</span> LassoCV: <span class="number">0.702444</span></span><br><span class="line">coef = pd.Series(reg.coef_, index = X.columns)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Lasso picked "</span> + <span class="built_in">str</span>(<span class="built_in">sum</span>(coef != <span class="number">0</span>)) + <span class="string">" variables and eliminated the other "</span> +  <span class="built_in">str</span>(<span class="built_in">sum</span>(coef == <span class="number">0</span>)) + <span class="string">" variables"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Lasso picked <span class="number">10</span> variables <span class="keyword">and</span> eliminated the other <span class="number">3</span> variables</span><br><span class="line">imp_coef = coef.sort_values()</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8.0</span>, <span class="number">10.0</span>)</span><br><span class="line">imp_coef.plot(kind = <span class="string">"barh"</span>)</span><br><span class="line">plt.title(<span class="string">"Feature importance using Lasso Model"</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Filter vs Wrapper</p>
<p>Now let us make a comparison between filter methods and wrapper methods, the two most commonly used ways in feature selection.</p>
<table>
<thead>
<tr>
<th>Characteristics</th>
<th>Filter Method</th>
<th>Wrapper Methods</th>
</tr>
</thead>
<tbody><tr>
<td>Measure of feature relevance</td>
<td>correlation with dependent variable</td>
<td>actually training a model on a subset of feature</td>
</tr>
<tr>
<td>Speed</td>
<td>Much faster</td>
<td>Slower due to model training</td>
</tr>
<tr>
<td>Performance Evaluation</td>
<td>statistical methods for evaluation</td>
<td>Model results cross validation</td>
</tr>
<tr>
<td>Quality of feature set selected</td>
<td>May be suboptimal</td>
<td>Guaranteed to output optimal/near-optimal feature set</td>
</tr>
<tr>
<td>Overfitting ?</td>
<td>Less likely</td>
<td>Much more prone to</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h3><p>Here we must clarify one important conceptual misunderstanding:</p>
<p><strong>Note</strong>: Classical Model selection mainly focuses on performing metrics evaluations through different models, tuning the model parameter and variating the training datasets. The choice of model in the end is often <em>manual</em>. Hence, it differs from the automated model selection procedure where the final selection of model is also done automatically. The latter is often known as AutoML, and has gained quick wide popularity in recent years. </p>
<p>We now think about what are the main strategies to improve model performance:</p>
<ol>
<li>Use a more complicated/more flexible model</li>
<li>Use a less complicated/less flexible model</li>
<li>Tuning hyperparameters</li>
<li>Gather more training samples</li>
<li>Gather more data to add features to each sample<br>Clearly, the first 4 are model selection strategies, and the last one is feature selection.</li>
</ol>
<p>When we make these adjustments, we must keep in mind the <code>The Bias-variance trade-off</code>:</p>
<ul>
<li><code>bias</code>: Usually the case where the model <code>underfits</code>, i.e. it does not have enough model flexibility to suitably account for all the features in the data</li>
<li><code>variance</code>: Usually the case where the model <code>overfits</code>, i.e. so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution</li>
<li>For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.</li>
<li>For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.</li>
</ul>
<p>We can easily visualize this via the <strong>learning curve</strong><br>    <figure align="center"><br>      <img src="/images/Data%20Science%20Concept/learning_curve.png" width="500px"><br>      <figcaption>Plot 1: The curve to find the best amount of train set size (too low –&gt; high variance; too high –&gt; high bias)</figcaption><br>    </figure></p>
<p>In the meantime, we observe from the <strong>validation curve</strong> below that model complexity/hyperparameter choices affect the model performances as well<br>    <figure align="center"><br>      <img src="/images/Data%20Science%20Concept/validation_curve.png" width="500px"><br>      <figcaption>Plot 2: The curve to find the best hyperparameters</figcaption><br>    </figure></p>
<p>For more details on metrics evaluation and hyperparameter tuning with feedback from validation sets, interested readers can read my blogs on these topics as well.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Feature Selection &amp; Model Selections</p><p><a href="https://criss-wang.github.io/post/blogs/mlops/feature-and-model-selections/">https://criss-wang.github.io/post/blogs/mlops/feature-and-model-selections/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Zhenlin Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2019-06-04</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-05-14</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/post/blogs/mlops/hyperparam-tuning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Hyperparameter Tuning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/post/blogs/supervised/supervised-learning/"><span class="level-item">Some Supervised Learning Models</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Selfie.webp" alt="Zhenlin Wang (Criss)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Zhenlin Wang (Criss)</p><div class="is-size-7 multiline is-block justify-content-center" style="white-space:pre;font-style: italic">Software Development
Machine Learning
Artificial Intelligence
</div><div style="padding-top: 10px;"></div><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sunnyvale, CA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">72</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">38</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">46</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Criss-Wang/" target="_blank" rel="noopener"><i class="fab fa-github"></i>   Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Criss-Wang/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/zhenlin-wang/"><i class="fab fa-linkedin-in"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:zhenlinw@cs.cmu.edu"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="CV" href="https://twitter.com/CrissWang4"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Mining-Data-Engineering/"><span class="tag">Data Mining/Data Engineering</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Supervised-Learning/"><span class="tag">Supervised Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Clustering/"><span class="tag">Clustering</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Database-System/"><span class="tag">Database System</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML-Infrastructure/"><span class="tag">ML Infrastructure</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Development/"><span class="tag">Model Development</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Data/"><span class="tag">Big Data</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistics/"><span class="tag">Statistics</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/System-Design/"><span class="tag">System Design</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommender-Systems/"><span class="tag">Recommender Systems</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Software-Engineering/"><span class="tag">Software Engineering</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analytics/"><span class="tag">Data Analytics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-Training/"><span class="tag">Distributed Training</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python-Project/"><span class="tag">Python Project</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-System/"><span class="tag">Distributed System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cloud-Computing/"><span class="tag">Cloud Computing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/A-B-testing/"><span class="tag">A/B testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix-Computation/"><span class="tag">Matrix Computation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dimensionality-Reduction/"><span class="tag">Dimensionality Reduction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-Systems/"><span class="tag">Distributed Systems</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Evaluation/"><span class="tag">Evaluation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLOps/"><span class="tag">MLOps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Management/"><span class="tag">Project Management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Markov-Models/"><span class="tag">Hidden Markov Models</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-Programming/"><span class="tag">Dynamic Programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistical-Inference/"><span class="tag">Statistical Inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representaiton-Learning/"><span class="tag">Representaiton Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bayesian-Statistics/"><span class="tag">Bayesian Statistics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-learning/"><span class="tag">Deep learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble/"><span class="tag">Ensemble</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Random-Forest/"><span class="tag">Random Forest</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regularization/"><span class="tag">Regularization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Evaluation-Methods/"><span class="tag">Evaluation Methods</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Overview"><span class="level-left"><span class="level-item">1</span><span class="level-item">Overview</span></span></a></li><li><a class="level is-mobile" href="#Feature-Selection"><span class="level-left"><span class="level-item">2</span><span class="level-item">Feature Selection</span></span></a></li><li><a class="level is-mobile" href="#Model-Selection"><span class="level-left"><span class="level-item">3</span><span class="level-item">Model Selection</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp" alt="Criss Wang&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2024 Zhenlin Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="/js/night.js" defer></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>