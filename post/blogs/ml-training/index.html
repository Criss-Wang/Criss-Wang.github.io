<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>DL Training: An in-depth discussion - Criss Wang&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Criss Wang&#039;s Log Book"><meta name="msapplication-TileImage" content="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Criss Wang&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Training can be hard a lot of time. But the bottlenecks vary"><meta property="og:type" content="blog"><meta property="og:title" content="DL Training: An in-depth discussion"><meta property="og:url" content="https://criss-wang.github.io/post/blogs/ml-training/"><meta property="og:site_name" content="Criss Wang&#039;s Log Book"><meta property="og:description" content="Training can be hard a lot of time. But the bottlenecks vary"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://criss-wang.github.io/img/og_image.png"><meta property="article:published_time" content="2024-02-19T05:00:00.000Z"><meta property="article:modified_time" content="2024-02-21T03:55:10.710Z"><meta property="article:author" content="Zhenlin Wang"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="MLOps"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://criss-wang.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://criss-wang.github.io/post/blogs/ml-training/"},"headline":"DL Training: An in-depth discussion","image":["https://criss-wang.github.io/img/og_image.png"],"datePublished":"2024-02-19T05:00:00.000Z","dateModified":"2024-02-21T03:55:10.710Z","author":{"@type":"Person","name":"Zhenlin Wang"},"publisher":{"@type":"Organization","name":"Criss Wang's Log Book","logo":{"@type":"ImageObject","url":"https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"}},"description":"Training can be hard a lot of time. But the bottlenecks vary"}</script><link rel="canonical" href="https://criss-wang.github.io/post/blogs/ml-training/"><link rel="icon" href="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto Slab:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp" alt="Criss Wang&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" style="font-weight: bold" href="/">Criss&#039;s Time Machine</a><a class="navbar-item" style="font-weight: bold" href="/categories/Blogs">Machine Learning</a><a class="navbar-item" style="font-weight: bold" href="/categories/Blogs_SWE">Software Engineering</a><a class="navbar-item" style="font-weight: bold" href="/categories/Projects">Projects</a><a class="navbar-item" style="font-weight: bold" href="/research">Research</a><a class="navbar-item" style="font-weight: bold" href="/archives">Archives</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Criss-Wang/"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-4 is-size-5-mobile has-text-weight-normal">DL Training: An in-depth discussion</h1><div class="article-meta is-size-7 level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-check"> </i><time dateTime="2024-02-19T05:00:00.000Z" title="2024-02-19T05:00:00.000Z">2024-02-19</time></span><span class="level-item"><i class="far fa-folder-open"> </i><a class="link-muted" href="/categories/Blogs/">Blogs</a></span></div></div><div><hr style="background-color:grey"></div><div style="padding-bottom:5px"></div><div class="content"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Training in the field of Deep Learning has been a well-studied component which contributed significantly to the development of large-scale models. As a pivotal part of model research, optimizing training of a model can lead of numerous benefits including cost saving, energy consumption reduction and expedition of effective model discovery. Personally I feel that it can be taught as an actual coursework due to the load of content it has and the fact that every DL researcher and engineer should master training thoroughly to be truly productive. Unfortunately, no school or even online coursework has considered this, partly because that the overhead cost of leveraging multi-gpu or gpu cluster can be high. Nonetheless, I'm optimistic that as we further scale the gpu resources and advent our cloud computing technology, my proposal will soon become feasible. But before that happens, let's have a simplified “course” on it in this blog post.</p>
<h2 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h2><p>This blog assumes the mastery of basic training pipeline and will only focus on advanced training techniques. We would also assume that data is clean and preprocessed. So anything related to data quality, feature transformation, tokenization or data loader definitions will be out of the scope of this blog. Instead, we focus on a training of a single (usually large scale) model with a given set of hyperparameters. In the meantime, we also have a fixed loss/evaluation metric for a given task during training. To learn more about these components. Checkout my post on <a target="_blank" rel="noopener" href="http://www.criss-wang.com/post/blogs/deep-learning-system-design-1/">designing a large-scale deep learning system</a></p>
<h2 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h2><p>[TODO] Read optimization chapter of Ian's book</p>
<p>Very often, one would also need to consider the scheduling of learning rate. The purpose is to prevent the model from converging too quickly and getting stuck in local minima. In the case of large-scale models, this is less of a problem. As studies have shown, the proximity of local minima to the global answer is generally good when model size is big enough. However, it has also been shown that warming up the gradients and rate scheduling will help stabilize training, leading to easier experiment monitoring as a consequence. Here are the overview of a few common schedulers.</p>
<ol>
<li><p><strong>Inverse Square Root (ISR)</strong>: this is one of the simplest and most common learning rate schedules, where the learning rate is set to 0 at the beginning of training and then increased by a fixed factor each epoch until reaching a maximum value, after which it remains constant for the rest of training. This helps prevent overfitting by gradually increasing the learning rate as the model becomes more efficient during early stages of training, while maintaining a steady pace throughout the remaining period.</p>
</li>
<li><p><strong>Exponential Decay (ED)</strong> - this is another simple and popular learning rate schedule that involves using an exponential function to adjust the learning rate over time. The learning rate is set to 0 at the beginning of training and then increased exponentially until reaching a maximum value, after which it remains constant for the rest of training. The allows for faster descent at early stage and more refined exploration at a later stage</p>
</li>
<li><p><strong>Stepwise Schedule</strong> - this is another simple and popular learning rate schedule that involves using discrete steps to adjust the learning rate over time. The learning rate is set to 0 at the beginning of training and then increased by a fixed factor each step until reaching a maximum value, after which it remains constant for the rest of training.</p>
</li>
</ol>
<h2 id="Memory-Reduction-in-Training"><a href="#Memory-Reduction-in-Training" class="headerlink" title="Memory Reduction in Training"></a>Memory Reduction in Training</h2><p>As DL model sizes grow bigger and bigger in recent years, our hardware (GPU, TPU) can hardly handle the amount of parameters in the model. Caching everything direclty in-memory is no longer a viable option, especially for individual researchers/engineers with limited support of GPU. Therefore, reducing memory consumption in training becomes extremely important. Using the following techniques, we can reduce the size of weights, biases and activations while maintaining accuracy, and hence enable smooth training without the heinous “Out-Of-Memory” error.</p>
<ol>
<li><p><strong>Automatic Mixed Precision (AMP)</strong><br>When working with large datasets or deep models where computing gradients on all data points can become computationally expensive, using a higher precision (e.g., floating-point) during training may result in slower convergence times due to the increased computational cost. On the other hand, using fixed-point arithmetic may not be able to capture fine details of the model's behavior and lead to lower accuracy.</p>
<p>To address this trade-off, automatic mixed precision algorithms select a precision level based on various factors such as memory usage, computation time, and accuracy requirements. In general, floating-point is used during training when more precision is needed (e.g., large weights or small gradients) while fixed-point arithmetic is used for inference when lower precision can result in faster performance (e.g., smaller models).</p>
</li>
<li><p><strong>True Low-Precision Training</strong><br>In contrary to AMP, true low-precision training is needed when size of parameters are still too big. This is one of the last resorts to sacrifice accuracy for training completeness. In this process, all parameters and gradients are in lower precision and are converted back to higher precision after training.</p>
</li>
<li><p><strong>Reduce Batch Size</strong><br>One of the easiest strategies to adopt when OOM appears is to use a smaller batch size. By doing so, we have much less data saved in cache, as the matrix used to compute gradient is much smaller (a scale of O(N) where N is number of parameters). However, the risk of slower convergence and lower accuracy is closely related to a small batch size as well. When mini-batch SGD is “too close” to SGD, the benefits brought by it will also diminsh. So be careful when using it.</p>
</li>
<li><p><strong>Gradient Accumulation</strong><br>When reducing batch size leads to severely poorer performance, and increasing it causes infeasible training, we have a work-around, gradient accumulation, which virtually increase the batch size during training. This is easily achievable (see following code):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_idx, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">   model.train()</span><br><span class="line"></span><br><span class="line">   <span class="comment"># FORWARD AND BACK PROP</span></span><br><span class="line">   outputs = model(</span><br><span class="line">      batch[<span class="string">"input_ids"</span>],</span><br><span class="line">      attention_mask=batch[<span class="string">"attention_mask"</span>],</span><br><span class="line">      labels=batch[<span class="string">"label"</span>]</span><br><span class="line">   )</span><br><span class="line"></span><br><span class="line">   outputs[<span class="string">"loss"</span>] = outputs[<span class="string">"loss"</span>] / accumulation_steps <span class="comment"># accumulate loss</span></span><br><span class="line">   fabric.backward(outputs[<span class="string">"loss"</span>])  <span class="comment"># optimizer accumulate the gradient derived from this loss</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">### UPDATE MODEL PARAMETERS</span></span><br><span class="line">   <span class="keyword">if</span> <span class="keyword">not</span> batch_idx % accumulation_steps <span class="keyword">or</span> (batch_idx + <span class="number">1</span> == <span class="built_in">len</span>(train_loader)):</span><br><span class="line">      <span class="comment"># update every accumulation_steps iterations</span></span><br><span class="line">      optimizer.step()</span><br><span class="line">      optimizer.zero_grad()</span><br></pre></td></tr></table></figure>

<p>Note that this method is at a cost of slower training due to the sequential update within the “larger” batch.</p>
</li>
<li><p><strong>Optimizer</strong></p>
<p>Sometimes, choose a stateless optimizer like SGD instead of Adam/AdamW will also help reduce memory consumption. Optimizer states can take up a significant amount of memory. For example, Adam optimizer requires first+second-order derivatives to be stored in the state for future updates. This is a additional storage term, as you can see from <a target="_blank" rel="noopener" href="https://docs.oneflow.org/en/master/cookies/imgs/Three_Stages_of_ZeRO-DP_Optimizations.jpg">this image</a>. On the other hand, SGD is simply stateless, and requires only an O(N) temporary storage of first-order gradients. Hence, you can consider switching from the commonly used Adam to SGD if it doens't result in significant performance drops.</p>
</li>
<li><p><strong>Distributed Training and Tensor Sharding</strong><br>Perhaps the most widely used strategy in industry when it comes to memory optimization is the use of sharding and distributed training. Companies have the resources and compute to perform model training on multiple GPUs and distribute the tensors across multiple servers. This is closely tied to the concept of parallelism. There are many forms of parallelisms in distributed training:</p>
<ul>
<li>Data Parallelism</li>
<li>Model Parallelism</li>
<li>Tensor Parallelism</li>
<li>Pipeline Parallelism</li>
</ul>
<p>Each of them aims to fix some inefficiencies in the training pipeline. Many packages have been developed to implement these strategies for the benefits of DL researchers and engineers. I provide a more in-depth discussion in a different <a target="_blank" rel="noopener" href="http://www.criss-wang.com/post/blogs/distributed-training">post</a>, together with discussion about the common errors people face during distributing training.</p>
</li>
<li><p><strong>Gradient Checkpointing</strong><br>[TODO] review gradeint checkpointing<br>Gradient Checkpointing is an ingenious method to reduce memory usage by repeated discard . The implementation is quite convoluted, but revolves around</p>
<p>A simple example using Hugging Face's <code>transformer</code> library looks like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">   ...</span><br><span class="line">   args=TrainingArguments(</span><br><span class="line">      ...</span><br><span class="line">      gradient_checkpointing=<span class="literal">True</span>,</span><br><span class="line">   ),</span><br><span class="line">   ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Parameter offloading</strong><br>While the model is training via gradient updates, it is usually a subsection of the parameters in the model that gets updated every time. This leads to idleness of GPU memory, which simply stores the result of the unused parameters before their turn of update. To further utilize this segment of the memory, people found a way to offload some parameters from GPU to CPU, and only reload it for update/computation whenever necessary. While this definitely increases communication cost and slows training down, it reduces memory usage by a large scale. In some other scenarios (e.g. ZeRO-3), optimizers states are also offloaded to save memory, which further reduces memory consumption.</p>
</li>
<li><p><strong>Flash Attention</strong><br>To improve the performance of Transformer-based models, flash attention was introduced by researchers at Google and Stanford University as an alternative to traditional self-attention mechanisms, which can be computationally expensive and prone to vanishing gradients.</p>
<p>In essence, Flash Attention works by using a small window size (e.g., 512) to capture local context within the input sequence. The model then applies two separate attention mechanisms: one that uses full-length self-attention to capture long-range dependencies, and another that uses Flash Attention to focus on shorter-range patterns.</p>
<p>The idea is that by using a smaller window size for the latter mechanism, the model can achieve better performance while still capturing important local features of the input sequence. This allows for more efficient computation and improved performance in downstream tasks like text classification or machine translation.</p>
</li>
<li><p><strong>Model Distillation</strong><br>Sometimes, it is just easier to reduce the model size when memory constraint is hit. This had long been a success aside from techiniques such as model pruning from the invention of Jeffery Hinton. Model Distillation, or knowledge distillation, transfers knowledge from a larger and more complex model to a smaller and simpler one. Intuitively, the larger model is trained on a given dataset. Then it will act as a “teacher” to generate output representations that can be interpreted by the smaller model. These output representations are often in the form of probability distributions over the original input data, which allows for easier interpretation and use in downstream tasks. On the other hand, the smaller “student” model will try to predict outputs based on these probability distributions. Distillation is an enormous field of study in DL, and people often use existing libraries and distilled models made available by thousands of generous researchers who willingly release their trained model weights to the public.</p>
</li>
<li><p><strong>Quantization</strong><br>[TODO] blog post about quantization<br>Another way to directly reduce model size is via quantization. Quantization typically involves mapping each value in the input to a range of possible output values, using techniques like thresholding or rounding. This is done by defining a set of bins that cover the entire range of possible outputs for a given input, and then assigning each output to its corresponding bin based on some threshold value.</p>
<p>During training, the weights and activations are typically quantized before being stored on disk or transmitted over networks in order to reduce their size without sacrificing too much accuracy. Additionally, the gradients of the loss function can be computed using an approximate gradient method called stochastic gradient descent with quantization<br>(SGD-Q), which allows for faster convergence and improved performance compared to standard SGD methods.</p>
<p>Do note that this method is slightly different from the low-precision or mixed precision strategy, as the former often recovers the precision after training, but this method allows the model trained to stay in the same precision during inference, which effecitvely reduced persistent model storage requirement as well. To learn more about quantization, checkout <a href="todo_link">this blog</a></p>
<p>A sample code implementation looks like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, BitsAndBytesConfig</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> prepare_model_for_kbit_training, get_peft_model</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">'gpt2-large'</span></span><br><span class="line">quantization_config = BitsAndBytesConfig(</span><br><span class="line">  load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">  bnb_4bit_quant_type=<span class="string">"nf4"</span>,</span><br><span class="line">  bnb_4bit_use_double_quant=<span class="literal">True</span>,</span><br><span class="line">  bnb_4bit_compute_dtype=torch.bfloat16,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># load model</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">     model_name,</span><br><span class="line">     device_map=<span class="string">"auto"</span>,</span><br><span class="line">     quantization_config=quantization_config,</span><br><span class="line">     trust_remote_code=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Note that if you are using <code>peft</code> (e.g. for LoRA), you should also consider a quantized version for it using <code>prepare_model_for_kbit_training</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> prepare_model_for_kbit_training</span><br><span class="line"><span class="comment"># preprocess the quantized model for traininng</span></span><br><span class="line">model = prepare_model_for_kbit_training(model)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Using-PyTorch-Lightning-for-simplified-training"><a href="#Using-PyTorch-Lightning-for-simplified-training" class="headerlink" title="Using PyTorch Lightning for simplified training"></a>Using PyTorch Lightning for simplified training</h2><p>Here we use lightning as an example to show how to quickly scale up training using the points mentioned above.</p>
<ol>
<li>Lightning Module<ul>
<li><code>ModelCheckpoint</code>, <code>CSVLogger</code></li>
</ul>
</li>
<li>Automatic Mixed Precision Training</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Lightning <span class="keyword">as</span> L</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LightningModel</span>(L.LightningModule):</span><br><span class="line">	...</span><br><span class="line"></span><br><span class="line">model = SomeModelLoader.from_pretrained(...)</span><br><span class="line">lightning_model = LightningModel(model)</span><br><span class="line"></span><br><span class="line">callbacks = [</span><br><span class="line">	ModelCheckpoint(save_top_k=<span class="number">1</span>, mode=<span class="string">"max"</span>, monitor=<span class="string">"val_acc"</span>)  <span class="comment"># save top 1 model</span></span><br><span class="line">]</span><br><span class="line">logger = CSVLogger(save_dir=<span class="string">"logs/"</span>, name=<span class="string">"my-model"</span>)</span><br><span class="line">trainer = L.Trainer(</span><br><span class="line">	max_epochs=<span class="number">3</span>,</span><br><span class="line">	callbacks=callbacks,</span><br><span class="line">	accelerator=<span class="string">"gpu"</span>,</span><br><span class="line">	precision=<span class="string">"16"</span>,  <span class="comment"># &lt;-- NEW</span></span><br><span class="line">	devices=[<span class="number">1</span>],</span><br><span class="line">	logger=logger,</span><br><span class="line">	log_every_n_steps=<span class="number">10</span>,</span><br><span class="line">	deterministic=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Static Graphs with <code>Torch.Compile</code><ul>
<li>New feature in Torch 2.0</li>
<li>speed up PyTorch code execution by generating optimized static graphs instead of running PyTorch code with dynamic graphs</li>
<li>Under the hood, this is a 3-step process including graph acquisition, graph lowering, and graph compilation. <a target="_blank" rel="noopener" href="https://pytorch.org/get-started/pytorch-2.0/#pytorch-2x-faster-more-pythonic-and-as-dynamic-as-ever">see official explanation</a></li>
<li>May not speed up necessarily (use it when dominated by model runtime rather than graph computation) -&gt; initial optimization compilation step takes a few minutes but eventually accelerates the model training</li>
<li>Code may break down in distributed settings</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">model = SomeModelLoader.from_pretrained(...)</span><br><span class="line">model = torch.<span class="built_in">compile</span>(model)</span><br><span class="line">lightning_model = LightningModel(model)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>DDP (model + data parallel -&gt; pipeline parallel)</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trainer = L.Trainer(</span><br><span class="line">	max_epochs=<span class="number">3</span>,</span><br><span class="line">	callbacks=callbacks,</span><br><span class="line">	accelerator=<span class="string">"gpu"</span>,</span><br><span class="line">	devices=<span class="number">4</span>,  <span class="comment"># &lt;-- NEW</span></span><br><span class="line">	strategy=<span class="string">"ddp"</span>,  <span class="comment"># &lt;-- NEW</span></span><br><span class="line">	precision=<span class="string">"16"</span>,</span><br><span class="line">	logger=logger,</span><br><span class="line">	log_every_n_steps=<span class="number">10</span>,</span><br><span class="line">	deterministic=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>Sharding<ul>
<li><code>strategy="deepspeed_stage_2"</code> to replace <code>strategy="ddp"</code></li>
<li>note that <code>deepspeed_stage_2</code> refers to the stage two of <a target="_blank" rel="noopener" href="https://docs.oneflow.org/en/master/cookies/zero.html#:~:text=Specifically%2C%20ZeRO%2DDP%20has%20three,of%20traffic%20as%20data%20parallel.">Zero Redundancy Optimizer (ZeRO)</a>, which effectively achieves the tensor sharding for optimizer states and gradients.</li>
<li>note that here stages mean different sharding strategies. Consider optimizer states/gradients/weights to be sharded and optimizer state to be offloaded to CPU (refer to Paged Optimizers if can't remember) if necessary. This corresponds to different stages of ZeRO method as well.</li>
</ul>
</li>
<li>Use Lightning <code>Fabric</code> module (faster than <code>Trainer</code>). You may look it up online for further references.</li>
</ol>
<h2 id="A-more-challenging-code-using-native-PyTorch"><a href="#A-more-challenging-code-using-native-PyTorch" class="headerlink" title="A more challenging code using native PyTorch"></a>A more challenging code using native PyTorch</h2><p>If you are interested in building it from scratch with PyTorch directly, checkout this (if you don’t understand the syntax, please DIY)</p>
<details>
<summary>more challenging code</summary>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PyTorch</span><br></pre></td></tr></table></figure>
</details>

<h2 id="Manage-your-training-process"><a href="#Manage-your-training-process" class="headerlink" title="Manage your training process"></a>Manage your training process</h2><p>After all these training optimizations, it would be wasteful not to save your experiment data. Therefore, post-training processing becomes extremely important. Here is a brief list of actions to take note of:</p>
<ul>
<li>Metrics Monitoring: keeping the experiment data in persistent storage like MongoDB or SQL DB via mature mlops platforms like <em>MLflow</em>, <em>Weights and Biases</em> or <em>Kubeflow</em> is a convenient and important step to further analyse progress in model training and debugging potential bottlenecks/errors.</li>
<li>Performance Monitoring: finding inefficiencies in hardward utilization (idle processes, slow hardware communication, low memory usage) can help further speed up your training and saving you tons of money. Your choice of server communication tools become important in this sense as well, and monitoring that part can be very tricky to carry out if you don't have sufficient low-level knowledge.</li>
<li>Logging: error logging is not an easy thing when it comes to model training. Problems often can only be found from a parameter/matrix level, and the fact that training is distributed accross servers make it even harder to achieve.</li>
<li>Model Checkpointing: saving you model half way during the training is critical to achieve fault-tolerant training. PyTorch maintainers has developed a thorough set of tools like <code>dcp</code>, <code>elastic</code>, <code>rendezvous</code> and <code>torchrun</code> in collaboration with deepspeed to help achieve that.</li>
</ul>
<p>If you are really into completing the full training cycle and squeezing every last sip of the resources you've paid for, I would urge you to check out <a target="_blank" rel="noopener" href="https://criss-wang.com/post/blogs/ml-post-training">my other post</a> to learn more about it.</p>
<h2 id="To-conclude…"><a href="#To-conclude…" class="headerlink" title="To conclude…"></a>To conclude…</h2><p>Training a model involves significant amount of design decisions that vary based on model, domain and data. It requires many years of experiences and mistake-making to form the right intuitions for model training setups. That said, constantly practicing it using some personal projects is what I did, and what I would recommend every passionate ML researcher/engineer should do to keep themselves updated with the latest progress in DL training. It might not “make perfect”, but it certainly saves you and your company enormous time and money in the day-to-day interactions with every-bloating monsterous models.</p>
<h2 id="References-and-Further-Readings"><a href="#References-and-Further-Readings" class="headerlink" title="References and Further Readings"></a>References and Further Readings</h2><ol>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention">https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention</a></li>
<li><a target="_blank" rel="noopener" href="https://sebastianraschka.com/blog/2023/pytorch-faster.html">https://sebastianraschka.com/blog/2023/pytorch-faster.html</a></li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>DL Training: An in-depth discussion</p><p><a href="https://criss-wang.github.io/post/blogs/ml-training/">https://criss-wang.github.io/post/blogs/ml-training/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Zhenlin Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-02-19</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-02-20</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/post/blogs/temp/distributed-training/"><i class="level-item fas fa-chevron-left"></i><span class="level-item"> </span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/post/blogs/project-considerations/"><span class="level-item">Starting your AI/ML Project: from research to engineering</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Selfie.webp" alt="Zhenlin Wang (Criss)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Zhenlin Wang (Criss)</p><div class="is-size-7 multiline is-block justify-content-center" style="white-space:pre;font-style: italic">Software Development
Machine Learning
Artificial Intelligence
</div><div style="padding-top: 10px;"></div><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Pittsburgh, PA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">66</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">37</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">39</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Criss-Wang/" target="_blank" rel="noopener"><i class="fab fa-github"></i>   Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Criss-Wang/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/zhenlin-wang/"><i class="fab fa-linkedin-in"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:zhenlinw@cs.cmu.edu"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="CV" href="https://twitter.com/CrissWang4"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Data-Mining-Data-Engineering/"><span class="tag">Data Mining/Data Engineering</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Clustering/"><span class="tag">Clustering</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Supervised-Learning/"><span class="tag">Supervised Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement Learning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Database-System/"><span class="tag">Database System</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistics/"><span class="tag">Statistics</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Data/"><span class="tag">Big Data</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommender-Systems/"><span class="tag">Recommender Systems</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Software-Engineering/"><span class="tag">Software Engineering</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analytics/"><span class="tag">Data Analytics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/System-Design/"><span class="tag">System Design</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/A-B-testing/"><span class="tag">A/B testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-System/"><span class="tag">Distributed System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cloud-Computing/"><span class="tag">Cloud Computing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix-Computation/"><span class="tag">Matrix Computation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dimensionality-Reduction/"><span class="tag">Dimensionality Reduction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble/"><span class="tag">Ensemble</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Random-Forest/"><span class="tag">Random Forest</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Markov-Models/"><span class="tag">Hidden Markov Models</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-Programming/"><span class="tag">Dynamic Programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLOps/"><span class="tag">MLOps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Management/"><span class="tag">Project Management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-learning/"><span class="tag">Deep learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regularization/"><span class="tag">Regularization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Evaluation-Methods/"><span class="tag">Evaluation Methods</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistical-Inference/"><span class="tag">Statistical Inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representaiton-Learning/"><span class="tag">Representaiton Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bayesian-Statistics/"><span class="tag">Bayesian Statistics</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Assumptions"><span class="level-left"><span class="level-item">2</span><span class="level-item">Assumptions</span></span></a></li><li><a class="level is-mobile" href="#Optimizers"><span class="level-left"><span class="level-item">3</span><span class="level-item">Optimizers</span></span></a></li><li><a class="level is-mobile" href="#Memory-Reduction-in-Training"><span class="level-left"><span class="level-item">4</span><span class="level-item">Memory Reduction in Training</span></span></a></li><li><a class="level is-mobile" href="#Using-PyTorch-Lightning-for-simplified-training"><span class="level-left"><span class="level-item">5</span><span class="level-item">Using PyTorch Lightning for simplified training</span></span></a></li><li><a class="level is-mobile" href="#A-more-challenging-code-using-native-PyTorch"><span class="level-left"><span class="level-item">6</span><span class="level-item">A more challenging code using native PyTorch</span></span></a></li><li><a class="level is-mobile" href="#Manage-your-training-process"><span class="level-left"><span class="level-item">7</span><span class="level-item">Manage your training process</span></span></a></li><li><a class="level is-mobile" href="#To-conclude…"><span class="level-left"><span class="level-item">8</span><span class="level-item">To conclude…</span></span></a></li><li><a class="level is-mobile" href="#References-and-Further-Readings"><span class="level-left"><span class="level-item">9</span><span class="level-item">References and Further Readings</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.statically.io/gh/Criss-Wang/image-host@master/Blog/Steins_Gate_Lab_Badge.webp" alt="Criss Wang&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2024 Zhenlin Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="/js/night.js" defer></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>